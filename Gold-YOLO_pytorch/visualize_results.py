#!/usr/bin/env python3
# -*- coding:utf-8 -*-

import os
import cv2
import numpy as np
from pathlib import Path
import matplotlib.pyplot as plt
import matplotlib.patches as patches
from PIL import Image

def load_voc_classes():
    """åŠ è½½VOCç±»åˆ«åç§°"""
    return [
        'aeroplane', 'bicycle', 'bird', 'boat', 'bottle',
        'bus', 'car', 'cat', 'chair', 'cow',
        'diningtable', 'dog', 'horse', 'motorbike', 'person',
        'pottedplant', 'sheep', 'sofa', 'train', 'tvmonitor'
    ]

def parse_yolo_label(label_path, img_width, img_height):
    """è§£æYOLOæ ¼å¼æ ‡ç­¾æ–‡ä»¶"""
    boxes = []
    if not os.path.exists(label_path):
        return boxes
    
    with open(label_path, 'r') as f:
        for line in f:
            parts = line.strip().split()
            if len(parts) >= 5:
                class_id = int(parts[0])
                x_center = float(parts[1]) * img_width
                y_center = float(parts[2]) * img_height
                width = float(parts[3]) * img_width
                height = float(parts[4]) * img_height
                
                # è½¬æ¢ä¸ºå·¦ä¸Šè§’åæ ‡
                x1 = x_center - width / 2
                y1 = y_center - height / 2
                x2 = x_center + width / 2
                y2 = y_center + height / 2
                
                confidence = float(parts[5]) if len(parts) > 5 else 1.0
                
                boxes.append({
                    'class_id': class_id,
                    'bbox': [x1, y1, x2, y2],
                    'confidence': confidence
                })
    return boxes

def visualize_detection_results():
    """å¯è§†åŒ–æ£€æµ‹ç»“æœ"""
    print('ğŸ¨ å¯è§†åŒ–Gold-YOLO-næ£€æµ‹ç»“æœ')
    print('=' * 60)
    
    # è·¯å¾„è®¾ç½®
    test_images_dir = Path('data/test_images')
    test_labels_dir = Path('data/test_labels')
    original_inference_dir = Path('runs/inference/gold_yolo_n_test/test_images')
    improved_inference_dir = Path('runs/inference/gold_yolo_n_improved_200epochs/test_images')
    output_dir = Path('runs/inference/gold_yolo_n_improved_200epochs/visualizations')
    output_dir.mkdir(exist_ok=True)
    
    classes = load_voc_classes()
    colors = plt.cm.Set3(np.linspace(0, 1, len(classes)))
    
    # å¤„ç†å‰5å¼ å›¾ç‰‡è¿›è¡Œå¯è§†åŒ–
    image_files = list(test_images_dir.glob('*.jpg'))[:5]
    
    for img_file in image_files:
        img_name = img_file.stem
        print(f'å¤„ç†å›¾ç‰‡: {img_name}')
        
        # åŠ è½½å›¾ç‰‡
        img = cv2.imread(str(img_file))
        img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
        img_height, img_width = img.shape[:2]
        
        # åŠ è½½çœŸå®æ ‡ç­¾å’Œé¢„æµ‹æ ‡ç­¾
        gt_label_file = test_labels_dir / f'{img_name}.txt'
        original_pred_file = original_inference_dir / f'{img_name}.txt'
        improved_pred_file = improved_inference_dir / f'{img_name}.txt'

        gt_boxes = parse_yolo_label(gt_label_file, img_width, img_height)
        original_pred_boxes = parse_yolo_label(original_pred_file, img_width, img_height)
        improved_pred_boxes = parse_yolo_label(improved_pred_file, img_width, img_height)
        
        # åˆ›å»ºå¯è§†åŒ– (ä¸‰åˆ—å¯¹æ¯”)
        fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(30, 10))

        # æ˜¾ç¤ºçœŸå®æ ‡ç­¾
        ax1.imshow(img_rgb)
        ax1.set_title(f'Ground Truth - {img_name}\\n{len(gt_boxes)} objects', fontsize=14)
        ax1.axis('off')
        
        for box in gt_boxes:
            x1, y1, x2, y2 = box['bbox']
            class_id = box['class_id']
            class_name = classes[class_id]
            color = colors[class_id]
            
            rect = patches.Rectangle((x1, y1), x2-x1, y2-y1, 
                                   linewidth=2, edgecolor=color, facecolor='none')
            ax1.add_patch(rect)
            ax1.text(x1, y1-5, f'{class_name}', 
                    bbox=dict(boxstyle='round,pad=0.3', facecolor=color, alpha=0.7),
                    fontsize=10, color='black')
        
        # æ˜¾ç¤ºé¢„æµ‹ç»“æœ
        ax2.imshow(img_rgb)
        ax2.set_title(f'Predictions - {img_name}\\n{len(pred_boxes)} detections', fontsize=14)
        ax2.axis('off')
        
        for box in pred_boxes:
            x1, y1, x2, y2 = box['bbox']
            class_id = box['class_id']
            confidence = box['confidence']
            class_name = classes[class_id]
            color = colors[class_id]
            
            rect = patches.Rectangle((x1, y1), x2-x1, y2-y1, 
                                   linewidth=2, edgecolor=color, facecolor='none')
            ax2.add_patch(rect)
            ax2.text(x1, y1-5, f'{class_name} {confidence:.2f}', 
                    bbox=dict(boxstyle='round,pad=0.3', facecolor=color, alpha=0.7),
                    fontsize=10, color='black')
        
        # ä¿å­˜å¯è§†åŒ–ç»“æœ
        output_file = output_dir / f'{img_name}_comparison.png'
        plt.tight_layout()
        plt.savefig(output_file, dpi=150, bbox_inches='tight')
        plt.close()
        
        print(f'   ä¿å­˜åˆ°: {output_file}')
        print(f'   çœŸå®ç›®æ ‡: {len(gt_boxes)}, é¢„æµ‹ç›®æ ‡: {len(pred_boxes)}')
    
    print(f'\\nâœ… å¯è§†åŒ–å®Œæˆï¼Œç»“æœä¿å­˜åœ¨: {output_dir}')
    
    # åˆ›å»ºç»Ÿè®¡å›¾è¡¨
    create_statistics_plot(output_dir)

def create_statistics_plot(output_dir):
    """åˆ›å»ºç»Ÿè®¡å›¾è¡¨"""
    print('\\nğŸ“Š åˆ›å»ºç»Ÿè®¡å›¾è¡¨')
    
    # ä»ä¹‹å‰çš„åˆ†æç»“æœåˆ›å»ºå›¾è¡¨
    test_images_dir = Path('data/test_images')
    test_labels_dir = Path('data/test_labels')
    inference_dir = Path('runs/inference/gold_yolo_n_test/test_images')
    
    image_names = []
    gt_counts = []
    pred_counts = []
    
    for img_file in test_images_dir.glob('*.jpg'):
        img_name = img_file.stem
        
        # åŠ è½½å›¾ç‰‡è·å–å°ºå¯¸
        img = cv2.imread(str(img_file))
        img_height, img_width = img.shape[:2]
        
        # ç»Ÿè®¡çœŸå®å’Œé¢„æµ‹çš„ç›®æ ‡æ•°é‡
        gt_label_file = test_labels_dir / f'{img_name}.txt'
        pred_label_file = inference_dir / f'{img_name}.txt'
        
        gt_boxes = parse_yolo_label(gt_label_file, img_width, img_height)
        pred_boxes = parse_yolo_label(pred_label_file, img_width, img_height)
        
        image_names.append(img_name)
        gt_counts.append(len(gt_boxes))
        pred_counts.append(len(pred_boxes))
    
    # åˆ›å»ºå¯¹æ¯”å›¾è¡¨
    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(15, 10))
    
    # ç›®æ ‡æ•°é‡å¯¹æ¯”
    x = np.arange(len(image_names))
    width = 0.35
    
    ax1.bar(x - width/2, gt_counts, width, label='Ground Truth', alpha=0.8, color='skyblue')
    ax1.bar(x + width/2, pred_counts, width, label='Predictions', alpha=0.8, color='lightcoral')
    
    ax1.set_xlabel('Images')
    ax1.set_ylabel('Number of Objects')
    ax1.set_title('Object Detection Count Comparison')
    ax1.set_xticks(x)
    ax1.set_xticklabels(image_names, rotation=45, ha='right')
    ax1.legend()
    ax1.grid(True, alpha=0.3)
    
    # æ€»ä½“ç»Ÿè®¡
    total_gt = sum(gt_counts)
    total_pred = sum(pred_counts)
    
    categories = ['Ground Truth', 'Predictions']
    values = [total_gt, total_pred]
    colors_pie = ['skyblue', 'lightcoral']
    
    ax2.pie(values, labels=categories, colors=colors_pie, autopct='%1.1f%%', startangle=90)
    ax2.set_title(f'Total Objects: GT={total_gt}, Pred={total_pred}')
    
    plt.tight_layout()
    stats_file = output_dir / 'detection_statistics.png'
    plt.savefig(stats_file, dpi=150, bbox_inches='tight')
    plt.close()
    
    print(f'   ç»Ÿè®¡å›¾è¡¨ä¿å­˜åˆ°: {stats_file}')

if __name__ == '__main__':
    visualize_detection_results()
