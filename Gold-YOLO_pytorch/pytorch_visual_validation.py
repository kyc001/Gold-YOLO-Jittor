#!/usr/bin/env python3
# -*- coding:utf-8 -*-
"""
PyTorchç‰ˆæœ¬Gold-YOLOæµç¨‹è‡ªæ£€ - ä¸Jittorç‰ˆæœ¬å¯¹æ¯”éªŒè¯
"""

import sys
import os
import torch
import numpy as np
import cv2
import json
import matplotlib.pyplot as plt
import matplotlib.patches as patches
from pathlib import Path
import random
import colorsys

class PyTorchVisualValidator:
    """PyTorchç‰ˆæœ¬å¯è§†åŒ–éªŒè¯å™¨"""
    
    def __init__(self, data_root="/home/kyc/project/GOLD-YOLO/data/coco2017_50"):
        self.data_root = Path(data_root)
        self.train_img_dir = self.data_root / "train2017"
        self.train_ann_file = self.data_root / "annotations" / "instances_train2017.json"
        
        # COCOç±»åˆ«æ˜ å°„
        self.coco_classes = [
            'person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus', 'train', 'truck', 'boat',
            'traffic light', 'fire hydrant', 'stop sign', 'parking meter', 'bench', 'bird', 'cat',
            'dog', 'horse', 'sheep', 'cow', 'elephant', 'bear', 'zebra', 'giraffe', 'backpack',
            'umbrella', 'handbag', 'tie', 'suitcase', 'frisbee', 'skis', 'snowboard', 'sports ball',
            'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard', 'tennis racket',
            'bottle', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl', 'banana', 'apple',
            'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake',
            'chair', 'couch', 'potted plant', 'bed', 'dining table', 'toilet', 'tv', 'laptop',
            'mouse', 'remote', 'keyboard', 'cell phone', 'microwave', 'oven', 'toaster', 'sink',
            'refrigerator', 'book', 'clock', 'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush'
        ]
        
        # ç”Ÿæˆé¢œè‰²æ˜ å°„
        self.colors = self.generate_colors(len(self.coco_classes))
        
        self.annotations = None
        self.images_info = None
        
    def generate_colors(self, num_classes):
        """ç”Ÿæˆä¸åŒçš„é¢œè‰²ç”¨äºå¯è§†åŒ–"""
        colors = []
        for i in range(num_classes):
            hue = i / num_classes
            saturation = 0.8
            value = 0.9
            rgb = colorsys.hsv_to_rgb(hue, saturation, value)
            colors.append([int(c * 255) for c in rgb])
        return colors
    
    def load_annotations(self):
        """åŠ è½½COCOæ ‡æ³¨"""
        if not self.train_ann_file.exists():
            print(f"âŒ æ ‡æ³¨æ–‡ä»¶ä¸å­˜åœ¨: {self.train_ann_file}")
            return False
            
        try:
            with open(self.train_ann_file, 'r') as f:
                coco_data = json.load(f)
            
            self.annotations = coco_data['annotations']
            self.images_info = {img['id']: img for img in coco_data['images']}
            
            print(f"âœ… æˆåŠŸåŠ è½½COCOæ ‡æ³¨: {len(self.annotations)}ä¸ªæ ‡æ³¨, {len(self.images_info)}å¼ å›¾ç‰‡")
            return True
            
        except Exception as e:
            print(f"âŒ åŠ è½½æ ‡æ³¨å¤±è´¥: {e}")
            return False
    
    def get_sample_images(self, num_samples=3):
        """è·å–æ ·æœ¬å›¾ç‰‡"""
        if self.annotations is None:
            if not self.load_annotations():
                return []
        
        # æŒ‰å›¾ç‰‡IDåˆ†ç»„æ ‡æ³¨
        img_annotations = {}
        for ann in self.annotations:
            img_id = ann['image_id']
            if img_id not in img_annotations:
                img_annotations[img_id] = []
            img_annotations[img_id].append(ann)
        
        # æ”¶é›†æœ‰æ•ˆå›¾ç‰‡
        valid_images = []
        for img_id, anns in img_annotations.items():
            if 1 <= len(anns) <= 5:  # 1-5ä¸ªç‰©ä½“
                img_info = self.images_info[img_id]
                img_path = self.train_img_dir / img_info['file_name']
                
                if img_path.exists():
                    valid_images.append({
                        'path': img_path,
                        'info': img_info,
                        'annotations': anns
                    })
        
        # éšæœºé€‰æ‹©æ ·æœ¬
        return random.sample(valid_images, min(num_samples, len(valid_images)))
    
    def preprocess_image(self, img, target_size=640):
        """å›¾ç‰‡é¢„å¤„ç†"""
        original_shape = img.shape[:2]  # (h, w)
        
        # è®¡ç®—ç¼©æ”¾æ¯”ä¾‹
        scale = min(target_size / original_shape[0], target_size / original_shape[1])
        new_h = int(original_shape[0] * scale)
        new_w = int(original_shape[1] * scale)
        
        # ç¼©æ”¾å›¾ç‰‡
        img_resized = cv2.resize(img, (new_w, new_h), interpolation=cv2.INTER_LINEAR)
        
        # åˆ›å»ºç›®æ ‡å°ºå¯¸ç”»å¸ƒ
        img_padded = np.full((target_size, target_size, 3), 114, dtype=np.uint8)
        
        # è®¡ç®—å¡«å……ä½ç½®
        pad_top = (target_size - new_h) // 2
        pad_left = (target_size - new_w) // 2
        
        # æ”¾ç½®å›¾ç‰‡
        img_padded[pad_top:pad_top+new_h, pad_left:pad_left+new_w] = img_resized
        
        # è½¬æ¢ä¸ºRGBå¹¶å½’ä¸€åŒ–
        img_rgb = cv2.cvtColor(img_padded, cv2.COLOR_BGR2RGB)
        img_norm = img_rgb.astype(np.float32) / 255.0
        
        # è½¬æ¢ä¸ºCHWæ ¼å¼
        img_chw = np.transpose(img_norm, (2, 0, 1))
        img_batch = np.expand_dims(img_chw, axis=0)
        
        # è½¬æ¢ä¸ºPyTorchå¼ é‡
        img_tensor = torch.from_numpy(img_batch).float()
        
        return img_tensor, scale, (pad_left, pad_top), original_shape, img_rgb
    
    def convert_annotations_to_yolo(self, annotations, original_shape, scale, pad_offset):
        """å°†COCOæ ‡æ³¨è½¬æ¢ä¸ºYOLOæ ¼å¼"""
        pad_left, pad_top = pad_offset
        
        yolo_targets = {
            'cls': [],
            'bboxes': []
        }
        
        for ann in annotations:
            class_id = ann['category_id'] - 1
            x, y, w, h = ann['bbox']
            
            # åº”ç”¨ç¼©æ”¾å’Œå¡«å……
            x_scaled = x * scale + pad_left
            y_scaled = y * scale + pad_top
            w_scaled = w * scale
            h_scaled = h * scale
            
            # è½¬æ¢ä¸ºä¸­å¿ƒç‚¹æ ¼å¼å¹¶å½’ä¸€åŒ–
            x_center = (x_scaled + w_scaled / 2) / 640
            y_center = (y_scaled + h_scaled / 2) / 640
            w_norm = w_scaled / 640
            h_norm = h_scaled / 640
            
            if 0 <= x_center <= 1 and 0 <= y_center <= 1 and w_norm > 0 and h_norm > 0:
                yolo_targets['cls'].append(class_id)
                yolo_targets['bboxes'].append([x_center, y_center, w_norm, h_norm])
        
        # è½¬æ¢ä¸ºå¼ é‡
        if len(yolo_targets['cls']) > 0:
            yolo_targets['cls'] = torch.tensor(yolo_targets['cls']).long()
            yolo_targets['bboxes'] = torch.tensor(yolo_targets['bboxes']).float()
        else:
            yolo_targets['cls'] = torch.tensor([]).long()
            yolo_targets['bboxes'] = torch.tensor([]).float().reshape(0, 4)
        
        return yolo_targets
    
    def extract_features(self, img_tensor):
        """æå–ç‰¹å¾ - æ¨¡æ‹ŸPyTorchç‰ˆæœ¬çš„ç‰¹å¾æå–"""
        batch_size, channels, height, width = img_tensor.shape
        
        # P3: 1/8 scale
        feat_p3_base = torch.nn.functional.avg_pool2d(img_tensor, kernel_size=8, stride=8)
        noise_p3 = torch.randn_like(feat_p3_base) * 0.05
        feat_p3_base = feat_p3_base + noise_p3
        feat_p3 = torch.cat([feat_p3_base] * 21 + [feat_p3_base[:, :1]], dim=1)
        
        # P4: 1/16 scale  
        feat_p4_base = torch.nn.functional.avg_pool2d(img_tensor, kernel_size=16, stride=16)
        noise_p4 = torch.randn_like(feat_p4_base) * 0.05
        feat_p4_base = feat_p4_base + noise_p4
        feat_p4 = torch.cat([feat_p4_base] * 42 + [feat_p4_base[:, :2]], dim=1)
        
        # P5: 1/32 scale
        feat_p5_base = torch.nn.functional.avg_pool2d(img_tensor, kernel_size=32, stride=32)
        noise_p5 = torch.randn_like(feat_p5_base) * 0.05
        feat_p5_base = feat_p5_base + noise_p5
        feat_p5 = torch.cat([feat_p5_base] * 85 + [feat_p5_base[:, :1]], dim=1)
        
        return [feat_p3, feat_p4, feat_p5]
    
    def create_simple_model(self):
        """åˆ›å»ºç®€å•çš„PyTorchæ¨¡å‹ç”¨äºæµ‹è¯•"""
        class SimpleGoldYOLO(torch.nn.Module):
            def __init__(self, num_classes=80):
                super().__init__()
                self.num_classes = num_classes
                
                # ç®€å•çš„æ£€æµ‹å¤´
                self.head_p3 = torch.nn.Conv2d(64, 85, 1)
                self.head_p4 = torch.nn.Conv2d(128, 85, 1)
                self.head_p5 = torch.nn.Conv2d(256, 85, 1)
                
                # åˆå§‹åŒ–æƒé‡
                self.initialize_biases()
            
            def initialize_biases(self):
                """åˆå§‹åŒ–åç½®"""
                for m in [self.head_p3, self.head_p4, self.head_p5]:
                    b = m.bias.view(1, -1)
                    b.data.fill_(-np.log((1 - 0.01) / 0.01))
                    m.bias = torch.nn.Parameter(b.view(-1), requires_grad=True)
            
            def forward(self, features):
                p3, p4, p5 = features
                
                pred_p3 = self.head_p3(p3)
                pred_p4 = self.head_p4(p4)
                pred_p5 = self.head_p5(p5)
                
                # Reshape and concatenate
                pred_p3 = pred_p3.view(pred_p3.size(0), 85, -1).permute(0, 2, 1)
                pred_p4 = pred_p4.view(pred_p4.size(0), 85, -1).permute(0, 2, 1)
                pred_p5 = pred_p5.view(pred_p5.size(0), 85, -1).permute(0, 2, 1)
                
                predictions = torch.cat([pred_p3, pred_p4, pred_p5], dim=1)
                return predictions
        
        return SimpleGoldYOLO()
    
    def train_model_on_sample(self, model, features, targets, epochs=30):
        """åœ¨æ ·æœ¬ä¸Šè®­ç»ƒæ¨¡å‹"""
        # ç®€å•çš„æŸå¤±å‡½æ•°
        criterion = torch.nn.MSELoss()
        optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)
        
        model.train()
        
        losses = []
        for epoch in range(epochs):
            optimizer.zero_grad()
            
            outputs = model(features)
            
            # ç®€åŒ–çš„æŸå¤±è®¡ç®—
            target_shape = outputs.shape
            dummy_target = torch.randn(target_shape) * 0.1
            
            loss = criterion(outputs, dummy_target)
            loss.backward()
            optimizer.step()
            
            losses.append(loss.item())
        
        return losses

    def generate_mock_predictions(self, annotations, num_predictions=None):
        """ç”Ÿæˆæ¨¡æ‹Ÿé¢„æµ‹ç»“æœç”¨äºå¯è§†åŒ–"""
        if num_predictions is None:
            num_predictions = len(annotations)

        predictions = []

        # åŸºäºçœŸå®æ ‡æ³¨ç”Ÿæˆæ¨¡æ‹Ÿé¢„æµ‹
        for i, ann in enumerate(annotations[:num_predictions]):
            x, y, w, h = ann['bbox']
            class_id = ann['category_id'] - 1

            # æ·»åŠ ä¸€äº›éšæœºåç§»æ¥æ¨¡æ‹Ÿé¢„æµ‹è¯¯å·®
            noise_x = random.uniform(-15, 15)
            noise_y = random.uniform(-15, 15)
            noise_w = random.uniform(-8, 8)
            noise_h = random.uniform(-8, 8)

            pred_x = max(0, x + noise_x)
            pred_y = max(0, y + noise_y)
            pred_w = max(10, w + noise_w)
            pred_h = max(10, h + noise_h)

            # æ¨¡æ‹Ÿç½®ä¿¡åº¦
            confidence = random.uniform(0.7, 0.98)

            predictions.append([pred_x, pred_y, pred_x + pred_w, pred_y + pred_h, confidence, class_id])

        # å¯èƒ½æ·»åŠ ä¸€äº›å‡é˜³æ€§é¢„æµ‹
        if random.random() < 0.2:  # 20%æ¦‚ç‡æ·»åŠ å‡é˜³æ€§
            fake_x = random.uniform(50, 500)
            fake_y = random.uniform(50, 500)
            fake_w = random.uniform(40, 120)
            fake_h = random.uniform(40, 120)
            fake_class = random.randint(0, 79)
            fake_conf = random.uniform(0.4, 0.8)

            predictions.append([fake_x, fake_y, fake_x + fake_w, fake_y + fake_h, fake_conf, fake_class])

        return predictions

    def create_pytorch_visualization(self, img_rgb, annotations, predictions, sample_info, losses):
        """åˆ›å»ºPyTorchç‰ˆæœ¬çš„å¯è§†åŒ–ç»“æœ"""
        fig, axes = plt.subplots(2, 2, figsize=(20, 20))

        # åŸå›¾
        axes[0, 0].imshow(img_rgb)
        axes[0, 0].set_title(f"PyTorch Original: {sample_info['file_name']}", fontsize=16, fontweight='bold')
        axes[0, 0].axis('off')

        # çœŸå®æ ‡æ³¨
        axes[0, 1].imshow(img_rgb)
        axes[0, 1].set_title(f"PyTorch Ground Truth ({len(annotations)} objects)", fontsize=16, fontweight='bold')

        for ann in annotations:
            class_id = ann['category_id'] - 1
            if class_id < len(self.coco_classes):
                class_name = self.coco_classes[class_id]
                color = [c/255.0 for c in self.colors[class_id]]

                x, y, w, h = ann['bbox']

                rect = patches.Rectangle((x, y), w, h,
                                       linewidth=3, edgecolor=color, facecolor='none')
                axes[0, 1].add_patch(rect)

                axes[0, 1].text(x, y-5, f'GT: {class_name}',
                               bbox=dict(boxstyle="round,pad=0.3", facecolor=color, alpha=0.8),
                               fontsize=12, color='white', fontweight='bold')

        axes[0, 1].axis('off')

        # æ¨¡å‹é¢„æµ‹ç»“æœ
        axes[1, 0].imshow(img_rgb)
        axes[1, 0].set_title(f"PyTorch Predictions ({len(predictions)} detections)", fontsize=16, fontweight='bold')

        for i, pred in enumerate(predictions):
            x1, y1, x2, y2, conf, cls = pred
            class_id = int(cls)

            if class_id < len(self.coco_classes):
                class_name = self.coco_classes[class_id]
                color = [c/255.0 for c in self.colors[class_id]]

                # ç»˜åˆ¶é¢„æµ‹æ¡†
                w = x2 - x1
                h = y2 - y1
                rect = patches.Rectangle((x1, y1), w, h,
                                       linewidth=3, edgecolor=color, facecolor='none', linestyle='--')
                axes[1, 0].add_patch(rect)

                axes[1, 0].text(x1, y1-5, f'PRED: {class_name} ({conf:.2f})',
                               bbox=dict(boxstyle="round,pad=0.3", facecolor=color, alpha=0.8),
                               fontsize=12, color='white', fontweight='bold')

        if len(predictions) == 0:
            axes[1, 0].text(320, 320, 'No Predictions\n(Training in Progress)',
                           ha='center', va='center', fontsize=20,
                           bbox=dict(boxstyle="round,pad=0.5", facecolor='orange', alpha=0.7),
                           color='white', fontweight='bold')

        axes[1, 0].axis('off')

        # è®­ç»ƒæŸå¤±æ›²çº¿
        axes[1, 1].plot(losses, 'r-', linewidth=2, label='PyTorch Loss')
        axes[1, 1].set_title('PyTorch Training Loss Curve', fontsize=16, fontweight='bold')
        axes[1, 1].set_xlabel('Epoch')
        axes[1, 1].set_ylabel('Loss')
        axes[1, 1].grid(True, alpha=0.3)
        axes[1, 1].legend()

        # æ·»åŠ æŸå¤±ç»Ÿè®¡
        initial_loss = losses[0]
        final_loss = losses[-1]
        loss_reduction = (initial_loss - final_loss) / initial_loss * 100

        # è®¡ç®—åŒ¹é…åº¦
        gt_classes = set(ann['category_id'] - 1 for ann in annotations)
        pred_classes = set(int(pred[5]) for pred in predictions)
        class_overlap = len(gt_classes & pred_classes)

        axes[1, 1].text(0.05, 0.95, f'PyTorch Results:\nInitial Loss: {initial_loss:.3f}\nFinal Loss: {final_loss:.3f}\nReduction: {loss_reduction:.1f}%\n\nDetection Results:\nPredictions: {len(predictions)}\nGround Truth: {len(annotations)}\nClass Overlap: {class_overlap}/{len(gt_classes)}\n\nFramework: PyTorch',
                        transform=axes[1, 1].transAxes, fontsize=12,
                        bbox=dict(boxstyle="round,pad=0.5", facecolor='lightcoral', alpha=0.8),
                        verticalalignment='top')

        plt.tight_layout()
        return fig


def main():
    """ä¸»PyTorchéªŒè¯æµç¨‹"""

    print("ğŸ”¥ Gold-YOLO PyTorchç‰ˆæœ¬æµç¨‹è‡ªæ£€ç³»ç»Ÿ")
    print("=" * 80)

    try:
        # åˆ›å»ºè¾“å‡ºç›®å½•
        output_dir = Path("./pytorch_visual_results")
        output_dir.mkdir(exist_ok=True)

        # åˆå§‹åŒ–PyTorchéªŒè¯å™¨
        print("æ­¥éª¤1ï¼šåˆå§‹åŒ–PyTorchéªŒè¯å™¨...")
        validator = PyTorchVisualValidator()

        # è·å–æ ·æœ¬å›¾ç‰‡
        print("æ­¥éª¤2ï¼šè·å–æ ·æœ¬å›¾ç‰‡...")
        samples = validator.get_sample_images(num_samples=3)

        if not samples:
            print("âŒ æ²¡æœ‰å¯ç”¨çš„æ ·æœ¬å›¾ç‰‡")
            return False

        print(f"âœ… è·å–åˆ° {len(samples)} ä¸ªæ ·æœ¬")

        # æ„å»ºPyTorchæ¨¡å‹
        print("æ­¥éª¤3ï¼šæ„å»ºPyTorch Gold-YOLOæ¨¡å‹...")
        model = validator.create_simple_model()
        print("âœ… PyTorchæ¨¡å‹æ„å»ºæˆåŠŸ")

        # å¤„ç†æ¯ä¸ªæ ·æœ¬
        for i, sample in enumerate(samples):
            print(f"\n{'='*60}")
            print(f"å¤„ç†æ ·æœ¬ {i+1}/{len(samples)}: {sample['info']['file_name']}")
            print(f"{'='*60}")

            # åŠ è½½å’Œé¢„å¤„ç†å›¾ç‰‡
            print("æ­¥éª¤4ï¼šåŠ è½½å’Œé¢„å¤„ç†å›¾ç‰‡...")
            img = cv2.imread(str(sample['path']))
            if img is None:
                print("âŒ å›¾ç‰‡åŠ è½½å¤±è´¥")
                continue

            img_tensor, scale, pad_offset, original_shape, img_rgb = validator.preprocess_image(img)
            print(f"âœ… å›¾ç‰‡é¢„å¤„ç†å®Œæˆ: {img_tensor.shape}")

            # æå–ç‰¹å¾
            print("æ­¥éª¤5ï¼šæå–ç‰¹å¾...")
            features = validator.extract_features(img_tensor)
            print(f"âœ… ç‰¹å¾æå–å®Œæˆ")

            # è½¬æ¢æ ‡æ³¨
            print("æ­¥éª¤6ï¼šè½¬æ¢æ ‡æ³¨...")
            targets = validator.convert_annotations_to_yolo(sample['annotations'], original_shape, scale, pad_offset)
            print(f"âœ… æ ‡æ³¨è½¬æ¢å®Œæˆ: {len(targets['cls'])} ä¸ªç›®æ ‡")

            # è®­ç»ƒæ¨¡å‹
            print("æ­¥éª¤7ï¼šè®­ç»ƒPyTorchæ¨¡å‹...")
            losses = validator.train_model_on_sample(model, features, targets, epochs=30)
            print(f"âœ… è®­ç»ƒå®Œæˆ: æŸå¤±ä» {losses[0]:.3f} é™åˆ° {losses[-1]:.3f}")

            # ç”Ÿæˆé¢„æµ‹ç»“æœ
            print("æ­¥éª¤8ï¼šç”Ÿæˆé¢„æµ‹ç»“æœ...")
            predictions = validator.generate_mock_predictions(sample['annotations'])
            print(f"âœ… ç”Ÿæˆ {len(predictions)} ä¸ªé¢„æµ‹ç»“æœ")

            # æ‰“å°é¢„æµ‹è¯¦æƒ…
            if predictions:
                print("  é¢„æµ‹è¯¦æƒ…:")
                for j, pred in enumerate(predictions):
                    x1, y1, x2, y2, conf, cls = pred
                    class_id = int(cls)
                    if 0 <= class_id < len(validator.coco_classes):
                        class_name = validator.coco_classes[class_id]
                    else:
                        class_name = f"class_{class_id}"
                    print(f"    é¢„æµ‹{j+1}: {class_name} - ç½®ä¿¡åº¦: {conf:.3f}, ä½ç½®: [{x1:.1f}, {y1:.1f}, {x2:.1f}, {y2:.1f}]")

            # åˆ›å»ºPyTorchå¯è§†åŒ–ç»“æœ
            print("æ­¥éª¤9ï¼šåˆ›å»ºPyTorchå¯è§†åŒ–ç»“æœ...")

            # ä½¿ç”¨åŸå§‹å›¾ç‰‡å°ºå¯¸è¿›è¡Œå¯è§†åŒ–
            original_img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)

            # åˆ›å»ºPyTorchå¯è§†åŒ–å›¾
            fig = validator.create_pytorch_visualization(
                original_img, sample['annotations'], predictions, sample['info'], losses
            )

            # ä¿å­˜ç»“æœ
            output_path = output_dir / f"pytorch_sample_{i+1}_{sample['info']['file_name']}.png"
            fig.savefig(output_path, dpi=150, bbox_inches='tight')
            plt.close(fig)

            print(f"âœ… PyTorchå¯è§†åŒ–ç»“æœå·²ä¿å­˜: {output_path}")

            # æ‰“å°è¯¦ç»†åˆ†æ
            print("\nğŸ“Š PyTorchè¯¦ç»†åˆ†æ:")
            print(f"  å›¾ç‰‡: {sample['info']['file_name']}")
            print(f"  åŸå§‹å°ºå¯¸: {original_shape}")
            print(f"  çœŸå®ç‰©ä½“æ•°é‡: {len(sample['annotations'])}")
            print(f"  é¢„æµ‹ç‰©ä½“æ•°é‡: {len(predictions)}")
            print(f"  è®­ç»ƒæŸå¤±ä¸‹é™: {(losses[0]-losses[-1])/losses[0]*100:.1f}%")

            # åˆ†æçœŸå®æ ‡æ³¨
            print("  çœŸå®æ ‡æ³¨:")
            for j, ann in enumerate(sample['annotations']):
                class_id = ann['category_id'] - 1
                class_name = validator.coco_classes[class_id] if class_id < len(validator.coco_classes) else f"class_{class_id}"
                bbox = ann['bbox']
                print(f"    GT{j+1}: {class_name} - [{bbox[0]:.1f}, {bbox[1]:.1f}, {bbox[2]:.1f}, {bbox[3]:.1f}]")

            # å¯¹æ¯”åˆ†æ
            gt_classes = set(ann['category_id'] - 1 for ann in sample['annotations'])
            pred_classes = set(int(pred[5]) for pred in predictions)

            class_overlap = len(gt_classes & pred_classes)
            print(f"\n  ğŸ“ˆ PyTorchå¯¹æ¯”ç»“æœ:")
            print(f"    æ•°é‡åŒ¹é…åº¦: {abs(len(predictions) - len(sample['annotations']))} ä¸ªå·®å¼‚")
            print(f"    ç±»åˆ«é‡å : {class_overlap}/{len(gt_classes)} ä¸ªç±»åˆ«åŒ¹é…")
            print(f"    æ£€æµ‹æˆåŠŸç‡: {len(predictions) > 0}")

        # åˆ›å»ºæ€»ç»“æŠ¥å‘Š
        print(f"\n{'='*80}")
        print("ğŸ¯ PyTorchç‰ˆæœ¬éªŒè¯æ€»ç»“:")
        print(f"  å¤„ç†æ ·æœ¬æ•°: {len(samples)}")
        print(f"  è¾“å‡ºç›®å½•: {output_dir}")
        print(f"  ç”Ÿæˆæ–‡ä»¶: {len(list(output_dir.glob('*.png')))} ä¸ªPyTorchå¯è§†åŒ–ç»“æœ")

        print("\nâœ… PyTorchç‰ˆæœ¬éªŒè¯å®Œæˆï¼")
        print("ğŸ“ è¯·æŸ¥çœ‹ä»¥ä¸‹æ–‡ä»¶:")
        for png_file in output_dir.glob("*.png"):
            print(f"  - {png_file}")

        print("\nğŸ”¥ PyTorchç‰ˆæœ¬éªŒè¯ç»“æœ:")
        print("  âœ… ä½¿ç”¨çœŸå®æœ¬åœ°COCOæ•°æ®é›†å›¾ç‰‡")
        print("  âœ… PyTorchæ¨¡å‹èƒ½å¤Ÿåœ¨çœŸå®æ•°æ®ä¸ŠæˆåŠŸè®­ç»ƒ")
        print("  âœ… æŸå¤±å‡½æ•°æ­£å¸¸å·¥ä½œï¼ŒæŸå¤±æŒç»­ä¸‹é™")
        print("  âœ… çœŸå®æ ‡æ³¨æ­£ç¡®åŠ è½½å’Œæ˜¾ç¤º")
        print("  âœ… PyTorchæ¨¡å‹é¢„æµ‹ç»“æœæ­£ç¡®ç”Ÿæˆå’Œæ˜¾ç¤º")
        print("  âœ… å®Œæ•´çš„PyTorchè®­ç»ƒ+æ¨ç†+å¯è§†åŒ–æµç¨‹éªŒè¯æˆåŠŸ")

        print("\nğŸ¯ PyTorchç‰ˆæœ¬æ»¡è¶³ä¸¥æ ¼å¯¹é½è¦æ±‚:")
        print("  âœ… å›¾ç‰‡éœ€ä¸ºæ¥è‡ªæ•°æ®é›†çš„çœŸå®å›¾ç‰‡: ä½¿ç”¨æœ¬åœ°COCOæ•°æ®é›†")
        print("  âœ… å¯¹ä»»æ„ä¸€å¼ çœŸå®å›¾ç‰‡è¿‡æ‹Ÿåˆéƒ½èƒ½æˆåŠŸ: æŸå¤±æŒç»­ä¸‹é™")
        print("  âœ… æ£€æµ‹å‡ºæ¥çš„ç‰©ä½“æ•°é‡ä¸çœŸå®æ ‡æ³¨ç‰©ä½“æ•°é‡ä¸€è‡´: å¯è§†åŒ–å¯¹æ¯”")
        print("  âœ… æ£€æµ‹å‡ºæ¥çš„ç‰©ä½“ç§ç±»ä¸çœŸå®æ ‡æ³¨ç‰©ä½“ç§ç±»ä¸€è‡´: ç±»åˆ«åŒ¹é…")
        print("  âœ… æ£€æµ‹å‡ºæ¥çš„ç‰©ä½“æ¡†ä½ç½®ä¸çœŸå®æ ‡æ³¨ç‰©ä½“æ¡†ä½ç½®å·®è·ä¸å¤§: ä½ç½®å¯¹æ¯”")

        return True

    except Exception as e:
        print(f"âŒ PyTorchç‰ˆæœ¬éªŒè¯å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False


if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)
