#!/usr/bin/env python3
"""
æ·±å…¥è°ƒè¯•æ•°æ®åŠ è½½é—®é¢˜
æ£€æŸ¥å›¾åƒå’Œæ ‡æ³¨æ•°æ®æ˜¯å¦æ­£ç¡®è¯»å–
"""

import os
import sys
import cv2
import numpy as np
import jittor as jt
from pathlib import Path
import matplotlib.pyplot as plt

# æ·»åŠ è·¯å¾„
sys.path.append('.')
sys.path.append('./yolov6')

from yolov6.data.data_augment import letterbox

# VOCæ•°æ®é›†ç±»åˆ«åç§°
VOC_CLASSES = [
    'aeroplane', 'bicycle', 'bird', 'boat', 'bottle', 'bus', 'car', 'cat', 'chair', 'cow',
    'diningtable', 'dog', 'horse', 'motorbike', 'person', 'pottedplant', 'sheep', 'sofa', 'train', 'tvmonitor'
]

def debug_data_loading():
    """æ·±å…¥è°ƒè¯•æ•°æ®åŠ è½½é—®é¢˜"""
    print(f"ğŸ” æ·±å…¥è°ƒè¯•æ•°æ®åŠ è½½é—®é¢˜")
    print("=" * 80)
    
    # æ•°æ®è·¯å¾„
    label_file = "/home/kyc/project/GOLD-YOLO/2008_001420.txt"
    img_path = "/home/kyc/project/GOLD-YOLO/2008_001420.jpg"
    
    print(f"ğŸ“ æ•°æ®è·¯å¾„:")
    print(f"   å›¾åƒ: {img_path}")
    print(f"   æ ‡æ³¨: {label_file}")
    print(f"   å›¾åƒå­˜åœ¨: {os.path.exists(img_path)}")
    print(f"   æ ‡æ³¨å­˜åœ¨: {os.path.exists(label_file)}")
    
    # 1. è¯»å–å¹¶åˆ†æåŸå§‹æ ‡æ³¨
    print(f"\nğŸ“‹ åŸå§‹æ ‡æ³¨åˆ†æ:")
    annotations = []
    with open(label_file, 'r') as f:
        for i, line in enumerate(f):
            line = line.strip()
            if line:
                parts = line.split()
                print(f"   ç¬¬{i+1}è¡Œ: {parts}")
                if len(parts) >= 5:
                    cls_id = int(parts[0])
                    x_center = float(parts[1])
                    y_center = float(parts[2])
                    width = float(parts[3])
                    height = float(parts[4])
                    annotations.append([cls_id, x_center, y_center, width, height])
                    print(f"     è§£æ: ç±»åˆ«={VOC_CLASSES[cls_id]}, ä¸­å¿ƒ=({x_center:.3f},{y_center:.3f}), å°ºå¯¸=({width:.3f},{height:.3f})")
    
    print(f"   æ€»æ ‡æ³¨æ•°: {len(annotations)}")
    
    # 2. è¯»å–å¹¶åˆ†æåŸå§‹å›¾åƒ
    print(f"\nğŸ–¼ï¸ åŸå§‹å›¾åƒåˆ†æ:")
    original_img = cv2.imread(img_path)
    if original_img is None:
        print(f"âŒ æ— æ³•è¯»å–å›¾åƒ: {img_path}")
        return
    
    img_height, img_width = original_img.shape[:2]
    print(f"   å›¾åƒå°ºå¯¸: {img_width} x {img_height}")
    print(f"   å›¾åƒé€šé“: {original_img.shape[2]}")
    print(f"   å›¾åƒç±»å‹: {original_img.dtype}")
    print(f"   åƒç´ èŒƒå›´: [{original_img.min()}, {original_img.max()}]")
    
    # 3. è½¬æ¢æ ‡æ³¨ä¸ºåƒç´ åæ ‡å¹¶éªŒè¯
    print(f"\nğŸ“ æ ‡æ³¨åæ ‡è½¬æ¢:")
    gt_boxes = []
    gt_classes = []
    
    for i, ann in enumerate(annotations):
        cls_id, x_center, y_center, width, height = ann
        
        # è½¬æ¢ä¸ºåƒç´ åæ ‡
        x1 = int((x_center - width/2) * img_width)
        y1 = int((y_center - height/2) * img_height)
        x2 = int((x_center + width/2) * img_width)
        y2 = int((y_center + height/2) * img_height)
        
        gt_boxes.append([x1, y1, x2, y2])
        gt_classes.append(cls_id)
        
        print(f"   æ ‡æ³¨{i+1}: {VOC_CLASSES[cls_id]}")
        print(f"     å½’ä¸€åŒ–: ä¸­å¿ƒ({x_center:.3f},{y_center:.3f}), å°ºå¯¸({width:.3f},{height:.3f})")
        print(f"     åƒç´ åæ ‡: ({x1},{y1}) -> ({x2},{y2})")
        print(f"     æ¡†å°ºå¯¸: {x2-x1} x {y2-y1}")
        
        # éªŒè¯åæ ‡æ˜¯å¦åˆç†
        if x1 < 0 or y1 < 0 or x2 >= img_width or y2 >= img_height:
            print(f"     âš ï¸ åæ ‡è¶…å‡ºå›¾åƒè¾¹ç•Œï¼")
        if x2 <= x1 or y2 <= y1:
            print(f"     âŒ æ— æ•ˆçš„æ¡†å°ºå¯¸ï¼")
    
    # 4. å›¾åƒé¢„å¤„ç†åˆ†æ
    print(f"\nğŸ”„ å›¾åƒé¢„å¤„ç†åˆ†æ:")
    img_resized = letterbox(original_img, new_shape=640, stride=32, auto=False)[0]
    print(f"   é¢„å¤„ç†åå°ºå¯¸: {img_resized.shape}")
    
    # è½¬æ¢ä¸ºæ¨¡å‹è¾“å…¥æ ¼å¼
    img_tensor_input = img_resized.transpose((2, 0, 1))[::-1]  # HWC -> CHW, BGR -> RGB
    img_tensor_input = np.ascontiguousarray(img_tensor_input)
    img_tensor_input = img_tensor_input.astype(np.float32) / 255.0
    
    print(f"   å¼ é‡å½¢çŠ¶: {img_tensor_input.shape}")
    print(f"   å¼ é‡ç±»å‹: {img_tensor_input.dtype}")
    print(f"   åƒç´ èŒƒå›´: [{img_tensor_input.min():.3f}, {img_tensor_input.max():.3f}]")
    
    img_tensor = jt.array(img_tensor_input).unsqueeze(0)
    print(f"   Jittorå¼ é‡å½¢çŠ¶: {img_tensor.shape}")
    
    # 5. æ ‡ç­¾å¼ é‡åˆ†æ
    print(f"\nğŸ·ï¸ æ ‡ç­¾å¼ é‡åˆ†æ:")
    targets = []
    for ann in annotations:
        cls_id, x_center, y_center, width, height = ann
        targets.append([0, cls_id, x_center, y_center, width, height])  # [batch_idx, cls, x, y, w, h]
    
    targets_tensor = jt.array(targets, dtype=jt.float32).unsqueeze(0)
    print(f"   æ ‡ç­¾å¼ é‡å½¢çŠ¶: {targets_tensor.shape}")
    print(f"   æ ‡ç­¾å¼ é‡å†…å®¹:")
    for i, target in enumerate(targets):
        print(f"     ç›®æ ‡{i+1}: batch={target[0]}, cls={target[1]}({VOC_CLASSES[int(target[1])]}), åæ ‡=({target[2]:.3f},{target[3]:.3f},{target[4]:.3f},{target[5]:.3f})")
    
    # 6. åˆ›å»ºå¯è§†åŒ–å›¾ç‰‡éªŒè¯æ•°æ®æ­£ç¡®æ€§
    print(f"\nğŸ“¸ åˆ›å»ºå¯è§†åŒ–éªŒè¯:")
    save_dir = Path("runs/debug_data_loading")
    save_dir.mkdir(parents=True, exist_ok=True)
    
    # åŸå§‹å›¾åƒ + æ ‡æ³¨æ¡†
    img_vis = original_img.copy()
    for i, (gt_box, gt_cls) in enumerate(zip(gt_boxes, gt_classes)):
        x1, y1, x2, y2 = gt_box
        cv2.rectangle(img_vis, (x1, y1), (x2, y2), (0, 255, 0), 2)
        cv2.putText(img_vis, f'{VOC_CLASSES[gt_cls]}', (x1, y1-10), 
                   cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)
        cv2.putText(img_vis, f'{i+1}', (x1+5, y1+20), 
                   cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)
    
    cv2.putText(img_vis, f'Original: {img_width}x{img_height}', (10, 30), 
               cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)
    
    original_vis_path = save_dir / 'original_with_annotations.jpg'
    cv2.imwrite(str(original_vis_path), img_vis)
    print(f"   åŸå§‹å›¾åƒ+æ ‡æ³¨: {original_vis_path}")
    
    # é¢„å¤„ç†åå›¾åƒ
    img_resized_vis = img_resized.copy()
    # éœ€è¦é‡æ–°è®¡ç®—é¢„å¤„ç†åçš„åæ ‡
    scale = min(640/img_width, 640/img_height)
    new_width = int(img_width * scale)
    new_height = int(img_height * scale)
    pad_x = (640 - new_width) // 2
    pad_y = (640 - new_height) // 2
    
    for i, (gt_box, gt_cls) in enumerate(zip(gt_boxes, gt_classes)):
        x1, y1, x2, y2 = gt_box
        # ç¼©æ”¾å’Œå¡«å……
        x1_new = int(x1 * scale + pad_x)
        y1_new = int(y1 * scale + pad_y)
        x2_new = int(x2 * scale + pad_x)
        y2_new = int(y2 * scale + pad_y)
        
        cv2.rectangle(img_resized_vis, (x1_new, y1_new), (x2_new, y2_new), (0, 255, 0), 2)
        cv2.putText(img_resized_vis, f'{VOC_CLASSES[gt_cls]}', (x1_new, y1_new-10), 
                   cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 1)
    
    cv2.putText(img_resized_vis, f'Resized: 640x640', (10, 30), 
               cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)
    
    resized_vis_path = save_dir / 'resized_with_annotations.jpg'
    cv2.imwrite(str(resized_vis_path), img_resized_vis)
    print(f"   é¢„å¤„ç†å›¾åƒ+æ ‡æ³¨: {resized_vis_path}")
    
    # 7. æ•°æ®å®Œæ•´æ€§æ£€æŸ¥
    print(f"\nâœ… æ•°æ®å®Œæ•´æ€§æ£€æŸ¥:")
    
    # æ£€æŸ¥æ ‡æ³¨æ˜¯å¦åˆç†
    valid_annotations = 0
    for i, (gt_box, gt_cls) in enumerate(zip(gt_boxes, gt_classes)):
        x1, y1, x2, y2 = gt_box
        if 0 <= x1 < x2 < img_width and 0 <= y1 < y2 < img_height:
            valid_annotations += 1
        else:
            print(f"   âŒ æ ‡æ³¨{i+1}åæ ‡æ— æ•ˆ: ({x1},{y1}) -> ({x2},{y2})")
    
    print(f"   æœ‰æ•ˆæ ‡æ³¨: {valid_annotations}/{len(annotations)}")
    
    # æ£€æŸ¥ç±»åˆ«æ˜¯å¦åˆç†
    valid_classes = 0
    for cls_id in gt_classes:
        if 0 <= cls_id < len(VOC_CLASSES):
            valid_classes += 1
        else:
            print(f"   âŒ æ— æ•ˆç±»åˆ«ID: {cls_id}")
    
    print(f"   æœ‰æ•ˆç±»åˆ«: {valid_classes}/{len(gt_classes)}")
    
    # æ£€æŸ¥å›¾åƒæ˜¯å¦æ­£å¸¸
    if img_tensor.shape == (1, 3, 640, 640):
        print(f"   âœ… å›¾åƒå¼ é‡å½¢çŠ¶æ­£ç¡®")
    else:
        print(f"   âŒ å›¾åƒå¼ é‡å½¢çŠ¶é”™è¯¯: {img_tensor.shape}")
    
    if 0.0 <= img_tensor.min() and img_tensor.max() <= 1.0:
        print(f"   âœ… å›¾åƒåƒç´ èŒƒå›´æ­£ç¡®")
    else:
        print(f"   âŒ å›¾åƒåƒç´ èŒƒå›´é”™è¯¯: [{img_tensor.min():.3f}, {img_tensor.max():.3f}]")
    
    # æ£€æŸ¥æ ‡ç­¾å¼ é‡
    if targets_tensor.shape[0] == 1 and targets_tensor.shape[2] == 6:
        print(f"   âœ… æ ‡ç­¾å¼ é‡å½¢çŠ¶æ­£ç¡®")
    else:
        print(f"   âŒ æ ‡ç­¾å¼ é‡å½¢çŠ¶é”™è¯¯: {targets_tensor.shape}")
    
    print(f"\nğŸ“Š æ•°æ®åŠ è½½è°ƒè¯•å®Œæˆ!")
    print(f"   å›¾åƒ: {img_width}x{img_height} -> 640x640")
    print(f"   æ ‡æ³¨: {len(annotations)}ä¸ªç›®æ ‡")
    print(f"   ç±»åˆ«: {set(VOC_CLASSES[cls] for cls in gt_classes)}")
    print(f"   å¯è§†åŒ–å›¾ç‰‡å·²ä¿å­˜åˆ°: {save_dir}")
    
    return {
        'img_tensor': img_tensor,
        'targets_tensor': targets_tensor,
        'gt_boxes': gt_boxes,
        'gt_classes': gt_classes,
        'original_img': original_img,
        'img_width': img_width,
        'img_height': img_height
    }

def main():
    print("ğŸ” æ·±å…¥è°ƒè¯•æ•°æ®åŠ è½½é—®é¢˜")
    print("=" * 80)
    
    data_info = debug_data_loading()
    
    if data_info:
        print(f"\nâœ… æ•°æ®åŠ è½½è°ƒè¯•æˆåŠŸ!")
        print(f"   å›¾åƒå¼ é‡: {data_info['img_tensor'].shape}")
        print(f"   æ ‡ç­¾å¼ é‡: {data_info['targets_tensor'].shape}")
        print(f"   çœŸå®æ¡†æ•°é‡: {len(data_info['gt_boxes'])}")
        print(f"   ç±»åˆ«æ•°é‡: {len(set(data_info['gt_classes']))}")
    else:
        print(f"\nâŒ æ•°æ®åŠ è½½è°ƒè¯•å¤±è´¥!")

if __name__ == "__main__":
    main()
