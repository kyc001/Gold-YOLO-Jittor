#!/usr/bin/env python3
# -*- coding:utf-8 -*-
"""
åˆ›å»ºè§„èŒƒçš„æ•°æ®é›†åˆ’åˆ†
æ–°èŠ½ç¬¬äºŒé˜¶æ®µï¼šè®­ç»ƒé›†/æµ‹è¯•é›†åˆ†ç¦»ï¼Œç»Ÿä¸€è¯„ä¼°
"""

import os
import json
import random
from pathlib import Path
from PIL import Image
from tqdm import tqdm

class DatasetSplitter:
    """æ•°æ®é›†åˆ’åˆ†å™¨"""
    
    def __init__(self, data_root, train_ratio=0.8, seed=42):
        self.data_root = Path(data_root)
        self.train_ratio = train_ratio
        self.seed = seed
        
        # è®¾ç½®éšæœºç§å­
        random.seed(seed)
        
        # è·¯å¾„é…ç½®
        self.img_dir = self.data_root / "images"
        self.ann_file = self.data_root / "annotations" / "instances_val2017.json"
        
        # è¾“å‡ºè·¯å¾„
        self.output_dir = self.data_root / "splits"
        self.output_dir.mkdir(exist_ok=True)
        
        print(f"ğŸ¯ æ•°æ®é›†åˆ’åˆ†å™¨")
        print(f"   æ•°æ®æ ¹ç›®å½•: {self.data_root}")
        print(f"   è®­ç»ƒæ¯”ä¾‹: {self.train_ratio}")
        print(f"   éšæœºç§å­: {self.seed}")
    
    def load_and_validate_data(self, max_images=1000):
        """åŠ è½½å¹¶éªŒè¯æ•°æ®"""
        print(f"\nğŸ“Š åŠ è½½COCOæ•°æ®...")
        
        with open(self.ann_file, 'r') as f:
            coco_data = json.load(f)
        
        print(f"âœ… åŸå§‹æ•°æ®: {len(coco_data['images'])}å¼ å›¾ç‰‡, {len(coco_data['annotations'])}ä¸ªæ ‡æ³¨")
        
        # æŒ‰å›¾ç‰‡IDç»„ç»‡æ ‡æ³¨
        image_annotations = {}
        for ann in coco_data['annotations']:
            img_id = ann['image_id']
            if img_id not in image_annotations:
                image_annotations[img_id] = []
            image_annotations[img_id].append(ann)
        
        # ç­›é€‰æœ‰æ•ˆå›¾ç‰‡
        valid_images = []
        print("éªŒè¯å›¾ç‰‡å®Œæ•´æ€§...")
        
        for img_info in tqdm(coco_data['images'][:max_images*2]):  # å¤šå–ä¸€äº›ä»¥é˜²ä¸å¤Ÿ
            img_path = self.img_dir / img_info['file_name']
            img_id = img_info['id']
            
            # æ£€æŸ¥æ–‡ä»¶å­˜åœ¨ä¸”æœ‰æ ‡æ³¨
            if img_path.exists() and img_id in image_annotations:
                try:
                    # éªŒè¯å›¾ç‰‡å¯è¯»
                    img = Image.open(img_path)
                    img.verify()
                    
                    valid_images.append({
                        'info': img_info,
                        'annotations': image_annotations[img_id],
                        'annotation_count': len(image_annotations[img_id])
                    })
                    
                    # è¾¾åˆ°ç›®æ ‡æ•°é‡å°±åœæ­¢
                    if len(valid_images) >= max_images:
                        break
                        
                except:
                    continue
        
        print(f"âœ… æœ‰æ•ˆå›¾ç‰‡: {len(valid_images)}")
        
        # æŒ‰æ ‡æ³¨æ•°é‡æ’åºï¼Œä¼˜å…ˆé€‰æ‹©æ ‡æ³¨ä¸°å¯Œçš„å›¾ç‰‡
        valid_images.sort(key=lambda x: x['annotation_count'], reverse=True)
        
        return valid_images[:max_images], coco_data
    
    def split_dataset(self, valid_images, coco_data):
        """åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†"""
        print(f"\nğŸ”„ åˆ’åˆ†æ•°æ®é›†...")
        
        # éšæœºæ‰“ä¹±
        random.shuffle(valid_images)
        
        # è®¡ç®—åˆ’åˆ†ç‚¹
        split_point = int(len(valid_images) * self.train_ratio)
        
        train_images = valid_images[:split_point]
        test_images = valid_images[split_point:]
        
        print(f"âœ… è®­ç»ƒé›†: {len(train_images)}å¼ å›¾ç‰‡")
        print(f"âœ… æµ‹è¯•é›†: {len(test_images)}å¼ å›¾ç‰‡")
        
        # åˆ›å»ºè®­ç»ƒé›†å’Œæµ‹è¯•é›†æ•°æ®
        train_data = self._create_subset_data(train_images, coco_data, "train")
        test_data = self._create_subset_data(test_images, coco_data, "test")
        
        return train_data, test_data
    
    def _create_subset_data(self, images, coco_data, split_name):
        """åˆ›å»ºå­é›†æ•°æ®"""
        # æ”¶é›†å›¾ç‰‡ä¿¡æ¯
        subset_images = [img['info'] for img in images]
        selected_img_ids = {img['info']['id'] for img in images}
        
        # æ”¶é›†å¯¹åº”çš„æ ‡æ³¨
        subset_annotations = []
        for ann in coco_data['annotations']:
            if ann['image_id'] in selected_img_ids:
                subset_annotations.append(ann)
        
        # åˆ›å»ºå­é›†æ•°æ®ç»“æ„
        subset_data = {
            'info': coco_data['info'],
            'licenses': coco_data['licenses'],
            'categories': coco_data['categories'],
            'images': subset_images,
            'annotations': subset_annotations
        }
        
        # ç»Ÿè®¡ä¿¡æ¯
        total_annotations = len(subset_annotations)
        avg_annotations = total_annotations / len(subset_images) if subset_images else 0
        
        # ç»Ÿè®¡ç±»åˆ«åˆ†å¸ƒ
        category_counts = {}
        for ann in subset_annotations:
            cat_id = ann['category_id']
            category_counts[cat_id] = category_counts.get(cat_id, 0) + 1
        
        print(f"   {split_name}é›†ç»Ÿè®¡:")
        print(f"     å›¾ç‰‡æ•°: {len(subset_images)}")
        print(f"     æ ‡æ³¨æ•°: {total_annotations}")
        print(f"     å¹³å‡æ ‡æ³¨/å›¾ç‰‡: {avg_annotations:.1f}")
        print(f"     ä½¿ç”¨ç±»åˆ«æ•°: {len(category_counts)}")
        
        return subset_data
    
    def save_splits(self, train_data, test_data):
        """ä¿å­˜æ•°æ®é›†åˆ’åˆ†"""
        print(f"\nğŸ’¾ ä¿å­˜æ•°æ®é›†åˆ’åˆ†...")
        
        # ä¿å­˜è®­ç»ƒé›†
        train_file = self.output_dir / "train_annotations.json"
        with open(train_file, 'w') as f:
            json.dump(train_data, f)
        print(f"âœ… è®­ç»ƒé›†å·²ä¿å­˜: {train_file}")
        
        # ä¿å­˜æµ‹è¯•é›†
        test_file = self.output_dir / "test_annotations.json"
        with open(test_file, 'w') as f:
            json.dump(test_data, f)
        print(f"âœ… æµ‹è¯•é›†å·²ä¿å­˜: {test_file}")
        
        # ä¿å­˜æ•°æ®é›†ä¿¡æ¯
        dataset_info = {
            'dataset_name': 'COCO2017_Val_Split',
            'total_images': len(train_data['images']) + len(test_data['images']),
            'train_images': len(train_data['images']),
            'test_images': len(test_data['images']),
            'train_annotations': len(train_data['annotations']),
            'test_annotations': len(test_data['annotations']),
            'train_ratio': self.train_ratio,
            'test_ratio': 1 - self.train_ratio,
            'random_seed': self.seed,
            'categories': len(train_data['categories']),
            'split_date': '2024-12-19'
        }
        
        info_file = self.output_dir / "dataset_info.json"
        with open(info_file, 'w') as f:
            json.dump(dataset_info, f, indent=2)
        print(f"âœ… æ•°æ®é›†ä¿¡æ¯å·²ä¿å­˜: {info_file}")
        
        # åˆ›å»ºå›¾ç‰‡åˆ—è¡¨æ–‡ä»¶
        train_list_file = self.output_dir / "train_images.txt"
        with open(train_list_file, 'w') as f:
            for img in train_data['images']:
                f.write(f"{img['file_name']}\n")
        
        test_list_file = self.output_dir / "test_images.txt"
        with open(test_list_file, 'w') as f:
            for img in test_data['images']:
                f.write(f"{img['file_name']}\n")
        
        print(f"âœ… å›¾ç‰‡åˆ—è¡¨å·²ä¿å­˜: {train_list_file}, {test_list_file}")
        
        return {
            'train_file': train_file,
            'test_file': test_file,
            'info_file': info_file,
            'train_list': train_list_file,
            'test_list': test_list_file
        }
    
    def create_splits(self, max_images=1000):
        """åˆ›å»ºå®Œæ•´çš„æ•°æ®é›†åˆ’åˆ†"""
        print("ğŸ¯ å¼€å§‹åˆ›å»ºæ•°æ®é›†åˆ’åˆ†...")
        print("=" * 60)
        
        # 1. åŠ è½½å’ŒéªŒè¯æ•°æ®
        valid_images, coco_data = self.load_and_validate_data(max_images)
        
        if len(valid_images) < 100:
            print(f"âŒ æœ‰æ•ˆå›¾ç‰‡å¤ªå°‘: {len(valid_images)}")
            return None
        
        # 2. åˆ’åˆ†æ•°æ®é›†
        train_data, test_data = self.split_dataset(valid_images, coco_data)
        
        # 3. ä¿å­˜æ•°æ®é›†
        files = self.save_splits(train_data, test_data)
        
        print("=" * 60)
        print("âœ… æ•°æ®é›†åˆ’åˆ†å®Œæˆï¼")
        print(f"ğŸ“ è¾“å‡ºç›®å½•: {self.output_dir}")
        
        return {
            'files': files,
            'train_data': train_data,
            'test_data': test_data,
            'stats': {
                'total_images': len(valid_images),
                'train_images': len(train_data['images']),
                'test_images': len(test_data['images']),
                'train_annotations': len(train_data['annotations']),
                'test_annotations': len(test_data['annotations'])
            }
        }


def main():
    """ä¸»å‡½æ•°"""
    # æ•°æ®é›†é…ç½®
    data_root = "/home/kyc/project/GOLD-YOLO/data/coco2017_val"
    max_images = 1000  # æ€»å›¾ç‰‡æ•°
    train_ratio = 0.8  # è®­ç»ƒé›†æ¯”ä¾‹
    
    print("ğŸ¯ Gold-YOLO æ•°æ®é›†åˆ’åˆ†")
    print("æ–°èŠ½ç¬¬äºŒé˜¶æ®µï¼šè§„èŒƒçš„è®­ç»ƒé›†/æµ‹è¯•é›†åˆ†ç¦»")
    print("=" * 60)
    print(f"ğŸ“Š é…ç½®:")
    print(f"   æ•°æ®æ ¹ç›®å½•: {data_root}")
    print(f"   æ€»å›¾ç‰‡æ•°: {max_images}")
    print(f"   è®­ç»ƒé›†æ¯”ä¾‹: {train_ratio}")
    print(f"   æµ‹è¯•é›†æ¯”ä¾‹: {1-train_ratio}")
    
    # åˆ›å»ºæ•°æ®é›†åˆ’åˆ†å™¨
    splitter = DatasetSplitter(
        data_root=data_root,
        train_ratio=train_ratio,
        seed=42
    )
    
    # åˆ›å»ºæ•°æ®é›†åˆ’åˆ†
    result = splitter.create_splits(max_images)
    
    if result:
        # æ˜¾ç¤ºæœ€ç»ˆç»Ÿè®¡
        stats = result['stats']
        
        print("\nğŸ“ˆ æœ€ç»ˆç»Ÿè®¡:")
        print(f"   æ€»å›¾ç‰‡: {stats['total_images']}")
        print(f"   è®­ç»ƒé›†: {stats['train_images']}å¼ å›¾ç‰‡, {stats['train_annotations']}ä¸ªæ ‡æ³¨")
        print(f"   æµ‹è¯•é›†: {stats['test_images']}å¼ å›¾ç‰‡, {stats['test_annotations']}ä¸ªæ ‡æ³¨")
        
        print("\nğŸš€ æ•°æ®é›†å‡†å¤‡å®Œæˆï¼Œå¯ä»¥å¼€å§‹è®­ç»ƒå’Œè¯„ä¼°ï¼")
        print("\nğŸ“‹ ä½¿ç”¨æ–¹æ³•:")
        print("   è®­ç»ƒ: python fixed_jittor_train.py --train-file splits/train_annotations.json")
        print("   æµ‹è¯•: python evaluate_model.py --test-file splits/test_annotations.json")
    else:
        print("âŒ æ•°æ®é›†åˆ’åˆ†å¤±è´¥")


if __name__ == "__main__":
    main()
