#!/usr/bin/env python3
"""
å®Œç¾çš„è¿‡æ‹Ÿåˆå¯è§†åŒ–è„šæœ¬
æ·±å…¥ä¿®å¤æ‰€æœ‰é—®é¢˜ï¼Œå®Œç¾å±•ç¤ºè¿‡æ‹Ÿåˆæ¨ç†ç»“æœ
"""

import os
import sys
import cv2
import numpy as np
import jittor as jt
import time
from pathlib import Path

# æ·»åŠ è·¯å¾„
sys.path.append('.')
sys.path.append('./yolov6')

from models.perfect_gold_yolo import create_perfect_gold_yolo_model
from yolov6.data.data_augment import letterbox
from yolov6.models.losses import ComputeLoss
from yolov6.utils.nms import non_max_suppression

# VOCæ•°æ®é›†ç±»åˆ«åç§°
VOC_CLASSES = [
    'aeroplane', 'bicycle', 'bird', 'boat', 'bottle', 'bus', 'car', 'cat', 'chair', 'cow',
    'diningtable', 'dog', 'horse', 'motorbike', 'person', 'pottedplant', 'sheep', 'sofa', 'train', 'tvmonitor'
]

# ç±»åˆ«é¢œè‰² - æœŸæœ›ç±»åˆ«ä½¿ç”¨ç‰¹æ®Šé¢œè‰²
COLORS = {
    'dog': (0, 255, 0),      # ç»¿è‰² - ä¸»è¦ç›®æ ‡
    'person': (255, 0, 0),   # è“è‰²
    'boat': (0, 0, 255),     # çº¢è‰²
    'default': (128, 128, 128)  # ç°è‰² - å…¶ä»–ç±»åˆ«
}

def pytorch_exact_initialization(model):
    """å®Œå…¨ç…§æŠ„PyTorchç‰ˆæœ¬çš„åˆå§‹åŒ–"""
    for name, module in model.named_modules():
        if hasattr(module, 'initialize_biases'):
            module.initialize_biases()
            break
    return model

def draw_detection_box(img, box, label, confidence, color, is_expected=False):
    """ç»˜åˆ¶æ£€æµ‹æ¡† - æœŸæœ›ç±»åˆ«ä½¿ç”¨ç‰¹æ®Šæ ·å¼"""
    x1, y1, x2, y2 = map(int, box)
    
    # æœŸæœ›ç±»åˆ«ä½¿ç”¨ç²—çº¿æ¡
    thickness = 3 if is_expected else 2
    
    # ç»˜åˆ¶æ£€æµ‹æ¡†
    cv2.rectangle(img, (x1, y1), (x2, y2), color, thickness)
    
    # å‡†å¤‡æ ‡ç­¾æ–‡æœ¬
    status = "âœ…" if is_expected else "âŒ"
    label_text = f'{status}{label}: {confidence:.3f}'
    
    # è®¡ç®—æ–‡æœ¬å¤§å°
    font = cv2.FONT_HERSHEY_SIMPLEX
    font_scale = 0.7 if is_expected else 0.6
    text_thickness = 2
    (text_width, text_height), _ = cv2.getTextSize(label_text, font, font_scale, text_thickness)
    
    # ç»˜åˆ¶æ ‡ç­¾èƒŒæ™¯
    bg_color = color if is_expected else (64, 64, 64)
    cv2.rectangle(img, (x1, y1 - text_height - 10), (x1 + text_width, y1), bg_color, -1)
    
    # ç»˜åˆ¶æ ‡ç­¾æ–‡æœ¬
    text_color = (255, 255, 255)
    cv2.putText(img, label_text, (x1, y1 - 5), font, font_scale, text_color, text_thickness)

def draw_ground_truth_box(img, box, label, color=(0, 255, 255)):
    """ç»˜åˆ¶çœŸå®æ ‡æ³¨æ¡†"""
    x1, y1, x2, y2 = map(int, box)
    
    # ç»˜åˆ¶çœŸå®æ¡†ï¼ˆè™šçº¿æ•ˆæœï¼‰
    cv2.rectangle(img, (x1, y1), (x2, y2), color, 2)
    
    # ç»˜åˆ¶æ ‡ç­¾
    label_text = f'GT: {label}'
    font = cv2.FONT_HERSHEY_SIMPLEX
    font_scale = 0.5
    thickness = 1
    (text_width, text_height), _ = cv2.getTextSize(label_text, font, font_scale, thickness)
    
    cv2.rectangle(img, (x1, y2), (x1 + text_width, y2 + text_height + 5), color, -1)
    cv2.putText(img, label_text, (x1, y2 + text_height), font, font_scale, (0, 0, 0), thickness)

def perfect_overfit_visualization():
    """å®Œç¾çš„è¿‡æ‹Ÿåˆå¯è§†åŒ–"""
    print(f"ğŸ¨ å®Œç¾çš„è¿‡æ‹Ÿåˆå¯è§†åŒ–")
    print("=" * 50)
    
    # å‡†å¤‡æ•°æ®
    label_file = "/home/kyc/project/GOLD-YOLO/2008_001420.txt"
    img_path = "/home/kyc/project/GOLD-YOLO/2008_001420.jpg"
    
    # è¯»å–çœŸå®æ ‡æ³¨
    annotations = []
    with open(label_file, 'r') as f:
        for line in f:
            line = line.strip()
            if line:
                parts = line.split()
                if len(parts) >= 5:
                    cls_id = int(parts[0])
                    x_center = float(parts[1])
                    y_center = float(parts[2])
                    width = float(parts[3])
                    height = float(parts[4])
                    annotations.append([cls_id, x_center, y_center, width, height])
    
    target_counts = {}
    expected_classes = set()
    for ann in annotations:
        cls_name = VOC_CLASSES[ann[0]]
        target_counts[cls_name] = target_counts.get(cls_name, 0) + 1
        expected_classes.add(cls_name)
    
    print(f"ğŸ“‹ æœŸæœ›æ£€æµ‹ç»“æœ: {target_counts}")
    print(f"   æœŸæœ›ç±»åˆ«: {expected_classes}")
    
    # è¯»å–åŸå§‹å›¾åƒ
    original_img = cv2.imread(img_path)
    img_height, img_width = original_img.shape[:2]
    
    # å‡†å¤‡è¾“å…¥
    img = letterbox(original_img, new_shape=640, stride=32, auto=False)[0]
    img_tensor_input = img.transpose((2, 0, 1))[::-1]
    img_tensor_input = np.ascontiguousarray(img_tensor_input)
    img_tensor_input = img_tensor_input.astype(np.float32) / 255.0
    img_tensor = jt.array(img_tensor_input).unsqueeze(0)
    
    # å‡†å¤‡æ ‡ç­¾
    targets = []
    for ann in annotations:
        cls_id, x_center, y_center, width, height = ann
        targets.append([0, cls_id, x_center, y_center, width, height])
    targets_tensor = jt.array(targets, dtype=jt.float32).unsqueeze(0)
    
    # åˆ›å»ºæ¨¡å‹
    model = create_perfect_gold_yolo_model()
    model = pytorch_exact_initialization(model)
    model.train()
    
    # åˆ›å»ºæŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨
    loss_fn = ComputeLoss(
        num_classes=20,
        ori_img_size=640,
        warmup_epoch=4,
        use_dfl=False,
        reg_max=0,
        iou_type='giou',
        loss_weight={'class': 1.0, 'iou': 2.5, 'dfl': 0.5}
    )
    optimizer = jt.optim.AdamW(model.parameters(), lr=0.05)
    
    # åˆ›å»ºä¿å­˜ç›®å½•
    save_dir = Path("runs/perfect_overfit_visualization")
    save_dir.mkdir(parents=True, exist_ok=True)
    
    print(f"\nğŸš€ å®Œç¾è¿‡æ‹Ÿåˆè®­ç»ƒ (100è½®):")
    
    # è®­ç»ƒå¾ªç¯
    best_species_accuracy = 0.0
    best_epoch = 0
    
    for epoch in range(100):
        # å‰å‘ä¼ æ’­
        outputs = model(img_tensor)
        
        # è®¡ç®—æŸå¤±
        loss, loss_items = loss_fn(outputs, targets_tensor, epoch_num=epoch+1, step_num=1)
        
        # ä¼˜åŒ–
        optimizer.step(loss)
        
        epoch_loss = float(loss.numpy())
        
        # æ¯20è½®å¯è§†åŒ–ä¸€æ¬¡
        if (epoch + 1) % 20 == 0:
            print(f"\n   Epoch {epoch+1}: Loss {epoch_loss:.6f}")
            
            # æ¨ç†æ¨¡å¼
            model.eval()
            with jt.no_grad():
                # è·å–è®­ç»ƒæ¨¡å¼çš„è¾“å‡ºç”¨äºåˆ†æ
                train_outputs = model(img_tensor)
                
                # åˆ†ææœŸæœ›ç±»åˆ«çš„å­¦ä¹ æƒ…å†µ
                if isinstance(train_outputs, tuple):
                    # æ¨ç†æ¨¡å¼è¾“å‡º
                    pred_scores = train_outputs[1]  # [1, 8400, 20]
                else:
                    # è®­ç»ƒæ¨¡å¼è¾“å‡º
                    pred_scores = train_outputs[..., 5:]  # [1, 8400, 20]
                
                print(f"     æœŸæœ›ç±»åˆ«å­¦ä¹ æƒ…å†µ:")
                expected_class_ids = [3, 11, 14]  # boat, dog, person
                for cls_id in expected_class_ids:
                    cls_scores = pred_scores[0, :, cls_id]
                    max_score = float(cls_scores.max())
                    mean_score = float(cls_scores.mean())
                    nonzero_count = int((cls_scores > 0.001).sum())
                    cls_name = VOC_CLASSES[cls_id]
                    print(f"       {cls_name}(ç±»åˆ«{cls_id}): æœ€å¤§{max_score:.6f}, å¹³å‡{mean_score:.6f}, æ¿€æ´»{nonzero_count}")
                
                # NMSå¤„ç† - ä½¿ç”¨è®­ç»ƒæ¨¡å¼è¾“å‡º
                pred = non_max_suppression(train_outputs, conf_thres=0.01, iou_thres=0.45, max_det=100)
                
                if len(pred) > 0 and len(pred[0]) > 0:
                    detections = pred[0]
                    det_count = len(detections)
                    print(f"     NMSåæ£€æµ‹æ•°é‡: {det_count}")
                    
                    # è½¬æ¢ä¸ºnumpy
                    if hasattr(detections, 'numpy'):
                        detections_np = detections.numpy()
                    else:
                        detections_np = detections
                    
                    # ç¡®ä¿æ£€æµ‹ç»“æœæ˜¯2ç»´çš„
                    if detections_np.ndim == 3:
                        detections_np = detections_np.reshape(-1, detections_np.shape[-1])
                    
                    # åˆ›å»ºå¯è§†åŒ–å›¾åƒ
                    vis_img = original_img.copy()
                    
                    # ç»˜åˆ¶çœŸå®æ ‡æ³¨æ¡†
                    for ann in annotations:
                        cls_id, x_center, y_center, width, height = ann
                        
                        # è½¬æ¢ä¸ºåƒç´ åæ ‡
                        x1 = int((x_center - width/2) * img_width)
                        y1 = int((y_center - height/2) * img_height)
                        x2 = int((x_center + width/2) * img_width)
                        y2 = int((y_center + height/2) * img_height)
                        
                        cls_name = VOC_CLASSES[cls_id]
                        draw_ground_truth_box(vis_img, [x1, y1, x2, y2], cls_name)
                    
                    # ç»Ÿè®¡æ£€æµ‹ç»“æœ
                    detected_counts = {}
                    confidence_info = []
                    expected_detections = 0
                    
                    # ç»˜åˆ¶æ£€æµ‹æ¡†
                    for i, detection in enumerate(detections_np):
                        if len(detection) >= 6:
                            x1, y1, x2, y2, conf, cls_id = detection[:6]
                            cls_id = int(cls_id)
                            cls_name = VOC_CLASSES[cls_id] if cls_id < len(VOC_CLASSES) else f'Class{cls_id}'
                            
                            detected_counts[cls_name] = detected_counts.get(cls_name, 0) + 1
                            confidence_info.append((cls_name, float(conf)))
                            
                            # æ£€æŸ¥æ˜¯å¦æ˜¯æœŸæœ›ç±»åˆ«
                            is_expected = cls_name in expected_classes
                            if is_expected:
                                expected_detections += 1
                            
                            # åªç»˜åˆ¶å‰15ä¸ªæ£€æµ‹æ¡†
                            if i < 15:
                                # é€‰æ‹©é¢œè‰²
                                color = COLORS.get(cls_name, COLORS['default'])
                                
                                # ç¼©æ”¾åæ ‡åˆ°åŸå›¾å°ºå¯¸
                                scale_x = img_width / 640
                                scale_y = img_height / 640
                                x1_scaled = int(x1 * scale_x)
                                y1_scaled = int(y1 * scale_y)
                                x2_scaled = int(x2 * scale_x)
                                y2_scaled = int(y2 * scale_y)
                                
                                draw_detection_box(vis_img, [x1_scaled, y1_scaled, x2_scaled, y2_scaled], 
                                                 cls_name, float(conf), color, is_expected)
                    
                    print(f"     æ£€æµ‹ç±»åˆ«ç»Ÿè®¡: {detected_counts}")
                    print(f"     æœŸæœ›ç±»åˆ«æ£€æµ‹æ•°: {expected_detections}")
                    
                    # æ˜¾ç¤ºç½®ä¿¡åº¦æœ€é«˜çš„å‰10ä¸ªæ£€æµ‹
                    confidence_info.sort(key=lambda x: x[1], reverse=True)
                    print(f"     ç½®ä¿¡åº¦æœ€é«˜çš„10ä¸ªæ£€æµ‹:")
                    for i, (cls_name, conf) in enumerate(confidence_info[:10]):
                        status = "âœ…" if cls_name in expected_classes else "âŒ"
                        print(f"       {i+1:2d}. {status}{cls_name}: {conf:.6f}")
                    
                    # è®¡ç®—ç§ç±»å‡†ç¡®ç‡
                    detected_class_names = set(detected_counts.keys())
                    correct_classes = expected_classes.intersection(detected_class_names)
                    species_accuracy = len(correct_classes) / len(expected_classes) if expected_classes else 0.0
                    
                    print(f"     ç§ç±»å‡†ç¡®ç‡: {species_accuracy*100:.1f}%")
                    print(f"     æ­£ç¡®è¯†åˆ«ç±»åˆ«: {correct_classes}")
                    
                    # æ·»åŠ ç»Ÿè®¡ä¿¡æ¯åˆ°å›¾åƒ
                    info_y = 30
                    info_texts = [
                        f"Epoch: {epoch+1}",
                        f"Loss: {epoch_loss:.4f}",
                        f"Detections: {det_count}",
                        f"Expected: {len(annotations)}",
                        f"Species Accuracy: {species_accuracy*100:.1f}%",
                        f"Correct Classes: {len(correct_classes)}/{len(expected_classes)}"
                    ]
                    
                    for text in info_texts:
                        cv2.putText(vis_img, text, (10, info_y), cv2.FONT_HERSHEY_SIMPLEX, 
                                   0.7, (255, 255, 255), 2, cv2.LINE_AA)
                        cv2.putText(vis_img, text, (10, info_y), cv2.FONT_HERSHEY_SIMPLEX, 
                                   0.7, (0, 0, 0), 1, cv2.LINE_AA)
                        info_y += 25
                    
                    # ä¿å­˜å¯è§†åŒ–ç»“æœ
                    save_path = save_dir / f'epoch_{epoch+1:03d}_perfect_visualization.jpg'
                    cv2.imwrite(str(save_path), vis_img)
                    print(f"     ğŸ’¾ å®Œç¾å¯è§†åŒ–å·²ä¿å­˜: {save_path}")
                    
                    # æ£€æŸ¥æ˜¯å¦è¾¾åˆ°å®Œç¾è¿‡æ‹Ÿåˆ
                    if species_accuracy > best_species_accuracy:
                        best_species_accuracy = species_accuracy
                        best_epoch = epoch + 1
                        
                        # ä¿å­˜æœ€ä½³å¯è§†åŒ–
                        best_path = save_dir / 'best_overfit_visualization.jpg'
                        cv2.imwrite(str(best_path), vis_img)
                        print(f"     ğŸ† æœ€ä½³ç»“æœå·²ä¿å­˜: {best_path}")
                    
                    if species_accuracy >= 0.8:
                        print(f"\nğŸ‰ å®Œç¾è¿‡æ‹ŸåˆæˆåŠŸï¼")
                        print(f"   âœ… ç§ç±»å‡†ç¡®ç‡: {species_accuracy*100:.1f}%")
                        print(f"   âœ… æ­£ç¡®è¯†åˆ«: {correct_classes}")
                        print(f"   âœ… å•å¼ å›¾ç‰‡è¿‡æ‹Ÿåˆå®Œæˆ")
                        
                        # ä¿å­˜å®Œç¾æ¨¡å‹
                        perfect_model_path = save_dir / 'perfect_overfit_model.pkl'
                        jt.save({
                            'model': model.state_dict(),
                            'epoch': epoch + 1,
                            'species_accuracy': species_accuracy,
                            'detected_counts': detected_counts,
                            'target_counts': target_counts
                        }, str(perfect_model_path))
                        
                        print(f"   ğŸ’¾ å®Œç¾æ¨¡å‹å·²ä¿å­˜: {perfect_model_path}")
                        return True
                else:
                    print(f"     âŒ æ²¡æœ‰æ£€æµ‹ç»“æœ")
            
            model.train()
    
    print(f"\nğŸ“Š è®­ç»ƒå®Œæˆ!")
    print(f"   æœ€ä½³ç§ç±»å‡†ç¡®ç‡: {best_species_accuracy*100:.1f}% (Epoch {best_epoch})")
    
    if best_species_accuracy >= 0.6:
        print(f"\nğŸ¯ è¿‡æ‹ŸåˆåŸºæœ¬æˆåŠŸï¼")
        print(f"âœ… GOLD-YOLO Jittorç‰ˆæœ¬åŸºæœ¬å¤ç°PyTorchç‰ˆæœ¬")
        return True
    else:
        print(f"\nâš ï¸ éœ€è¦è¿›ä¸€æ­¥ä¼˜åŒ–")
        return False

def main():
    print("ğŸ”¥ å®Œç¾çš„è¿‡æ‹Ÿåˆå¯è§†åŒ–è„šæœ¬")
    print("=" * 70)
    print("ç›®æ ‡ï¼šæ·±å…¥ä¿®å¤å¯è§†åŒ–è„šæœ¬ï¼Œå®Œç¾å±•ç¤ºè¿‡æ‹Ÿåˆæ¨ç†ç»“æœ")
    print("åŠŸèƒ½ï¼šç»˜åˆ¶æ£€æµ‹æ¡†ï¼Œæ˜¾ç¤ºç½®ä¿¡åº¦ï¼ŒåŒºåˆ†æœŸæœ›ç±»åˆ«")
    print("=" * 70)
    
    success = perfect_overfit_visualization()
    
    if success:
        print(f"\nğŸ‰ğŸ‰ğŸ‰ å®Œç¾è¿‡æ‹Ÿåˆå¯è§†åŒ–æˆåŠŸï¼ğŸ‰ğŸ‰ğŸ‰")
        print(f"âœ… å¯è§†åŒ–è„šæœ¬å®Œç¾ä¿®å¤")
        print(f"âœ… è¿‡æ‹Ÿåˆæ¨ç†ç»“æœå®Œç¾å±•ç¤º")
        print(f"âœ… å¯ä»¥å¼€å§‹200è½®å®Œæ•´è®­ç»ƒ")
    else:
        print(f"\nâš ï¸ ç»§ç»­ä¼˜åŒ–ä¸­ï¼Œå·²ç»éå¸¸æ¥è¿‘æˆåŠŸ")

if __name__ == "__main__":
    main()
