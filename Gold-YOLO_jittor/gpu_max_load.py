#!/usr/bin/env python3
"""
GPUæœ€å¤§è´Ÿè½½æµ‹è¯• - æ™ºèƒ½è°ƒèŠ‚è´Ÿè½½ä»¥è·‘æ»¡GPU
"""

import os
import time
import threading
import argparse

# è®¾ç½®GPUç¯å¢ƒ
os.environ['CUDA_VISIBLE_DEVICES'] = '0'
os.environ['JT_SYNC'] = '0'
os.environ['JT_CUDA_MEMORY_POOL'] = '1'

import jittor as jt

# å¼ºåˆ¶GPUæ¨¡å¼
jt.flags.use_cuda = 1
jt.flags.lazy_execution = 0

class GPULoadBalancer:
    def __init__(self):
        self.current_load = 1000  # åˆå§‹çŸ©é˜µå¤§å°
        self.target_gpu_util = 95  # ç›®æ ‡GPUåˆ©ç”¨ç‡
        self.max_memory_usage = 0.8  # æœ€å¤§å†…å­˜ä½¿ç”¨ç‡
        
    def get_gpu_status(self):
        """è·å–GPUçŠ¶æ€"""
        try:
            import subprocess
            result = subprocess.run([
                'nvidia-smi', 
                '--query-gpu=utilization.gpu,memory.used,memory.total',
                '--format=csv,noheader,nounits'
            ], capture_output=True, text=True)
            
            if result.returncode == 0:
                data = result.stdout.strip().split(', ')
                gpu_util = int(data[0])
                mem_used = int(data[1])
                mem_total = int(data[2])
                mem_usage = mem_used / mem_total
                return gpu_util, mem_usage
        except:
            pass
        return 0, 0
    
    def adjust_load(self, gpu_util, mem_usage):
        """åŠ¨æ€è°ƒæ•´è´Ÿè½½"""
        if mem_usage > self.max_memory_usage:
            # å†…å­˜ä½¿ç”¨è¿‡é«˜ï¼Œå‡å°‘è´Ÿè½½
            self.current_load = max(500, int(self.current_load * 0.9))
            print(f"ğŸ”½ å†…å­˜è¿‡é«˜({mem_usage:.1%})ï¼Œå‡å°‘è´Ÿè½½åˆ° {self.current_load}")
        elif gpu_util < self.target_gpu_util - 10:
            # GPUåˆ©ç”¨ç‡å¤ªä½ï¼Œå¢åŠ è´Ÿè½½
            self.current_load = min(4000, int(self.current_load * 1.1))
            print(f"ğŸ”¼ GPUåˆ©ç”¨ç‡ä½({gpu_util}%)ï¼Œå¢åŠ è´Ÿè½½åˆ° {self.current_load}")
        elif gpu_util > self.target_gpu_util + 5:
            # GPUåˆ©ç”¨ç‡è¿‡é«˜ï¼Œç¨å¾®å‡å°‘è´Ÿè½½
            self.current_load = max(500, int(self.current_load * 0.95))
            print(f"ğŸ”½ GPUåˆ©ç”¨ç‡è¿‡é«˜({gpu_util}%)ï¼Œå‡å°‘è´Ÿè½½åˆ° {self.current_load}")
        
        return self.current_load

def gpu_intensive_task(load_balancer, duration=60):
    """GPUå¯†é›†è®¡ç®—ä»»åŠ¡"""
    print(f"ğŸ”¥ å¯åŠ¨GPUå¯†é›†è®¡ç®—ä»»åŠ¡ ({duration}ç§’)")
    
    start_time = time.time()
    iteration = 0
    
    while time.time() - start_time < duration:
        try:
            # è·å–å½“å‰è´Ÿè½½å¤§å°
            matrix_size = load_balancer.current_load
            
            # çŸ©é˜µè¿ç®—
            a = jt.randn(matrix_size, matrix_size)
            b = jt.randn(matrix_size, matrix_size)
            c = jt.matmul(a, b)
            
            # å·ç§¯è¿ç®—
            batch_size = min(32, max(4, 8000 // matrix_size))
            channels = min(128, max(32, 4000 // matrix_size))
            size = min(256, max(64, 16000 // matrix_size))
            
            conv_input = jt.randn(batch_size, channels, size, size)
            conv = jt.nn.Conv2d(channels, channels, 3, padding=1)
            conv_output = conv(conv_input)
            
            # æ¿€æ´»å‡½æ•°
            relu_output = jt.nn.relu(conv_output)
            
            # å¼ºåˆ¶è®¡ç®—å®Œæˆ
            result1 = float(jt.sum(c))
            result2 = float(jt.sum(relu_output))
            
            iteration += 1
            
            # æ¯10æ¬¡è¿­ä»£è°ƒæ•´ä¸€æ¬¡è´Ÿè½½
            if iteration % 10 == 0:
                gpu_util, mem_usage = load_balancer.get_gpu_status()
                new_load = load_balancer.adjust_load(gpu_util, mem_usage)
                
                elapsed = time.time() - start_time
                print(f"   è¿­ä»£ {iteration}: GPU {gpu_util}% | å†…å­˜ {mem_usage:.1%} | è´Ÿè½½ {new_load} | {elapsed:.1f}s")
        
        except Exception as e:
            if "memory" in str(e).lower() or "overflow" in str(e).lower():
                # å†…å­˜ä¸è¶³ï¼Œå‡å°‘è´Ÿè½½
                load_balancer.current_load = max(500, int(load_balancer.current_load * 0.8))
                print(f"âš ï¸ å†…å­˜ä¸è¶³ï¼Œå‡å°‘è´Ÿè½½åˆ° {load_balancer.current_load}")
            else:
                print(f"âš ï¸ è®¡ç®—é”™è¯¯: {e}")
            time.sleep(1)
    
    total_time = time.time() - start_time
    print(f"âœ… GPUä»»åŠ¡å®Œæˆ: {iteration} æ¬¡è¿­ä»£, æ€»æ—¶é—´: {total_time:.1f}s")
    return iteration

def monitor_gpu_detailed(duration=60):
    """è¯¦ç»†ç›‘æ§GPUçŠ¶æ€"""
    print("ğŸ“Š å¼€å§‹è¯¦ç»†GPUç›‘æ§...")
    
    start_time = time.time()
    max_util = 0
    max_memory = 0
    
    while time.time() - start_time < duration:
        try:
            import subprocess
            result = subprocess.run([
                'nvidia-smi', 
                '--query-gpu=utilization.gpu,memory.used,memory.total,temperature.gpu,power.draw',
                '--format=csv,noheader,nounits'
            ], capture_output=True, text=True)
            
            if result.returncode == 0:
                data = result.stdout.strip().split(', ')
                gpu_util = int(data[0])
                mem_used = int(data[1])
                mem_total = int(data[2])
                temp = int(data[3])
                power = float(data[4])
                
                max_util = max(max_util, gpu_util)
                mem_percent = (mem_used / mem_total) * 100
                max_memory = max(max_memory, mem_percent)
                
                print(f"ğŸ”¥ GPU: {gpu_util:3d}% | å†…å­˜: {mem_percent:5.1f}% | æ¸©åº¦: {temp:2d}Â°C | åŠŸè€—: {power:5.1f}W")
            
        except Exception as e:
            print(f"âš ï¸ ç›‘æ§é”™è¯¯: {e}")
        
        time.sleep(3)
    
    print(f"ğŸ“Š ç›‘æ§å®Œæˆ - æœ€å¤§GPUåˆ©ç”¨ç‡: {max_util}% | æœ€å¤§å†…å­˜ä½¿ç”¨: {max_memory:.1f}%")
    return max_util, max_memory

def main():
    parser = argparse.ArgumentParser(description='GPUæœ€å¤§è´Ÿè½½æµ‹è¯•')
    parser.add_argument('--duration', type=int, default=120, help='æµ‹è¯•æ—¶é•¿(ç§’)')
    parser.add_argument('--target-util', type=int, default=95, help='ç›®æ ‡GPUåˆ©ç”¨ç‡(%)')
    
    args = parser.parse_args()
    
    print(f"ğŸš€ GPUæœ€å¤§è´Ÿè½½æµ‹è¯•")
    print(f"   Jittorç‰ˆæœ¬: {jt.__version__}")
    print(f"   GPUå¯ç”¨: {jt.has_cuda}")
    print(f"   ç›®æ ‡åˆ©ç”¨ç‡: {args.target_util}%")
    print(f"   æµ‹è¯•æ—¶é•¿: {args.duration}ç§’")
    print("=" * 60)
    
    # åˆ›å»ºè´Ÿè½½å‡è¡¡å™¨
    load_balancer = GPULoadBalancer()
    load_balancer.target_gpu_util = args.target_util
    
    # å¯åŠ¨ç›‘æ§çº¿ç¨‹
    monitor_thread = threading.Thread(target=monitor_gpu_detailed, args=(args.duration,))
    monitor_thread.start()
    
    # æ‰§è¡ŒGPUå¯†é›†ä»»åŠ¡
    iterations = gpu_intensive_task(load_balancer, args.duration)
    
    # ç­‰å¾…ç›‘æ§å®Œæˆ
    monitor_thread.join()
    
    print(f"\nğŸ‰ GPUæœ€å¤§è´Ÿè½½æµ‹è¯•å®Œæˆ!")
    print(f"   æ€»è¿­ä»£æ¬¡æ•°: {iterations}")
    print(f"   å¹³å‡è¿­ä»£é€Ÿåº¦: {iterations/args.duration:.2f} iter/s")

if __name__ == "__main__":
    main()
