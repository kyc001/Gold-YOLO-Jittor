#!/usr/bin/env python3
# -*- coding:utf-8 -*-
"""
æœ€ç»ˆæ™ºèƒ½æ¨ç†æµ‹è¯•
ä½¿ç”¨æ™ºèƒ½åŒ¹é…æƒé‡è¿›è¡Œæœ€ç»ˆæ¨ç†æµ‹è¯•
"""

import os
import sys
import numpy as np
import jittor as jt
import cv2
import glob
from pathlib import Path
import matplotlib.pyplot as plt
import matplotlib.patches as patches
import time

# è®¾ç½®Jittor
jt.flags.use_cuda = 1 if jt.has_cuda else 0

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append(str(Path(__file__).parent))

from pytorch_aligned_model import PyTorchAlignedGoldYOLO


def letterbox(img, new_shape=(640, 640), color=(114, 114, 114), auto=True, scaleFill=False, scaleup=True, stride=32):
    """ä¸¥æ ¼æŒ‰ç…§PyTorchç‰ˆæœ¬çš„letterboxå®ç°"""
    shape = img.shape[:2]  # current shape [height, width]
    if isinstance(new_shape, int):
        new_shape = (new_shape, new_shape)

    # Scale ratio (new / old)
    r = min(new_shape[0] / shape[0], new_shape[1] / shape[1])
    if not scaleup:  # only scale down, do not scale up (for better test mAP)
        r = min(r, 1.0)

    # Compute padding
    ratio = r, r  # width, height ratios
    new_unpad = int(round(shape[1] * r)), int(round(shape[0] * r))
    dw, dh = new_shape[1] - new_unpad[0], new_shape[0] - new_unpad[1]  # wh padding
    if auto:  # minimum rectangle
        dw, dh = np.mod(dw, stride), np.mod(dh, stride)  # wh padding
    elif scaleFill:  # stretch
        dw, dh = 0.0, 0.0
        new_unpad = (new_shape[1], new_shape[0])
        ratio = new_shape[1] / shape[1], new_shape[0] / shape[0]  # width, height ratios

    dw /= 2  # divide padding into 2 sides
    dh /= 2

    if shape[::-1] != new_unpad:  # resize
        img = cv2.resize(img, new_unpad, interpolation=cv2.INTER_LINEAR)
    top, bottom = int(round(dh - 0.1)), int(round(dh + 0.1))
    left, right = int(round(dw - 0.1)), int(round(dw + 0.1))
    img = cv2.copyMakeBorder(img, top, bottom, left, right, cv2.BORDER_CONSTANT, value=color)  # add border
    return img, ratio, (dw, dh)


class FinalSmartInferenceTester:
    """æœ€ç»ˆæ™ºèƒ½æ¨ç†æµ‹è¯•å™¨"""
    
    def __init__(self):
        """åˆå§‹åŒ–"""
        self.smart_weights_path = "weights/smart_matched_weights.npz"
        self.test_images_dir = "/home/kyc/project/GOLD-YOLO/Gold-YOLO_pytorch/runs/inference/gold_yolo_n_test/test_images"
        
        # VOC 20ç±»åˆ«åç§°
        self.class_names = [
            'aeroplane', 'bicycle', 'bird', 'boat', 'bottle',
            'bus', 'car', 'cat', 'chair', 'cow',
            'diningtable', 'dog', 'horse', 'motorbike', 'person',
            'pottedplant', 'sheep', 'sofa', 'train', 'tvmonitor'
        ]
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        os.makedirs("outputs/final_smart_inference", exist_ok=True)
        
        print("ğŸ§  æœ€ç»ˆæ™ºèƒ½æ¨ç†æµ‹è¯•å™¨")
        print("   ä½¿ç”¨æ™ºèƒ½åŒ¹é…æƒé‡è¿›è¡Œæ¨ç†")
        print("=" * 80)
    
    def load_smart_model(self):
        """åŠ è½½æ™ºèƒ½åŒ¹é…æƒé‡çš„æ¨¡å‹"""
        print("\nğŸ“¦ åŠ è½½æ™ºèƒ½åŒ¹é…æƒé‡æ¨¡å‹")
        print("-" * 60)
        
        # åˆ›å»ºæ¨¡å‹
        model = PyTorchAlignedGoldYOLO(num_classes=20)
        
        # åŠ è½½æ™ºèƒ½åŒ¹é…æƒé‡
        if not os.path.exists(self.smart_weights_path):
            print(f"âŒ æ™ºèƒ½åŒ¹é…æƒé‡æ–‡ä»¶ä¸å­˜åœ¨: {self.smart_weights_path}")
            return None
        
        weights = np.load(self.smart_weights_path)
        jt_state_dict = {name: jt.array(weight) for name, weight in weights.items()}
        model.load_state_dict(jt_state_dict)
        model.eval()
        
        # è®¡ç®—è¦†ç›–ç‡
        model_params = dict(model.named_parameters())
        coverage = len(weights) / len(model_params) * 100
        
        print(f"âœ… æ™ºèƒ½åŒ¹é…æ¨¡å‹åŠ è½½æˆåŠŸ")
        print(f"   æƒé‡è¦†ç›–ç‡: {coverage:.1f}%")
        print(f"   åŠ è½½æƒé‡æ•°: {len(weights)}")
        
        return model
    
    def preprocess_image(self, img_path, img_size=640):
        """å›¾åƒé¢„å¤„ç†"""
        # è¯»å–å›¾åƒ
        img0 = cv2.imread(img_path)  # BGR
        assert img0 is not None, f'Image Not Found {img_path}'
        
        # Letterbox
        img = letterbox(img0, img_size, stride=32)[0]
        
        # Convert
        img = img[:, :, ::-1].transpose(2, 0, 1)  # BGR to RGB, to 3x416x416
        img = np.ascontiguousarray(img)
        
        # Normalize
        img = img.astype(np.float32)
        img /= 255.0  # 0 - 255 to 0.0 - 1.0
        
        # Add batch dimension
        if len(img.shape) == 3:
            img = img[None]  # expand for batch dim
        
        return jt.array(img), img0
    
    def smart_inference(self, model, img_path, conf_thres=0.01):  # é™ä½é˜ˆå€¼
        """æ™ºèƒ½æ¨ç†"""
        print(f"\nğŸ§  æ™ºèƒ½æ¨ç†æµ‹è¯•")
        print("-" * 60)
        
        # é¢„å¤„ç†
        img, img0 = self.preprocess_image(img_path)
        print(f"   å›¾åƒé¢„å¤„ç†: {img.shape} -> åŸå›¾: {img0.shape}")
        
        # æ¨ç†
        t1 = time.time()
        with jt.no_grad():
            output = model(img)
        t2 = time.time()
        
        print(f"   æ¨¡å‹æ¨ç†: è€—æ—¶ {(t2-t1)*1000:.1f}ms")
        
        # è§£æè¾“å‡º
        if isinstance(output, list):
            detections, featmaps = output
            print(f"   è¾“å‡ºè§£æ: æ£€æµ‹{detections.shape}, ç‰¹å¾å›¾{len(featmaps)}ä¸ª")
        else:
            detections = output
            print(f"   è¾“å‡ºè§£æ: æ£€æµ‹{detections.shape}")
        
        # åˆ†ææ£€æµ‹ç»“æœ
        det = detections[0]  # [anchors, 25]
        coords = det[:, :4]  # xywh
        obj_conf = det[:, 4]  # objectness
        cls_probs = det[:, 5:]  # class probabilities
        
        # è®¡ç®—æœ€ç»ˆç½®ä¿¡åº¦
        max_cls_probs = jt.max(cls_probs, dim=1)[0]
        cls_ids = jt.argmax(cls_probs, dim=1)[0]
        total_conf = obj_conf * max_cls_probs
        
        print(f"   ğŸ” æ£€æµ‹åˆ†æ:")
        print(f"      æ€»anchoræ•°: {len(det)}")
        print(f"      ç›®æ ‡ç½®ä¿¡åº¦èŒƒå›´: [{obj_conf.min():.6f}, {obj_conf.max():.6f}]")
        print(f"      ç›®æ ‡ç½®ä¿¡åº¦å”¯ä¸€å€¼: {len(jt.unique(obj_conf))}")
        print(f"      ç±»åˆ«æ¦‚ç‡èŒƒå›´: [{max_cls_probs.min():.6f}, {max_cls_probs.max():.6f}]")
        print(f"      æœ€é«˜æ€»ç½®ä¿¡åº¦: {total_conf.max():.6f}")
        print(f"      >0.01æ£€æµ‹æ•°: {(total_conf > 0.01).sum()}")
        print(f"      >0.005æ£€æµ‹æ•°: {(total_conf > 0.005).sum()}")
        
        # åå¤„ç† - ä½¿ç”¨æ›´ä½çš„é˜ˆå€¼
        final_detections = self.postprocess_detections(det, img.shape[2:], img0.shape, conf_thres)
        
        print(f"   åå¤„ç†: {len(final_detections)}ä¸ªæ£€æµ‹")
        
        return final_detections, img0
    
    def postprocess_detections(self, pred, img_shape, orig_shape, conf_thres=0.01):
        """åå¤„ç†æ£€æµ‹ç»“æœ"""
        # åˆ†ç¦»åæ ‡ã€ç½®ä¿¡åº¦å’Œç±»åˆ«æ¦‚ç‡
        boxes = pred[:, :4]  # xywh
        obj_conf = pred[:, 4]  # objectness
        cls_probs = pred[:, 5:]  # class probabilities
        
        # è®¡ç®—æœ€ç»ˆç½®ä¿¡åº¦
        max_cls_probs = jt.max(cls_probs, dim=1)[0]
        cls_ids = jt.argmax(cls_probs, dim=1)[0]
        total_conf = obj_conf * max_cls_probs
        
        # ç½®ä¿¡åº¦è¿‡æ»¤
        conf_mask = total_conf > conf_thres
        if not jt.any(conf_mask):
            return []
        
        # è¿‡æ»¤
        boxes = boxes[conf_mask]
        total_conf = total_conf[conf_mask]
        cls_ids = cls_ids[conf_mask]
        
        # è½¬æ¢åæ ‡æ ¼å¼ xywh -> xyxy
        x_center, y_center, w, h = boxes[:, 0], boxes[:, 1], boxes[:, 2], boxes[:, 3]
        x1 = x_center - w / 2
        y1 = y_center - h / 2
        x2 = x_center + w / 2
        y2 = y_center + h / 2
        xyxy_boxes = jt.stack([x1, y1, x2, y2], dim=1)
        
        # åæ ‡ç¼©æ”¾å›åŸå›¾
        gain = min(img_shape[0] / orig_shape[0], img_shape[1] / orig_shape[1])
        pad = (img_shape[1] - orig_shape[1] * gain) / 2, (img_shape[0] - orig_shape[0] * gain) / 2
        
        xyxy_boxes[:, [0, 2]] -= pad[0]  # x padding
        xyxy_boxes[:, [1, 3]] -= pad[1]  # y padding
        xyxy_boxes[:, :4] /= gain
        
        # é™åˆ¶åœ¨å›¾åƒèŒƒå›´å†…
        xyxy_boxes[:, 0] = jt.clamp(xyxy_boxes[:, 0], 0, orig_shape[1])
        xyxy_boxes[:, 1] = jt.clamp(xyxy_boxes[:, 1], 0, orig_shape[0])
        xyxy_boxes[:, 2] = jt.clamp(xyxy_boxes[:, 2], 0, orig_shape[1])
        xyxy_boxes[:, 3] = jt.clamp(xyxy_boxes[:, 3], 0, orig_shape[0])
        
        # è½¬æ¢ä¸ºæ£€æµ‹ç»“æœ
        detections = []
        for i in range(len(xyxy_boxes)):
            x1, y1, x2, y2 = xyxy_boxes[i].numpy()
            conf = float(total_conf[i].numpy())
            cls_id = int(cls_ids[i].numpy())
            
            # æ£€æŸ¥æ¡†çš„æœ‰æ•ˆæ€§
            if x2 > x1 and y2 > y1 and (x2 - x1) > 5 and (y2 - y1) > 5:
                detections.append({
                    'bbox': [x1, y1, x2, y2],
                    'confidence': conf,
                    'class_id': cls_id,
                    'class_name': self.class_names[cls_id]
                })
        
        # æŒ‰ç½®ä¿¡åº¦æ’åº
        detections = sorted(detections, key=lambda x: x['confidence'], reverse=True)
        
        # é™åˆ¶æ£€æµ‹æ•°é‡
        detections = detections[:50]  # å¢åŠ æ£€æµ‹æ•°é‡é™åˆ¶
        
        return detections
    
    def visualize_detections(self, img, detections, save_path):
        """å¯è§†åŒ–æ£€æµ‹ç»“æœ"""
        img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
        
        fig, ax = plt.subplots(1, 1, figsize=(12, 8))
        ax.imshow(img_rgb)
        
        # ç»˜åˆ¶æ£€æµ‹æ¡†
        colors = plt.cm.Set3(np.linspace(0, 1, 20))
        
        for det in detections:
            x1, y1, x2, y2 = det['bbox']
            class_id = det['class_id']
            confidence = det['confidence']
            class_name = det['class_name']
            
            # ç»˜åˆ¶è¾¹ç•Œæ¡†
            rect = patches.Rectangle(
                (x1, y1), x2 - x1, y2 - y1,
                linewidth=2, edgecolor=colors[class_id], facecolor='none'
            )
            ax.add_patch(rect)
            
            # æ·»åŠ æ ‡ç­¾
            label = f"{class_name}: {confidence:.4f}"
            ax.text(x1, y1 - 5, label, 
                   bbox=dict(boxstyle="round,pad=0.3", facecolor=colors[class_id], alpha=0.7),
                   fontsize=10, color='black', weight='bold')
        
        image_name = Path(save_path).stem.replace('_smart', '')
        ax.set_title(f"Smart Matched: {image_name} - {len(detections)} detections", fontsize=14)
        ax.axis('off')
        
        plt.tight_layout()
        plt.savefig(save_path, dpi=150, bbox_inches='tight')
        plt.close()
        
        print(f"   å¯è§†åŒ–ä¿å­˜: {Path(save_path).name}")
    
    def run_final_smart_test(self):
        """è¿è¡Œæœ€ç»ˆæ™ºèƒ½æµ‹è¯•"""
        print("ğŸ§  è¿è¡Œæœ€ç»ˆæ™ºèƒ½æ¨ç†æµ‹è¯•")
        print("=" * 80)
        
        # 1. åŠ è½½æ™ºèƒ½æ¨¡å‹
        model = self.load_smart_model()
        if model is None:
            return
        
        # 2. è·å–æµ‹è¯•å›¾åƒ
        image_files = glob.glob(os.path.join(self.test_images_dir, "*.jpg"))
        print(f"\nğŸ–¼ï¸ æ‰¾åˆ° {len(image_files)} å¼ æµ‹è¯•å›¾åƒ")
        
        # 3. å¯¹æ¯å¼ å›¾åƒè¿›è¡Œæ™ºèƒ½æ¨ç†
        all_results = []
        
        for i, image_path in enumerate(image_files[:3]):  # æµ‹è¯•å‰3å¼ 
            image_name = Path(image_path).stem
            print(f"\nğŸ“· å¤„ç†å›¾åƒ {i+1}: {image_name}")
            
            # è¿è¡Œæ™ºèƒ½æ¨ç†
            detections, orig_img = self.smart_inference(model, image_path, conf_thres=0.01)
            
            # å¯è§†åŒ–ç»“æœ
            save_path = f"outputs/final_smart_inference/{image_name}_smart.png"
            self.visualize_detections(orig_img, detections, save_path)
            
            # è®°å½•ç»“æœ
            all_results.append({
                'image_name': image_name,
                'detections': detections,
                'detection_count': len(detections)
            })
            
            # æ˜¾ç¤ºæ£€æµ‹ç»“æœ
            if detections:
                print(f"   âœ… æ£€æµ‹ç»“æœ:")
                for j, det in enumerate(detections[:10]):  # æ˜¾ç¤ºå‰10ä¸ª
                    print(f"      {j+1}. {det['class_name']}: {det['confidence']:.4f}")
                if len(detections) > 10:
                    print(f"      ... è¿˜æœ‰{len(detections)-10}ä¸ªæ£€æµ‹")
            else:
                print(f"   âŒ æœªæ£€æµ‹åˆ°ç›®æ ‡")
        
        print(f"\nğŸ‰ æœ€ç»ˆæ™ºèƒ½æ¨ç†æµ‹è¯•å®Œæˆ!")
        print("=" * 80)
        
        # æ€»ç»“
        total_detections = sum(r['detection_count'] for r in all_results)
        avg_detections = total_detections / len(all_results) if all_results else 0
        
        print(f"ğŸ“Š æ™ºèƒ½æ¨ç†æ€»ç»“:")
        print(f"   æµ‹è¯•å›¾åƒ: {len(all_results)}")
        print(f"   æ€»æ£€æµ‹æ•°: {total_detections}")
        print(f"   å¹³å‡æ£€æµ‹æ•°: {avg_detections:.1f}")
        
        if total_detections > 0:
            print(f"   ğŸ¯ æ™ºèƒ½æ¨ç†æˆåŠŸ: æ£€æµ‹åˆ°ç›®æ ‡ï¼")
            print(f"   ğŸ† Gold-YOLO Jittorç‰ˆæœ¬æ™ºèƒ½åŒ¹é…æˆåŠŸï¼")
            
            # æœ€ç»ˆè¯„ä¼°
            if avg_detections > 5:
                print(f"   ğŸŒŸ æ€§èƒ½è¯„ä¼°: ä¼˜ç§€")
            elif avg_detections > 1:
                print(f"   âœ… æ€§èƒ½è¯„ä¼°: è‰¯å¥½")
            else:
                print(f"   âš ï¸ æ€§èƒ½è¯„ä¼°: ä¸€èˆ¬")
        else:
            print(f"   âš ï¸ ä»éœ€è¿›ä¸€æ­¥ä¼˜åŒ–")


def main():
    """ä¸»å‡½æ•°"""
    tester = FinalSmartInferenceTester()
    tester.run_final_smart_test()


if __name__ == '__main__':
    main()
