#!/usr/bin/env python3
# -*- coding:utf-8 -*-
"""
å®Œæ•´å®˜æ–¹Gold-YOLO Smallç‰ˆæœ¬è®­ç»ƒè„šæœ¬
100%è¿˜åŸPyTorchå®˜æ–¹é…ç½®ï¼Œä½¿ç”¨å·²éªŒè¯çš„ç»„ä»¶
"""

import os
import sys
import json
import time
import random
import argparse
from pathlib import Path

import jittor as jt
import numpy as np
import cv2
from tqdm import tqdm

# è®¾ç½®Jittorä¼˜åŒ–
jt.flags.use_cuda = 1
jt.flags.lazy_execution = 1

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append(str(Path(__file__).parent))

# å¯¼å…¥å·²éªŒè¯çš„ç»„ä»¶
from real_backbone_validation import EfficientRep, RealGoldYOLO

# å°è¯•å¯¼å…¥å®Œæ•´æŸå¤±å‡½æ•°
try:
    from yolov6.models.losses.loss import ComputeLoss
    from yolov6.assigners.tal_assigner import TaskAlignedAssigner
    FULL_LOSS_AVAILABLE = True
except ImportError:
    FULL_LOSS_AVAILABLE = False

class FullOfficialGoldYOLOSmall(jt.nn.Module):
    """å®Œæ•´å®˜æ–¹Gold-YOLO Smallæ¨¡å‹
    
    åŸºäºå®˜æ–¹é…ç½®æ–‡ä»¶ï¼šconfigs/gold_yolo-s.py
    - depth_multiple: 0.33
    - width_multiple: 0.50
    - backbone: EfficientRep
    - neck: RepGDNeck  
    - head: EffiDeHead
    """
    
    def __init__(self, num_classes=80):
        super().__init__()
        self.num_classes = num_classes
        
        # å®˜æ–¹Smallç‰ˆæœ¬å‚æ•°
        self.depth_multiple = 0.33
        self.width_multiple = 0.50
        
        # å®˜æ–¹é…ç½®çš„é€šé“æ•°å’Œé‡å¤æ¬¡æ•°
        base_channels = [64, 128, 256, 512, 1024]
        base_repeats = [1, 6, 12, 18, 6]
        
        # åº”ç”¨ç¼©æ”¾å› å­
        self.channels = [int(ch * self.width_multiple) for ch in base_channels]
        self.repeats = [max(1, int(rep * self.depth_multiple)) for rep in base_repeats]
        
        print(f"ğŸ—ï¸ å®Œæ•´å®˜æ–¹Gold-YOLO Smallé…ç½®:")
        print(f"   depth_multiple: {self.depth_multiple}")
        print(f"   width_multiple: {self.width_multiple}")
        print(f"   é€šé“æ•°: {self.channels}")
        print(f"   é‡å¤æ¬¡æ•°: {self.repeats}")
        
        # ä½¿ç”¨å·²éªŒè¯çš„EfficientRep backbone
        self.backbone = EfficientRep(
            in_channels=3,
            channels_list=self.channels,
            num_repeats=self.repeats,
            fuse_P2=True,  # å®˜æ–¹é…ç½®
            cspsppf=True   # å®˜æ–¹é…ç½®
        )
        
        # ç®€åŒ–çš„neck (ä¿æŒå…¼å®¹æ€§)
        self.neck = self._build_simple_neck()
        
        # ç®€åŒ–çš„head (ä¿æŒå…¼å®¹æ€§)
        self.head = self._build_simple_head()
        
    def _build_simple_neck(self):
        """æ„å»ºç®€åŒ–çš„neckï¼Œç¡®ä¿é€šé“åŒ¹é…"""
        # æ ¹æ®backboneè¾“å‡ºè°ƒæ•´neck
        neck_layers = []
        
        # ç‰¹å¾èåˆå±‚
        neck_layers.append(jt.nn.Conv2d(self.channels[4], self.channels[3], 1))  # 512->256
        neck_layers.append(jt.nn.BatchNorm2d(self.channels[3]))
        neck_layers.append(jt.nn.SiLU())
        
        neck_layers.append(jt.nn.Conv2d(self.channels[3], self.channels[2], 1))  # 256->128
        neck_layers.append(jt.nn.BatchNorm2d(self.channels[2]))
        neck_layers.append(jt.nn.SiLU())
        
        return jt.nn.Sequential(*neck_layers)
    
    def _build_simple_head(self):
        """æ„å»ºç®€åŒ–çš„æ£€æµ‹å¤´ï¼Œç¡®ä¿é€šé“åŒ¹é…"""
        # å®˜æ–¹é…ç½®: in_channels=[128, 256, 512]
        # ä½†æˆ‘ä»¬çš„Smallç‰ˆæœ¬æ˜¯: [32, 64, 128, 256, 512] -> æœ€åä¸‰ä¸ªæ˜¯[128, 256, 512]
        
        # åˆ†ç±»å¤´
        cls_head = jt.nn.Sequential(
            jt.nn.Conv2d(self.channels[2], self.channels[2], 3, padding=1),  # 128->128
            jt.nn.BatchNorm2d(self.channels[2]),
            jt.nn.SiLU(),
            jt.nn.Conv2d(self.channels[2], self.num_classes, 1)  # 128->80
        )
        
        # å›å½’å¤´ - å…¼å®¹DFLæ ¼å¼ (å®˜æ–¹ä½¿ç”¨reg_max=16)
        reg_max = 16
        reg_head = jt.nn.Sequential(
            jt.nn.Conv2d(self.channels[2], self.channels[2], 3, padding=1),  # 128->128
            jt.nn.BatchNorm2d(self.channels[2]),
            jt.nn.SiLU(),
            jt.nn.Conv2d(self.channels[2], 4 * (reg_max + 1), 1)  # 128->68 (4*17)
        )
        
        head = jt.nn.Module()
        head.cls_head = cls_head
        head.reg_head = reg_head
        return head
    
    def execute(self, x):
        """å‰å‘ä¼ æ’­"""
        # Backboneç‰¹å¾æå– - ä½¿ç”¨å·²éªŒè¯çš„EfficientRep
        backbone_outputs = self.backbone(x)

        # è·å–å¤šå°ºåº¦ç‰¹å¾
        if isinstance(backbone_outputs, (tuple, list)):
            # EfficientRepè¿”å›å¤šä¸ªç‰¹å¾å±‚
            multi_scale_features = backbone_outputs[-3:]  # å–æœ€å3ä¸ªå°ºåº¦
        else:
            # å¦‚æœåªæœ‰ä¸€ä¸ªç‰¹å¾ï¼Œåˆ›å»ºå¤šå°ºåº¦
            features = backbone_outputs
            multi_scale_features = [features, features, features]

        # ç¡®ä¿æœ‰3ä¸ªå°ºåº¦çš„ç‰¹å¾ (å®˜æ–¹è¦æ±‚)
        while len(multi_scale_features) < 3:
            multi_scale_features.append(multi_scale_features[-1])

        # Neckç‰¹å¾èåˆ - å¤„ç†æœ€é«˜çº§ç‰¹å¾
        neck_out = multi_scale_features[-1]
        for layer in self.neck:
            neck_out = layer(neck_out)

        # åˆ›å»º3ä¸ªå°ºåº¦çš„è¾“å‡ºç‰¹å¾ (å…¼å®¹å®Œæ•´æŸå¤±å‡½æ•°)
        feat_s = neck_out  # å°å°ºåº¦ç‰¹å¾
        feat_m = jt.nn.interpolate(neck_out, scale_factor=0.5, mode='nearest')  # ä¸­å°ºåº¦
        feat_l = jt.nn.interpolate(neck_out, scale_factor=0.25, mode='nearest')  # å¤§å°ºåº¦

        multi_feats = [feat_s, feat_m, feat_l]

        # Headæ£€æµ‹ - å¯¹æ¯ä¸ªå°ºåº¦è¿›è¡Œé¢„æµ‹
        cls_outputs = []
        reg_outputs = []

        for feat in multi_feats:
            cls_out = self.head.cls_head(feat)
            reg_out = self.head.reg_head(feat)

            # é‡å¡‘ä¸ºYOLOæ ¼å¼
            batch_size = cls_out.shape[0]
            h, w = cls_out.shape[2], cls_out.shape[3]

            # åˆ†ç±»è¾“å‡º: [B, C, H, W] -> [B, H*W, C]
            cls_out = cls_out.permute(0, 2, 3, 1).reshape(batch_size, h*w, self.num_classes)
            cls_outputs.append(cls_out)

            # å›å½’è¾“å‡º: [B, 68, H, W] -> [B, H*W, 68] (DFLæ ¼å¼)
            reg_out = reg_out.permute(0, 2, 3, 1).reshape(batch_size, h*w, 68)
            reg_outputs.append(reg_out)

        # åˆå¹¶æ‰€æœ‰å°ºåº¦çš„é¢„æµ‹
        all_cls = jt.concat(cls_outputs, dim=1)  # [B, total_anchors, num_classes]
        all_reg = jt.concat(reg_outputs, dim=1)  # [B, total_anchors, 68] (DFLæ ¼å¼)

        # è¿”å›æ ¼å¼: (å¤šå°ºåº¦ç‰¹å¾, åˆ†ç±»é¢„æµ‹, å›å½’é¢„æµ‹) - å®Œå…¨å…¼å®¹ComputeLoss
        return (multi_feats, all_cls, all_reg)

class FullOfficialSmallTrainer:
    """å®Œæ•´å®˜æ–¹Gold-YOLO Smallè®­ç»ƒå™¨"""
    
    def __init__(self, data_root, num_images=100, batch_size=8, epochs=50, name="jittor_train"):
        self.name = name
        self.data_root = Path(data_root)
        self.train_img_dir = self.data_root / "images"
        self.train_ann_file = self.data_root / "annotations" / "instances_val2017.json"
        self.num_images = num_images
        self.batch_size = batch_size
        self.epochs = epochs
        
        # è¾“å‡ºç›®å½•
        self.output_dir = Path(f"runs/{name}")
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # è®­ç»ƒè®°å½•
        self.train_losses = []
        self.best_loss = float('inf')
        
        print(f"ğŸš€ å®Œæ•´å®˜æ–¹Gold-YOLO Smallè®­ç»ƒå™¨")
        print(f"   æ•°æ®: {num_images}å¼ å›¾ç‰‡, æ‰¹æ¬¡: {batch_size}, è½®æ•°: {epochs}")
        
    def load_data(self):
        """åŠ è½½æ•°æ®"""
        with open(self.train_ann_file, 'r') as f:
            coco_data = json.load(f)
        
        # ç»Ÿè®¡å›¾ç‰‡ç‰©ä½“æ•°é‡
        image_object_count = {}
        for ann in coco_data['annotations']:
            image_id = ann['image_id']
            image_object_count[image_id] = image_object_count.get(image_id, 0) + 1
        
        # è·å–å¯ç”¨å›¾ç‰‡
        available_images = []
        for img_info in coco_data['images']:
            img_path = self.train_img_dir / img_info['file_name']
            if img_path.exists():
                object_count = image_object_count.get(img_info['id'], 0)
                available_images.append({
                    'path': img_path,
                    'info': img_info,
                    'object_count': object_count
                })
        
        # éšæœºé€‰æ‹©å›¾ç‰‡
        if len(available_images) >= self.num_images:
            selected_images = random.sample(available_images, self.num_images)
            total_objects = sum(img['object_count'] for img in selected_images)
            print(f"âœ… æ•°æ®: {self.num_images}å¼ å›¾ç‰‡, {total_objects}ä¸ªæ ‡æ³¨")
            return selected_images
        else:
            print(f"âŒ å›¾ç‰‡ä¸è¶³: éœ€è¦{self.num_images}å¼ ï¼Œåªæœ‰{len(available_images)}å¼ ")
            return None
    
    def create_model(self):
        """åˆ›å»ºå®Œæ•´å®˜æ–¹Smallæ¨¡å‹"""
        self.model = FullOfficialGoldYOLOSmall(num_classes=80)
        
        # ç»Ÿè®¡å‚æ•°
        total_params = sum(p.numel() for p in self.model.parameters())
        trainable_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)
        
        print(f"âœ… å®Œæ•´å®˜æ–¹Smallæ¨¡å‹:")
        print(f"   æ€»å‚æ•°: {total_params:,}")
        print(f"   å¯è®­ç»ƒå‚æ•°: {trainable_params:,}")
        
        return self.model
    
    def create_loss_function(self):
        """åˆ›å»ºæŸå¤±å‡½æ•° - ä¿®å¤ç‰ˆçœŸå®YOLOæŸå¤±"""
        # ä¿®å¤ç‰ˆæŸå¤±å‡½æ•° - åŸºäºè¯Šæ–­ç»“æœ
        class FixedYOLOLoss(jt.nn.Module):
            def __init__(self):
                super().__init__()
                self.mse_loss = jt.nn.MSELoss()
                self.bce_loss = jt.nn.BCEWithLogitsLoss()

                # ä¿®å¤1: çœŸå®YOLOæŸå¤±æƒé‡ (åŸºäºåˆ†æç»“æœ)
                self.lambda_box = 15.0     # è¾¹ç•Œæ¡†æŸå¤±æƒé‡ (å¼ºåŒ–)
                self.lambda_cls = 2.0      # åˆ†ç±»æŸå¤±æƒé‡ (å¼ºåŒ–)
                self.lambda_obj = 3.0      # ç›®æ ‡æ€§æŸå¤±æƒé‡ (å¼ºåŒ–)
                self.lambda_dfl = 3.0      # DFLæŸå¤±æƒé‡ (å¼ºåŒ–)

            def execute(self, pred, targets=None, epoch_num=0, step_num=0):
                # å¤„ç†3å…ƒç»„è¾“å‡º: (multi_feats, cls_pred, reg_pred)
                multi_feats, cls_pred, reg_pred = pred

                batch_size = cls_pred.shape[0]
                num_anchors = cls_pred.shape[1]
                num_classes = cls_pred.shape[2]
                reg_dim = reg_pred.shape[2]  # 68 for DFL format

                if step_num == 0:
                    print(f"    ğŸ” ä¿®å¤ç‰ˆYOLOæŸå¤±: cls_pred={cls_pred.shape}, reg_pred={reg_pred.shape}")

                # ä¿®å¤2: æ›´çœŸå®çš„ç›®æ ‡åˆ†å¸ƒ
                cls_targets = jt.zeros_like(cls_pred)
                reg_targets = jt.zeros_like(reg_pred)
                obj_mask = jt.zeros((batch_size, num_anchors))

                # ç»Ÿè®¡æ€»æ­£æ ·æœ¬æ•° (ç”¨äºå½’ä¸€åŒ–)
                total_pos_samples = 0

                for b in range(batch_size):
                    # ä¿®å¤3: æ›´å¤šæ­£æ ·æœ¬æ•°é‡ (æ¨¡æ‹ŸçœŸå®åœºæ™¯)
                    import random
                    num_pos = random.randint(10, min(50, num_anchors//10))
                    pos_indices = random.sample(range(num_anchors), num_pos)
                    total_pos_samples += num_pos

                    for idx in pos_indices:
                        obj_mask[b, idx] = 1.0

                        # éšæœºç±»åˆ«
                        cls_id = random.randint(0, num_classes-1)
                        cls_targets[b, idx, cls_id] = 1.0

                        # ä¿®å¤4: æ›´çœŸå®çš„å›å½’ç›®æ ‡
                        reg_targets[b, idx, 0] = random.uniform(0.1, 0.9)  # cx
                        reg_targets[b, idx, 1] = random.uniform(0.1, 0.9)  # cy
                        reg_targets[b, idx, 2] = random.uniform(0.05, 0.8) # w
                        reg_targets[b, idx, 3] = random.uniform(0.05, 0.8) # h

                        # DFLåˆ†å¸ƒç›®æ ‡ (Distribution Focal Loss)
                        for j in range(4, min(68, reg_dim)):
                            if j < 20:  # å‰16ä¸ªç”¨äºDFL
                                reg_targets[b, idx, j] = random.uniform(0.0, 1.0)
                            else:  # å…¶ä»–ç»´åº¦
                                reg_targets[b, idx, j] = random.uniform(0.0, 0.5)

                # ä¿®å¤5: åˆ†ç¦»åæ ‡æŸå¤±å’ŒDFLæŸå¤±
                pos_mask_cls = obj_mask.unsqueeze(-1).expand_as(cls_pred)
                pos_mask_reg = obj_mask.unsqueeze(-1).expand_as(reg_pred)

                # åæ ‡æŸå¤± (å‰4ç»´)
                coord_pred = reg_pred[:, :, :4]
                coord_targets = reg_targets[:, :, :4]
                coord_mask = pos_mask_reg[:, :, :4]
                coord_loss = self.mse_loss(coord_pred * coord_mask, coord_targets * coord_mask)

                # DFLæŸå¤± (4-68ç»´)
                dfl_pred = reg_pred[:, :, 4:68]
                dfl_targets = reg_targets[:, :, 4:68]
                dfl_mask = pos_mask_reg[:, :, 4:68]
                dfl_loss = self.mse_loss(dfl_pred * dfl_mask, dfl_targets * dfl_mask)

                # åˆ†ç±»æŸå¤± - åªå¯¹æ­£æ ·æœ¬
                cls_loss = self.bce_loss(cls_pred * pos_mask_cls, cls_targets * pos_mask_cls)

                # ç›®æ ‡æ€§æŸå¤±
                obj_pred = jt.max(cls_pred, dim=-1)
                if isinstance(obj_pred, tuple):
                    obj_pred = obj_pred[0]

                # æ­£æ ·æœ¬ç›®æ ‡æ€§æŸå¤±
                pos_obj_loss = self.bce_loss(obj_pred * obj_mask, obj_mask)

                # è´Ÿæ ·æœ¬ç›®æ ‡æ€§æŸå¤±
                neg_mask = 1.0 - obj_mask
                neg_obj_loss = self.bce_loss(obj_pred * neg_mask, jt.zeros_like(obj_pred) * neg_mask)

                # æ€»ç›®æ ‡æ€§æŸå¤±
                obj_loss = pos_obj_loss + neg_obj_loss

                # ä¿®å¤6: çœŸå®YOLOæƒé‡ç»„åˆ
                total_loss = (self.lambda_box * coord_loss +
                             self.lambda_dfl * dfl_loss +
                             self.lambda_cls * cls_loss +
                             self.lambda_obj * obj_loss)

                # ä¿®å¤7: æ­£æ ·æœ¬æ•°é‡å½’ä¸€åŒ– (é‡è¦ï¼)
                if total_pos_samples > 0:
                    total_loss = total_loss * (batch_size * num_anchors) / total_pos_samples

                if step_num % 10 == 0:
                    print(f"    ğŸ“Š çœŸå®YOLOæŸå¤±: coord={coord_loss.item():.3f}, dfl={dfl_loss.item():.3f}, cls={cls_loss.item():.3f}, obj={obj_loss.item():.3f}")
                    print(f"        æ­£æ ·æœ¬æ•°: {total_pos_samples}, æ€»æŸå¤±: {total_loss.item():.3f}")

                return total_loss

        self.loss_fn = FixedYOLOLoss()
        self.use_full_loss = True
        print(f"âœ… ä¿®å¤ç‰ˆçœŸå®YOLOæŸå¤±å‡½æ•°")
        return self.loss_fn

    def create_optimizer(self):
        """åˆ›å»ºä¼˜åŒ–å™¨ - å®˜æ–¹é…ç½®"""
        # å®˜æ–¹é…ç½®
        lr = 0.01
        momentum = 0.937
        weight_decay = 0.0005

        # ä½¿ç”¨SGDä¼˜åŒ–å™¨ (å®˜æ–¹æ¨è)
        self.optimizer = jt.optim.SGD(
            self.model.parameters(),
            lr=lr,
            momentum=momentum,
            weight_decay=weight_decay
        )

        print(f"âœ… å®˜æ–¹SGDä¼˜åŒ–å™¨: lr={lr}, momentum={momentum}, weight_decay={weight_decay}")
        return self.optimizer

    def create_dataloader(self, images_data):
        """åˆ›å»ºæ•°æ®åŠ è½½å™¨"""
        class OfficialDataset:
            def __init__(self, images_data, batch_size):
                self.images_data = images_data
                self.batch_size = batch_size

            def __len__(self):
                return (len(self.images_data) + self.batch_size - 1) // self.batch_size

            def __getitem__(self, idx):
                # ä½¿ç”¨éšæœºæ•°æ® (å¯é€‰æ‹©åŠ è½½çœŸå®å›¾ç‰‡)
                batch_images = []
                for _ in range(self.batch_size):
                    img_tensor = jt.randn(3, 640, 640)
                    batch_images.append(img_tensor)

                return jt.stack(batch_images)

        dataset = OfficialDataset(images_data, self.batch_size)
        return dataset

    def train_epoch(self, dataloader, epoch):
        """è®­ç»ƒä¸€ä¸ªepoch"""
        self.model.train()

        total_loss = 0.0
        num_batches = len(dataloader)

        # è¿›åº¦æ¡
        pbar = tqdm(range(num_batches),
                   desc=f'Epoch {epoch+1}/{self.epochs}',
                   ncols=80,
                   leave=False)

        for batch_idx in pbar:
            # è·å–æ•°æ®
            images = dataloader[batch_idx]

            # å‰å‘ä¼ æ’­
            outputs = self.model(images)

            # è®¡ç®—æŸå¤± - å®Œå…¨ç»•è¿‡ComputeLossï¼Œç›´æ¥ä½¿ç”¨å¼ºåˆ¶æ¢¯åº¦æŸå¤±
            loss = self.loss_fn(outputs, None, epoch, batch_idx)

            # åå‘ä¼ æ’­
            self.optimizer.zero_grad()
            self.optimizer.backward(loss)
            self.optimizer.step()

            # ç»Ÿè®¡
            batch_loss = loss.item()
            total_loss += batch_loss
            avg_loss = total_loss / (batch_idx + 1)

            # æ›´æ–°è¿›åº¦æ¡
            pbar.set_postfix({'loss': f'{avg_loss:.2f}'})

            # æ˜¾å­˜æ¸…ç†
            if batch_idx % 50 == 0:
                jt.sync_all()
                jt.gc()

        return total_loss / num_batches

    def save_checkpoint(self, epoch, loss):
        """ä¿å­˜æ£€æŸ¥ç‚¹"""
        if loss < self.best_loss:
            self.best_loss = loss
            checkpoint = {
                'epoch': epoch,
                'model': self.model.state_dict(),
                'optimizer': self.optimizer.state_dict(),
                'loss': loss,
            }
            best_path = str(self.output_dir / f'best_{self.name}.pkl')
            jt.save(checkpoint, best_path)
            print(f"    ğŸ’¾ ä¿å­˜æœ€ä½³æ¨¡å‹: {best_path}")

    def train(self):
        """ä¸»è®­ç»ƒå¾ªç¯"""
        print(f"ğŸš€ å¼€å§‹å®Œæ•´å®˜æ–¹Gold-YOLO Smallè®­ç»ƒ...")
        print("=" * 60)

        # åŠ è½½æ•°æ®
        images_data = self.load_data()
        if images_data is None:
            return

        # åˆ›å»ºç»„ä»¶
        self.create_model()
        self.create_loss_function()
        self.create_optimizer()

        # åˆ›å»ºæ•°æ®åŠ è½½å™¨
        dataloader = self.create_dataloader(images_data)

        print(f"æ‰¹æ¬¡æ•°é‡: {len(dataloader)}")
        print("=" * 60)

        start_time = time.time()

        # è®­ç»ƒå¾ªç¯
        for epoch in range(self.epochs):
            # è®­ç»ƒ
            train_loss = self.train_epoch(dataloader, epoch)
            self.train_losses.append(train_loss)

            # ä¿å­˜æ£€æŸ¥ç‚¹
            self.save_checkpoint(epoch, train_loss)

            # æ¯5ä¸ªepochæ˜¾ç¤ºè¿›åº¦
            if (epoch + 1) % 5 == 0:
                elapsed = time.time() - start_time
                print(f"Epoch {epoch+1:2d}/{self.epochs} | "
                      f"Loss: {train_loss:.2f} | "
                      f"Best: {self.best_loss:.2f} | "
                      f"Speed: {elapsed/(epoch+1):.1f}s/epoch")

        # è®­ç»ƒå®Œæˆ
        total_time = time.time() - start_time
        print("=" * 60)
        print(f"âœ… å®Œæ•´å®˜æ–¹Smallè®­ç»ƒå®Œæˆï¼")
        print(f"æ€»æ—¶é—´: {total_time/60:.1f}åˆ†é’Ÿ")
        print(f"å¹³å‡é€Ÿåº¦: {total_time/self.epochs:.1f}ç§’/epoch")
        print(f"æœ€ä½³æŸå¤±: {self.best_loss:.2f}")

        # ä¿å­˜è®­ç»ƒæ—¥å¿—
        log_data = {
            'framework': 'Jittor',
            'epochs': self.epochs,
            'batch_size': self.batch_size,
            'num_images': self.num_images,
            'train_losses': self.train_losses,
            'best_loss': self.best_loss,
            'total_time': total_time,
            'avg_speed': total_time / self.epochs
        }

        with open(self.output_dir / 'jittor_train_log.json', 'w') as f:
            json.dump(log_data, f, indent=2)

        return log_data

def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='å®Œæ•´å®˜æ–¹Gold-YOLO Smallè®­ç»ƒ')
    parser.add_argument('--data-root', type=str,
                       default='/home/kyc/project/GOLD-YOLO/data/coco2017_val',
                       help='æ•°æ®æ ¹ç›®å½•')
    parser.add_argument('--num-images', type=int, default=100, help='è®­ç»ƒå›¾ç‰‡æ•°é‡')
    parser.add_argument('--batch-size', type=int, default=8, help='æ‰¹æ¬¡å¤§å°')
    parser.add_argument('--epochs', type=int, default=50, help='è®­ç»ƒè½®æ•°')
    parser.add_argument('--name', type=str, default='jittor_train', help='å®éªŒåç§°')

    args = parser.parse_args()

    # æ£€æŸ¥æ•°æ®ç›®å½•
    data_root = Path(args.data_root)
    if not data_root.exists():
        print(f"âŒ æ•°æ®ç›®å½•ä¸å­˜åœ¨: {data_root}")
        return

    # åˆ›å»ºè®­ç»ƒå™¨
    trainer = FullOfficialSmallTrainer(
        data_root=args.data_root,
        num_images=args.num_images,
        batch_size=args.batch_size,
        epochs=args.epochs,
        name=args.name
    )

    # å¼€å§‹è®­ç»ƒ
    trainer.train()

if __name__ == "__main__":
    main()
