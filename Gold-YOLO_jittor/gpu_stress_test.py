#!/usr/bin/env python3
"""
GPUæ»¡è½½å‹åŠ›æµ‹è¯•è„šæœ¬ - è·‘æ»¡GPUæ€§èƒ½
"""

import os
import time
import threading
import argparse
from concurrent.futures import ThreadPoolExecutor

# è®¾ç½®GPUç¯å¢ƒ
os.environ['CUDA_VISIBLE_DEVICES'] = '0'
os.environ['JT_SYNC'] = '0'
os.environ['JT_CUDA_MEMORY_POOL'] = '1'
os.environ['JT_ENABLE_TUNER'] = '1'

import jittor as jt

# å¼ºåˆ¶GPUæ¨¡å¼
jt.flags.use_cuda = 1
jt.flags.lazy_execution = 0

def gpu_compute_task(task_id, duration=60, matrix_size=4000):
    """GPUè®¡ç®—ä»»åŠ¡"""
    print(f"ğŸ”¥ å¯åŠ¨GPUä»»åŠ¡ {task_id} - çŸ©é˜µå¤§å°: {matrix_size}x{matrix_size}")
    
    start_time = time.time()
    iteration = 0
    
    while time.time() - start_time < duration:
        # åˆ›å»ºå¤§çŸ©é˜µ
        a = jt.randn(matrix_size, matrix_size)
        b = jt.randn(matrix_size, matrix_size)
        
        # çŸ©é˜µä¹˜æ³•
        c = jt.matmul(a, b)
        
        # æ›´å¤šGPUå¯†é›†æ“ä½œ
        d = jt.nn.relu(c)
        e = jt.nn.softmax(d, dim=1)
        f = jt.sum(e)
        
        # å·ç§¯æ“ä½œ
        conv_input = jt.randn(32, 64, 256, 256)
        conv = jt.nn.Conv2d(64, 128, 3, padding=1)
        conv_output = conv(conv_input)
        conv_result = jt.sum(conv_output)
        
        # å¼ºåˆ¶åŒæ­¥ï¼Œç¡®ä¿è®¡ç®—å®Œæˆ
        result = float(f) + float(conv_result)
        
        iteration += 1
        if iteration % 10 == 0:
            elapsed = time.time() - start_time
            print(f"   ä»»åŠ¡ {task_id}: {iteration} æ¬¡è¿­ä»£, {elapsed:.1f}s, ç»“æœ: {result:.2f}")
    
    total_time = time.time() - start_time
    print(f"âœ… ä»»åŠ¡ {task_id} å®Œæˆ: {iteration} æ¬¡è¿­ä»£, æ€»æ—¶é—´: {total_time:.1f}s")
    return iteration

def monitor_gpu_usage(duration=60):
    """ç›‘æ§GPUä½¿ç”¨ç‡"""
    print("ğŸ“Š å¼€å§‹ç›‘æ§GPUä½¿ç”¨ç‡...")
    
    start_time = time.time()
    max_util = 0
    
    while time.time() - start_time < duration:
        try:
            import subprocess
            result = subprocess.run([
                'nvidia-smi', 
                '--query-gpu=utilization.gpu,memory.used,memory.total,temperature.gpu',
                '--format=csv,noheader,nounits'
            ], capture_output=True, text=True)
            
            if result.returncode == 0:
                data = result.stdout.strip().split(', ')
                gpu_util = int(data[0])
                mem_used = int(data[1])
                mem_total = int(data[2])
                temp = int(data[3])
                
                max_util = max(max_util, gpu_util)
                mem_percent = (mem_used / mem_total) * 100
                
                print(f"ğŸ”¥ GPU: {gpu_util}% | å†…å­˜: {mem_used}/{mem_total}MB ({mem_percent:.1f}%) | æ¸©åº¦: {temp}Â°C")
            
        except Exception as e:
            print(f"âš ï¸ ç›‘æ§é”™è¯¯: {e}")
        
        time.sleep(2)
    
    print(f"ğŸ“Š ç›‘æ§å®Œæˆ - æœ€å¤§GPUåˆ©ç”¨ç‡: {max_util}%")
    return max_util

def stress_test_single_thread(duration=60):
    """å•çº¿ç¨‹GPUå‹åŠ›æµ‹è¯•"""
    print(f"\nğŸš€ å•çº¿ç¨‹GPUå‹åŠ›æµ‹è¯• ({duration}ç§’)")
    print("=" * 50)
    
    # å¯åŠ¨ç›‘æ§çº¿ç¨‹
    monitor_thread = threading.Thread(target=monitor_gpu_usage, args=(duration,))
    monitor_thread.start()
    
    # æ‰§è¡ŒGPUè®¡ç®—
    iterations = gpu_compute_task(1, duration, 4000)
    
    # ç­‰å¾…ç›‘æ§å®Œæˆ
    monitor_thread.join()
    
    print(f"âœ… å•çº¿ç¨‹æµ‹è¯•å®Œæˆ: {iterations} æ¬¡è¿­ä»£")
    return iterations

def stress_test_multi_thread(duration=60, num_threads=4):
    """å¤šçº¿ç¨‹GPUå‹åŠ›æµ‹è¯•"""
    print(f"\nğŸš€ å¤šçº¿ç¨‹GPUå‹åŠ›æµ‹è¯• ({num_threads}çº¿ç¨‹, {duration}ç§’)")
    print("=" * 50)
    
    # å¯åŠ¨ç›‘æ§çº¿ç¨‹
    monitor_thread = threading.Thread(target=monitor_gpu_usage, args=(duration,))
    monitor_thread.start()
    
    # å¯åŠ¨å¤šä¸ªGPUè®¡ç®—çº¿ç¨‹
    with ThreadPoolExecutor(max_workers=num_threads) as executor:
        futures = []
        for i in range(num_threads):
            matrix_size = 3000 + i * 200  # ä¸åŒå¤§å°çš„çŸ©é˜µ
            future = executor.submit(gpu_compute_task, i+1, duration, matrix_size)
            futures.append(future)
        
        # ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆ
        total_iterations = 0
        for future in futures:
            iterations = future.result()
            total_iterations += iterations
    
    # ç­‰å¾…ç›‘æ§å®Œæˆ
    monitor_thread.join()
    
    print(f"âœ… å¤šçº¿ç¨‹æµ‹è¯•å®Œæˆ: æ€»è®¡ {total_iterations} æ¬¡è¿­ä»£")
    return total_iterations

def extreme_stress_test(duration=60):
    """æé™GPUå‹åŠ›æµ‹è¯•"""
    print(f"\nğŸ”¥ æé™GPUå‹åŠ›æµ‹è¯• ({duration}ç§’)")
    print("=" * 50)
    
    # å¯åŠ¨ç›‘æ§çº¿ç¨‹
    monitor_thread = threading.Thread(target=monitor_gpu_usage, args=(duration,))
    monitor_thread.start()
    
    start_time = time.time()
    iteration = 0
    
    while time.time() - start_time < duration:
        # åŒæ—¶è¿›è¡Œå¤šç§GPUå¯†é›†æ“ä½œ
        tasks = []
        
        # ä»»åŠ¡1: å¤§çŸ©é˜µä¹˜æ³•
        a1 = jt.randn(5000, 5000)
        b1 = jt.randn(5000, 5000)
        c1 = jt.matmul(a1, b1)
        
        # ä»»åŠ¡2: æ·±åº¦å·ç§¯
        conv_input = jt.randn(64, 256, 512, 512)
        conv1 = jt.nn.Conv2d(256, 512, 3, padding=1)
        conv2 = jt.nn.Conv2d(512, 1024, 3, padding=1)
        x = conv1(conv_input)
        x = jt.nn.relu(x)
        x = conv2(x)
        
        # ä»»åŠ¡3: å¤§è§„æ¨¡softmax
        softmax_input = jt.randn(1000, 10000)
        softmax_output = jt.nn.softmax(softmax_input, dim=1)
        
        # ä»»åŠ¡4: æ‰¹é‡å½’ä¸€åŒ–
        bn_input = jt.randn(128, 1024, 64, 64)
        bn = jt.nn.BatchNorm2d(1024)
        bn_output = bn(bn_input)
        
        # å¼ºåˆ¶åŒæ­¥æ‰€æœ‰è®¡ç®—
        result1 = float(jt.sum(c1))
        result2 = float(jt.sum(x))
        result3 = float(jt.sum(softmax_output))
        result4 = float(jt.sum(bn_output))
        
        total_result = result1 + result2 + result3 + result4
        
        iteration += 1
        if iteration % 5 == 0:
            elapsed = time.time() - start_time
            print(f"   æé™æµ‹è¯•: {iteration} æ¬¡è¿­ä»£, {elapsed:.1f}s, ç»“æœ: {total_result:.2f}")
    
    # ç­‰å¾…ç›‘æ§å®Œæˆ
    monitor_thread.join()
    
    total_time = time.time() - start_time
    print(f"ğŸ”¥ æé™æµ‹è¯•å®Œæˆ: {iteration} æ¬¡è¿­ä»£, æ€»æ—¶é—´: {total_time:.1f}s")
    return iteration

def main():
    parser = argparse.ArgumentParser(description='GPUæ»¡è½½å‹åŠ›æµ‹è¯•')
    parser.add_argument('--duration', type=int, default=60, help='æµ‹è¯•æ—¶é•¿(ç§’)')
    parser.add_argument('--mode', type=str, default='all', 
                       choices=['single', 'multi', 'extreme', 'all'],
                       help='æµ‹è¯•æ¨¡å¼')
    parser.add_argument('--threads', type=int, default=4, help='å¤šçº¿ç¨‹æ•°é‡')
    
    args = parser.parse_args()
    
    print(f"ğŸš€ GPUæ»¡è½½å‹åŠ›æµ‹è¯•")
    print(f"   Jittorç‰ˆæœ¬: {jt.__version__}")
    print(f"   GPUå¯ç”¨: {jt.has_cuda}")
    print(f"   use_cuda: {jt.flags.use_cuda}")
    print(f"   æµ‹è¯•æ—¶é•¿: {args.duration}ç§’")
    
    if args.mode in ['single', 'all']:
        stress_test_single_thread(args.duration)
    
    if args.mode in ['multi', 'all']:
        stress_test_multi_thread(args.duration, args.threads)
    
    if args.mode in ['extreme', 'all']:
        extreme_stress_test(args.duration)
    
    print(f"\nğŸ‰ æ‰€æœ‰GPUå‹åŠ›æµ‹è¯•å®Œæˆï¼")

if __name__ == "__main__":
    main()
