#!/usr/bin/env python3
"""
å®Œå…¨å¯¹é½PyTorchç‰ˆæœ¬çš„æ¨ç†è„šæœ¬
è®­ç»ƒæ¨¡å‹å¹¶å±•ç¤ºé¢„æµ‹ç»“æœ
"""

import os
import sys
import cv2
import numpy as np
import jittor as jt
from pathlib import Path
import time
import math

# æ·»åŠ è·¯å¾„
sys.path.append('.')
sys.path.append('./yolov6')

from models.perfect_gold_yolo import create_perfect_gold_yolo_model
from yolov6.data.data_augment import letterbox
from yolov6.models.pytorch_aligned_losses import ComputeLoss
from yolov6.utils.nms import non_max_suppression

# VOCæ•°æ®é›†ç±»åˆ«åç§°
VOC_CLASSES = [
    'aeroplane', 'bicycle', 'bird', 'boat', 'bottle', 'bus', 'car', 'cat',
    'chair', 'cow', 'diningtable', 'dog', 'horse', 'motorbike', 'person',
    'pottedplant', 'sheep', 'sofa', 'train', 'tvmonitor'
]

def cosine_lr_scheduler(epoch, total_epochs, lr0, lrf):
    """Cosineå­¦ä¹ ç‡è°ƒåº¦å™¨"""
    return lrf + (lr0 - lrf) * (1 + math.cos(math.pi * epoch / total_epochs)) / 2

def draw_bbox(img, bbox, label, conf, color=(0, 255, 0)):
    """ç»˜åˆ¶è¾¹ç•Œæ¡†"""
    x1, y1, x2, y2 = bbox
    x1, y1, x2, y2 = int(x1), int(y1), int(x2), int(y2)
    
    # ç»˜åˆ¶è¾¹ç•Œæ¡†
    cv2.rectangle(img, (x1, y1), (x2, y2), color, 2)
    
    # ç»˜åˆ¶æ ‡ç­¾
    label_text = f"{VOC_CLASSES[int(label)]}: {conf:.2f}"
    label_size = cv2.getTextSize(label_text, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 2)[0]
    cv2.rectangle(img, (x1, y1 - label_size[1] - 10), (x1 + label_size[0], y1), color, -1)
    cv2.putText(img, label_text, (x1, y1 - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 2)

def train_and_infer():
    """è®­ç»ƒæ¨¡å‹å¹¶è¿›è¡Œæ¨ç†"""
    print(f"ğŸ¯ PyTorchå¯¹é½ç‰ˆæœ¬ï¼šè®­ç»ƒå¹¶æ¨ç†")
    print("=" * 80)
    
    # å‡†å¤‡æ•°æ®
    label_file = "/home/kyc/project/GOLD-YOLO/2008_001420.txt"
    img_path = "/home/kyc/project/GOLD-YOLO/2008_001420.jpg"
    
    # è¯»å–æ ‡æ³¨
    annotations = []
    with open(label_file, 'r') as f:
        for line in f:
            line = line.strip()
            if line:
                parts = line.split()
                if len(parts) >= 5:
                    cls_id = int(parts[0])
                    x_center = float(parts[1])
                    y_center = float(parts[2])
                    width = float(parts[3])
                    height = float(parts[4])
                    annotations.append([cls_id, x_center, y_center, width, height])
    
    # è¯»å–å›¾åƒ
    original_img = cv2.imread(img_path)
    img_height, img_width = original_img.shape[:2]
    
    # é¢„å¤„ç†å›¾åƒ - å¯¹é½PyTorchç‰ˆæœ¬
    img_size = 640  # ä½¿ç”¨PyTorchç‰ˆæœ¬çš„é»˜è®¤å°ºå¯¸
    img = letterbox(original_img, new_shape=img_size, stride=32, auto=False)[0]
    img_tensor_input = img.transpose((2, 0, 1))[::-1]
    img_tensor_input = np.ascontiguousarray(img_tensor_input)
    img_tensor_input = img_tensor_input.astype(np.float32) / 255.0
    img_tensor = jt.array(img_tensor_input).unsqueeze(0)
    
    # å‡†å¤‡æ ‡ç­¾
    targets = []
    for ann in annotations:
        cls_id, x_center, y_center, width, height = ann
        targets.append([0, cls_id, x_center, y_center, width, height])
    targets_tensor = jt.array(targets, dtype=jt.float32).unsqueeze(0)
    
    print(f"ğŸ“Š æ•°æ®å‡†å¤‡:")
    print(f"   åŸå§‹å›¾åƒå°ºå¯¸: {img_width}x{img_height}")
    print(f"   é¢„å¤„ç†åå°ºå¯¸: {img.shape}")
    print(f"   ç›®æ ‡æ•°é‡: {len(annotations)}ä¸ª")
    for i, ann in enumerate(annotations):
        cls_id, x_center, y_center, width, height = ann
        print(f"     ç›®æ ‡{i+1}: {VOC_CLASSES[cls_id]} ({x_center:.3f},{y_center:.3f}) {width:.3f}x{height:.3f}")
    
    # åˆ›å»ºæ¨¡å‹
    print(f"\nğŸ¯ åˆ›å»ºæ¨¡å‹:")
    model = create_perfect_gold_yolo_model()
    model.train()
    
    # åˆ›å»ºæŸå¤±å‡½æ•° - å®Œå…¨å¯¹é½PyTorchç‰ˆæœ¬
    loss_fn = ComputeLoss(
        num_classes=20,
        ori_img_size=img_size,
        warmup_epoch=0,
        use_dfl=False,
        reg_max=0,
        fpn_strides=[8, 16, 32],
        grid_cell_size=5.0,
        grid_cell_offset=0.5,
        loss_weight={'class': 1.0, 'iou': 2.5, 'dfl': 0.5}
    )
    
    # ä½¿ç”¨PyTorchç‰ˆæœ¬çš„è¶…å‚æ•°
    lr0 = 0.02
    lrf = 0.01
    momentum = 0.937
    weight_decay = 0.0005
    total_epochs = 50  # å‡å°‘è½®æ¬¡ä»¥ä¾¿å¿«é€Ÿçœ‹åˆ°ç»“æœ
    
    optimizer = jt.optim.SGD(
        model.parameters(), 
        lr=lr0, 
        momentum=momentum, 
        weight_decay=weight_decay
    )
    
    print(f"\nğŸ”§ å¼€å§‹è®­ç»ƒ:")
    print(f"   è®­ç»ƒè½®æ¬¡: {total_epochs}")
    print(f"   å­¦ä¹ ç‡: {lr0} -> {lrf} (Cosine)")
    print(f"   å›¾åƒå°ºå¯¸: {img_size}x{img_size}")
    print("-" * 60)
    
    # è®­ç»ƒè¿‡ç¨‹
    loss_history = []
    
    for epoch in range(total_epochs):
        # æ›´æ–°å­¦ä¹ ç‡
        current_lr = cosine_lr_scheduler(epoch, total_epochs, lr0, lrf)
        for param_group in optimizer.param_groups:
            param_group['lr'] = current_lr
        
        # å‰å‘ä¼ æ’­
        outputs = model(img_tensor)
        
        # è®¡ç®—æŸå¤±
        loss, loss_items = loss_fn(outputs, targets_tensor, epoch_num=epoch, step_num=epoch)
        
        # åå‘ä¼ æ’­
        optimizer.zero_grad()
        optimizer.backward(loss)
        optimizer.step()
        
        loss_value = float(loss.data.item())
        loss_history.append(loss_value)
        
        # æ¯10è½®æ‰“å°ä¸€æ¬¡
        if (epoch + 1) % 10 == 0:
            loss_items_values = [float(item.data.item()) for item in loss_items]
            print(f"   è½®æ¬¡ {epoch+1:2d}: æ€»æŸå¤±={loss_value:.6f}, IoU={loss_items_values[0]:.4f}, åˆ†ç±»={loss_items_values[2]:.4f}")
    
    # è®­ç»ƒå®Œæˆï¼Œè®¡ç®—æŸå¤±ä¸‹é™
    initial_loss = loss_history[0]
    final_loss = loss_history[-1]
    loss_reduction = (initial_loss - final_loss) / initial_loss * 100
    
    print(f"\nğŸ“Š è®­ç»ƒå®Œæˆ:")
    print(f"   åˆå§‹æŸå¤±: {initial_loss:.6f}")
    print(f"   æœ€ç»ˆæŸå¤±: {final_loss:.6f}")
    print(f"   æŸå¤±ä¸‹é™: {loss_reduction:.1f}%")
    
    # åˆ‡æ¢åˆ°æ¨ç†æ¨¡å¼ - å¯¹é½PyTorchç‰ˆæœ¬
    print(f"\nğŸ” å¼€å§‹æ¨ç† (å¯¹é½PyTorchç‰ˆæœ¬):")
    model.eval()
    
    with jt.no_grad():
        # æ¨ç†
        outputs = model(img_tensor)
        
        # ç¡®ä¿è¾“å‡ºæ ¼å¼æ­£ç¡®
        if len(outputs.shape) == 2:
            outputs = outputs.unsqueeze(0)  # [anchors, features] -> [1, anchors, features]
        
        print(f"   æ¨ç†è¾“å‡ºå½¢çŠ¶: {outputs.shape}")
        print(f"   é¢„æµ‹èŒƒå›´: [{float(outputs.min()):.6f}, {float(outputs.max()):.6f}]")
        
        # åº”ç”¨NMS - å¯¹é½PyTorchç‰ˆæœ¬å‚æ•°
        pred_results = non_max_suppression(
            outputs, 
            conf_thres=0.4,   # å¯¹é½PyTorchç‰ˆæœ¬é»˜è®¤å€¼
            iou_thres=0.45,   # å¯¹é½PyTorchç‰ˆæœ¬é»˜è®¤å€¼
            max_det=1000      # å¯¹é½PyTorchç‰ˆæœ¬é»˜è®¤å€¼
        )
        
        print(f"   NMSåæ£€æµ‹æ•°é‡: {len(pred_results[0]) if pred_results[0] is not None else 0}")
        
        # å¯è§†åŒ–ç»“æœ
        vis_img = original_img.copy()
        
        # ç»˜åˆ¶GTæ¡† (ç»¿è‰²)
        print(f"\nğŸ“‹ ç»˜åˆ¶GTæ¡† (ç»¿è‰²):")
        for i, ann in enumerate(annotations):
            cls_id, x_center, y_center, width, height = ann
            
            # è½¬æ¢ä¸ºåƒç´ åæ ‡
            x_center_px = x_center * img_width
            y_center_px = y_center * img_height
            width_px = width * img_width
            height_px = height * img_height
            
            x1 = x_center_px - width_px / 2
            y1 = y_center_px - height_px / 2
            x2 = x_center_px + width_px / 2
            y2 = y_center_px + height_px / 2
            
            draw_bbox(vis_img, [x1, y1, x2, y2], cls_id, 1.0, color=(0, 255, 0))
            print(f"   GT{i+1}: {VOC_CLASSES[cls_id]} ({x1:.0f},{y1:.0f}) -> ({x2:.0f},{y2:.0f})")
        
        # ç»˜åˆ¶é¢„æµ‹æ¡† (çº¢è‰²)
        print(f"\nğŸ¯ ç»˜åˆ¶é¢„æµ‹æ¡† (çº¢è‰²):")
        if pred_results[0] is not None and len(pred_results[0]) > 0:
            detections = pred_results[0]
            
            # ç¼©æ”¾åˆ°åŸå§‹å›¾åƒå°ºå¯¸
            scale_x = img_width / img_size
            scale_y = img_height / img_size
            
            print(f"   ç¼©æ”¾å› å­: x={scale_x:.3f}, y={scale_y:.3f}")
            
            # æ£€æŸ¥detectionsçš„æ ¼å¼
            print(f"   æ£€æµ‹ç»“æœæ ¼å¼: {type(detections)}, å½¢çŠ¶: {detections.shape if hasattr(detections, 'shape') else 'N/A'}")
            
            # ä¿®å¤NMSè¾“å‡ºæ ¼å¼é—®é¢˜
            try:
                detections_data = detections.data if hasattr(detections, 'data') else detections

                # æ£€æŸ¥å¹¶ä¿®å¤å½¢çŠ¶ [240,1,6] -> [240,6]
                if len(detections_data.shape) == 3 and detections_data.shape[1] == 1:
                    detections_data = detections_data.squeeze(1)  # [240,1,6] -> [240,6]

                print(f"   ä¿®å¤åæ£€æµ‹ç»“æœå½¢çŠ¶: {detections_data.shape}")

                for i in range(min(10, len(detections_data))):  # æœ€å¤šæ˜¾ç¤º10ä¸ªæ£€æµ‹
                    det = detections_data[i]

                    # æ£€æŸ¥detçš„æ ¼å¼
                    if hasattr(det, '__len__') and len(det) >= 6:
                        x1, y1, x2, y2, conf, cls_id = det[:6]
                        
                        # ç¼©æ”¾åæ ‡
                        x1 = float(x1) * scale_x
                        y1 = float(y1) * scale_y
                        x2 = float(x2) * scale_x
                        y2 = float(y2) * scale_y
                        conf = float(conf)
                        cls_id = int(cls_id)
                        
                        # æ£€æŸ¥åæ ‡å’Œç±»åˆ«æ˜¯å¦åˆç†
                        if 0 <= cls_id < len(VOC_CLASSES) and conf > 0.1:
                            draw_bbox(vis_img, [x1, y1, x2, y2], cls_id, conf, color=(0, 0, 255))
                            print(f"   é¢„æµ‹{i+1}: {VOC_CLASSES[cls_id]} {conf:.3f} ({x1:.0f},{y1:.0f}) -> ({x2:.0f},{y2:.0f})")
                        else:
                            print(f"   é¢„æµ‹{i+1}: æ— æ•ˆæ£€æµ‹ (cls={cls_id}, conf={conf:.3f})")
                    else:
                        print(f"   é¢„æµ‹{i+1}: æ ¼å¼é”™è¯¯ (é•¿åº¦={len(det) if hasattr(det, '__len__') else 'N/A'})")
                        
            except Exception as e:
                print(f"   âŒ è§£ææ£€æµ‹ç»“æœå¤±è´¥: {e}")
                print(f"   æ£€æµ‹ç»“æœç±»å‹: {type(detections)}")
                if hasattr(detections, 'shape'):
                    print(f"   æ£€æµ‹ç»“æœå½¢çŠ¶: {detections.shape}")
        else:
            print(f"   âŒ æ²¡æœ‰æ£€æµ‹åˆ°ä»»ä½•ç›®æ ‡")
            print(f"   å¯èƒ½åŸå› : 1) ç½®ä¿¡åº¦é˜ˆå€¼è¿‡é«˜ 2) æ¨¡å‹è®­ç»ƒä¸è¶³ 3) NMSå‚æ•°ä¸å½“")
        
        # ä¿å­˜å¯è§†åŒ–ç»“æœ
        output_path = "pytorch_aligned_inference_result.jpg"
        cv2.imwrite(output_path, vis_img)
        print(f"\nğŸ’¾ æ¨ç†ç»“æœå·²ä¿å­˜: {output_path}")
        
        # æ·»åŠ å›¾ä¾‹
        legend_height = 120
        legend_img = np.zeros((legend_height, img_width, 3), dtype=np.uint8)
        cv2.putText(legend_img, "Green: Ground Truth", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)
        cv2.putText(legend_img, "Red: Predictions", (10, 60), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)
        cv2.putText(legend_img, f"Loss Reduction: {loss_reduction:.1f}%", (10, 90), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)
        cv2.putText(legend_img, f"PyTorch Aligned Inference", (10, 115), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)
        
        # åˆå¹¶å›¾åƒå’Œå›¾ä¾‹
        combined_img = np.vstack([vis_img, legend_img])
        combined_path = "pytorch_aligned_inference_with_legend.jpg"
        cv2.imwrite(combined_path, combined_img)
        print(f"ğŸ’¾ å¸¦å›¾ä¾‹çš„ç»“æœå·²ä¿å­˜: {combined_path}")
        
        return loss_reduction, len(pred_results[0]) if pred_results[0] is not None else 0

def main():
    print("ğŸ¯ PyTorchå¯¹é½ç‰ˆæœ¬æ¨ç†æµ‹è¯•")
    print("=" * 80)
    
    try:
        loss_reduction, num_detections = train_and_infer()
        
        print(f"\n" + "=" * 80)
        print(f"ğŸ“Š PyTorchå¯¹é½ç‰ˆæœ¬æ¨ç†ç»“æœ:")
        print(f"=" * 80)
        print(f"   æŸå¤±ä¸‹é™: {loss_reduction:.1f}%")
        print(f"   æ£€æµ‹æ•°é‡: {num_detections}")
        
        if loss_reduction > 50:
            print(f"   âœ… æ¨¡å‹è®­ç»ƒæˆåŠŸ")
        else:
            print(f"   âš ï¸ æ¨¡å‹è®­ç»ƒæ•ˆæœä¸€èˆ¬")
        
        if num_detections > 0:
            print(f"   âœ… æ¨¡å‹èƒ½å¤Ÿæ£€æµ‹ç›®æ ‡")
        else:
            print(f"   âŒ æ¨¡å‹æœªæ£€æµ‹åˆ°ç›®æ ‡ï¼Œå¯èƒ½éœ€è¦:")
            print(f"     1. é™ä½ç½®ä¿¡åº¦é˜ˆå€¼")
            print(f"     2. å¢åŠ è®­ç»ƒè½®æ¬¡")
            print(f"     3. è°ƒæ•´æŸå¤±æƒé‡")
        
        print(f"\nğŸ¯ è¯·æŸ¥çœ‹ç”Ÿæˆçš„æ¨ç†å›¾åƒ:")
        print(f"   - pytorch_aligned_inference_result.jpg")
        print(f"   - pytorch_aligned_inference_with_legend.jpg")
        
    except Exception as e:
        print(f"\nâŒ æ¨ç†å¼‚å¸¸: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()
