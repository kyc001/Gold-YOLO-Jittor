#!/usr/bin/env python3
# -*- coding:utf-8 -*-
"""
æ·±å…¥æµ‹è¯•æ¢¯åº¦è£å‰ªä¿®å¤
"""

import os
os.environ['JT_SYNC'] = '1'

import jittor as jt
jt.flags.use_cuda = 0

def test_gradient_norm_calculation():
    """æ·±å…¥æµ‹è¯•æ¢¯åº¦èŒƒæ•°è®¡ç®—"""
    print("ğŸ” æ·±å…¥æµ‹è¯•æ¢¯åº¦èŒƒæ•°è®¡ç®—")
    
    try:
        # åˆ›å»ºç®€å•æ¨¡å‹
        from models.perfect_gold_yolo import create_perfect_gold_yolo_model
        model = create_perfect_gold_yolo_model('gold_yolo-n', 20)
        model.train()
        
        # åˆ›å»ºä¼˜åŒ–å™¨
        optimizer = jt.optim.SGD(model.parameters(), lr=0.01)
        
        # åˆ›å»ºæµ‹è¯•æ•°æ®
        images = jt.randn(1, 3, 640, 640, dtype='float32')
        
        # å‰å‘ä¼ æ’­
        outputs = model(images)
        
        # åˆ›å»ºè™šæ‹ŸæŸå¤±
        loss = jt.mean(outputs)
        
        # åå‘ä¼ æ’­
        optimizer.zero_grad()
        optimizer.backward(loss)
        
        print("âœ… åå‘ä¼ æ’­æˆåŠŸ")
        
        # æµ‹è¯•ä¸åŒçš„æ¢¯åº¦èŒƒæ•°è®¡ç®—æ–¹æ³•
        print("\nğŸ” æµ‹è¯•ä¸åŒçš„æ¢¯åº¦èŒƒæ•°è®¡ç®—æ–¹æ³•:")
        
        # æ–¹æ³•1: æ‰‹åŠ¨è®¡ç®—L2èŒƒæ•°
        print("   æ–¹æ³•1: æ‰‹åŠ¨è®¡ç®—L2èŒƒæ•°")
        total_norm_method1 = 0.0
        param_count = 0
        
        for name, param in model.named_parameters():
            if param.opt_grad(optimizer) is not None:
                grad = param.opt_grad(optimizer)
                print(f"     å‚æ•° {name}: æ¢¯åº¦å½¢çŠ¶ {list(grad.shape)}")
                
                # è®¡ç®—å¹³æ–¹å’Œ
                param_norm_squared = jt.sum(grad * grad)
                print(f"     å¹³æ–¹å’Œå½¢çŠ¶: {list(param_norm_squared.shape)}")
                
                # è½¬æ¢ä¸ºæ ‡é‡
                try:
                    if hasattr(param_norm_squared, 'data'):
                        norm_val = float(param_norm_squared.data)
                    else:
                        norm_val = float(param_norm_squared.numpy())
                    
                    print(f"     èŒƒæ•°å¹³æ–¹: {norm_val:.8f}")
                    total_norm_method1 += norm_val
                    param_count += 1
                    
                    if param_count >= 3:  # åªæµ‹è¯•å‰3ä¸ªå‚æ•°
                        break
                        
                except Exception as e:
                    print(f"     âŒ è½¬æ¢å¤±è´¥: {e}")
                    continue
        
        total_norm_method1 = (total_norm_method1 ** 0.5)
        print(f"   æ–¹æ³•1æ€»èŒƒæ•°: {total_norm_method1:.8f}")
        
        # æ–¹æ³•2: ä½¿ç”¨jt.normä½†å¤„ç†ç»“æœ
        print("\n   æ–¹æ³•2: ä½¿ç”¨jt.normå¤„ç†ç»“æœ")
        total_norm_method2 = 0.0
        param_count = 0
        
        for name, param in model.named_parameters():
            if param.opt_grad(optimizer) is not None:
                grad = param.opt_grad(optimizer)
                
                try:
                    # ä½¿ç”¨jt.norm
                    param_norm = jt.norm(grad)
                    print(f"     å‚æ•° {name}: normå½¢çŠ¶ {list(param_norm.shape)}")
                    
                    # å¦‚æœnormç»“æœä¸æ˜¯æ ‡é‡ï¼Œéœ€è¦è¿›ä¸€æ­¥å¤„ç†
                    if param_norm.numel() > 1:
                        # å¦‚æœnormè¿”å›çš„ä¸æ˜¯æ ‡é‡ï¼Œè®¡ç®—å…¶æ€»å’Œ
                        param_norm = jt.sum(param_norm)
                    
                    # è½¬æ¢ä¸ºæ ‡é‡
                    norm_val = float(param_norm.numpy())
                    print(f"     èŒƒæ•°å€¼: {norm_val:.8f}")
                    total_norm_method2 += norm_val ** 2
                    param_count += 1
                    
                    if param_count >= 3:  # åªæµ‹è¯•å‰3ä¸ªå‚æ•°
                        break
                        
                except Exception as e:
                    print(f"     âŒ normæ–¹æ³•å¤±è´¥: {e}")
                    continue
        
        total_norm_method2 = (total_norm_method2 ** 0.5)
        print(f"   æ–¹æ³•2æ€»èŒƒæ•°: {total_norm_method2:.8f}")
        
        # æ–¹æ³•3: æœ€å®‰å…¨çš„æ–¹æ³• - é€å…ƒç´ è®¡ç®—
        print("\n   æ–¹æ³•3: æœ€å®‰å…¨çš„é€å…ƒç´ æ–¹æ³•")
        total_norm_method3 = 0.0
        param_count = 0
        
        for name, param in model.named_parameters():
            if param.opt_grad(optimizer) is not None:
                grad = param.opt_grad(optimizer)
                
                try:
                    # å°†æ¢¯åº¦å±•å¹³å¹¶è®¡ç®—èŒƒæ•°
                    grad_flat = grad.reshape(-1)  # å±•å¹³ä¸º1ç»´
                    grad_norm_squared = jt.sum(grad_flat * grad_flat)
                    
                    # è½¬æ¢ä¸ºæ ‡é‡
                    norm_val = float(grad_norm_squared.numpy())
                    print(f"     å‚æ•° {name}: å±•å¹³åèŒƒæ•°å¹³æ–¹ {norm_val:.8f}")
                    total_norm_method3 += norm_val
                    param_count += 1
                    
                    if param_count >= 3:  # åªæµ‹è¯•å‰3ä¸ªå‚æ•°
                        break
                        
                except Exception as e:
                    print(f"     âŒ å±•å¹³æ–¹æ³•å¤±è´¥: {e}")
                    continue
        
        total_norm_method3 = (total_norm_method3 ** 0.5)
        print(f"   æ–¹æ³•3æ€»èŒƒæ•°: {total_norm_method3:.8f}")
        
        # é€‰æ‹©æœ€ä½³æ–¹æ³•
        if total_norm_method1 > 0:
            print(f"\nâœ… æ–¹æ³•1æˆåŠŸï¼Œä½¿ç”¨æ‰‹åŠ¨L2èŒƒæ•°è®¡ç®—")
            return True, 1
        elif total_norm_method3 > 0:
            print(f"\nâœ… æ–¹æ³•3æˆåŠŸï¼Œä½¿ç”¨å±•å¹³æ–¹æ³•")
            return True, 3
        else:
            print(f"\nâŒ æ‰€æœ‰æ–¹æ³•éƒ½å¤±è´¥")
            return False, 0
        
    except Exception as e:
        print(f"âŒ æ¢¯åº¦èŒƒæ•°æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False, 0


def test_complete_gradient_clipping():
    """æµ‹è¯•å®Œæ•´çš„æ¢¯åº¦è£å‰ªæµç¨‹"""
    print("\nğŸ” æµ‹è¯•å®Œæ•´æ¢¯åº¦è£å‰ªæµç¨‹")
    
    try:
        # åˆ›å»ºæ¨¡å‹å’Œä¼˜åŒ–å™¨
        from models.perfect_gold_yolo import create_perfect_gold_yolo_model
        model = create_perfect_gold_yolo_model('gold_yolo-n', 20)
        model.train()
        
        optimizer = jt.optim.SGD(model.parameters(), lr=0.01)
        
        # åˆ›å»ºæµ‹è¯•æ•°æ®
        images = jt.randn(2, 3, 640, 640, dtype='float32')
        
        # å‰å‘ä¼ æ’­
        outputs = model(images)
        loss = jt.mean(outputs)
        
        # åå‘ä¼ æ’­
        optimizer.zero_grad()
        optimizer.backward(loss)
        
        print("âœ… å‰å‘å’Œåå‘ä¼ æ’­æˆåŠŸ")
        
        # å®ç°å®Œæ•´çš„æ¢¯åº¦è£å‰ª
        print("   å¼€å§‹æ¢¯åº¦è£å‰ª...")
        
        max_norm = 5.0
        total_norm = 0.0
        param_count = 0
        
        # è®¡ç®—æ€»æ¢¯åº¦èŒƒæ•°
        for param in model.parameters():
            if param.opt_grad(optimizer) is not None:
                grad = param.opt_grad(optimizer)
                
                # ä½¿ç”¨æœ€å®‰å…¨çš„æ–¹æ³•ï¼šå±•å¹³åè®¡ç®—
                grad_flat = grad.reshape(-1)
                grad_norm_squared = jt.sum(grad_flat * grad_flat)
                norm_val = float(grad_norm_squared.numpy())
                total_norm += norm_val
                param_count += 1
        
        total_norm = (total_norm ** 0.5)
        print(f"   æ€»æ¢¯åº¦èŒƒæ•°: {total_norm:.8f}")
        print(f"   å‚æ•°æ•°é‡: {param_count}")
        
        # è®¡ç®—è£å‰ªç³»æ•°
        clip_coef = max_norm / (total_norm + 1e-6)
        print(f"   è£å‰ªç³»æ•°: {clip_coef:.8f}")
        
        # åº”ç”¨æ¢¯åº¦è£å‰ª
        if clip_coef < 1.0:
            print(f"   éœ€è¦è£å‰ªï¼Œåº”ç”¨ç³»æ•° {clip_coef:.6f}")
            for param in model.parameters():
                if param.opt_grad(optimizer) is not None:
                    param.opt_grad(optimizer).data.mul_(clip_coef)
        else:
            print(f"   ä¸éœ€è¦è£å‰ª")
        
        # å‚æ•°æ›´æ–°
        optimizer.step()
        
        print("âœ… æ¢¯åº¦è£å‰ªå’Œå‚æ•°æ›´æ–°æˆåŠŸ")
        
        return True
        
    except Exception as e:
        print(f"âŒ å®Œæ•´æ¢¯åº¦è£å‰ªæµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False


def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("ğŸš€ æ·±å…¥æ¢¯åº¦è£å‰ªä¿®å¤æµ‹è¯•")
    print("=" * 60)
    print("ğŸ¯ ä¸æ“…è‡ªç®€åŒ–ï¼Œæ·±å…¥è§£å†³é—®é¢˜")
    print("=" * 60)
    
    # æµ‹è¯•1: æ¢¯åº¦èŒƒæ•°è®¡ç®—æ–¹æ³•
    success1, best_method = test_gradient_norm_calculation()
    
    # æµ‹è¯•2: å®Œæ•´æ¢¯åº¦è£å‰ªæµç¨‹
    success2 = test_complete_gradient_clipping()
    
    print("\n" + "=" * 60)
    print("ğŸ¯ æ·±å…¥ä¿®å¤æµ‹è¯•ç»“æœ")
    print("=" * 60)
    print(f"   æ¢¯åº¦èŒƒæ•°è®¡ç®—: {'âœ… ä¿®å¤æˆåŠŸ' if success1 else 'âŒ ä»æœ‰é—®é¢˜'}")
    if success1:
        print(f"   æœ€ä½³æ–¹æ³•: æ–¹æ³•{best_method}")
    print(f"   å®Œæ•´æ¢¯åº¦è£å‰ª: {'âœ… ä¿®å¤æˆåŠŸ' if success2 else 'âŒ ä»æœ‰é—®é¢˜'}")
    
    if success1 and success2:
        print("\nğŸ‰ æ¢¯åº¦è£å‰ªé—®é¢˜å®Œå…¨è§£å†³ï¼")
        print("âœ… æ‰¾åˆ°äº†æ­£ç¡®çš„æ¢¯åº¦èŒƒæ•°è®¡ç®—æ–¹æ³•")
        print("âœ… å®Œæ•´çš„æ¢¯åº¦è£å‰ªæµç¨‹æ­£å¸¸å·¥ä½œ")
        print("âœ… æ²¡æœ‰æ“…è‡ªç®€åŒ–ï¼Œæ·±å…¥è§£å†³äº†æ ¹æœ¬é—®é¢˜")
    else:
        print("\nâŒ è¿˜éœ€è¦è¿›ä¸€æ­¥æ·±å…¥ä¿®å¤")


if __name__ == "__main__":
    main()
