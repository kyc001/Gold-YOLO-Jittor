#!/usr/bin/env python3
"""
æµ‹è¯•æ¢¯åº¦çˆ†ç‚¸ä¿®å¤æ•ˆæœ
éªŒè¯VarifocalLossä¿®å¤åçš„æ¢¯åº¦æ˜¯å¦æ­£å¸¸
"""

import os
import sys
import numpy as np
import jittor as jt
from jittor import nn

sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from models.perfect_gold_yolo import create_perfect_gold_yolo_model

def test_gradient_fix():
    """æµ‹è¯•æ¢¯åº¦çˆ†ç‚¸ä¿®å¤æ•ˆæœ"""
    print("""
    â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
    â•‘              æµ‹è¯•æ¢¯åº¦çˆ†ç‚¸ä¿®å¤æ•ˆæœ                             â•‘
    â•‘                                                              â•‘
    â•‘  ğŸ”§ éªŒè¯VarifocalLossä¿®å¤åçš„æ¢¯åº¦æ˜¯å¦æ­£å¸¸                    â•‘
    â•‘  ğŸ“Š å¯¹æ¯”ä¿®å¤å‰åçš„æ¢¯åº¦å¤§å°                                   â•‘
    â•‘  ğŸ¯ ç¡®ä¿åˆ†ç±»å¤´èƒ½å¤Ÿæ­£å¸¸è®­ç»ƒ                                   â•‘
    â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    """)
    
    # åˆ›å»ºæ¨¡å‹
    print("ğŸ”§ åˆ›å»ºæ¨¡å‹...")
    model = create_perfect_gold_yolo_model('gold_yolo-n', num_classes=20)
    
    # é‡æ–°åˆå§‹åŒ–åˆ†ç±»å¤´
    print("ğŸ”§ é‡æ–°åˆå§‹åŒ–åˆ†ç±»å¤´...")
    for name, module in model.named_modules():
        if 'cls_pred' in name and isinstance(module, nn.Conv2d):
            jt.init.gauss_(module.weight, mean=0.0, std=0.01)
            if module.bias is not None:
                jt.init.constant_(module.bias, -2.0)
    
    # åˆ›å»ºä¿®å¤åçš„æŸå¤±å‡½æ•°
    print("ğŸ”§ åˆ›å»ºä¿®å¤åçš„æŸå¤±å‡½æ•°...")
    import importlib.util
    losses_file = os.path.join(os.path.dirname(__file__), 'yolov6', 'models', 'losses.py')
    spec = importlib.util.spec_from_file_location("losses", losses_file)
    losses_module = importlib.util.module_from_spec(spec)
    spec.loader.exec_module(losses_module)
    
    loss_fn = losses_module.ComputeLoss(
        num_classes=20,
        ori_img_size=640,
        warmup_epoch=0,
        use_dfl=False,
        reg_max=0,
        iou_type='siou',
        loss_weight={
            'class': 1.0,
            'iou': 2.5,
            'dfl': 0.5
        }
    )
    
    # åˆ›å»ºæµ‹è¯•æ•°æ®
    print("ğŸ”§ åˆ›å»ºæµ‹è¯•æ•°æ®...")
    images = jt.randn(1, 3, 640, 640)
    targets = jt.array([[0, 0.5, 0.5, 0.8, 0.8, 0]], dtype='float32')
    
    # åˆ›å»ºä¼˜åŒ–å™¨
    optimizer = nn.SGD(model.parameters(), lr=0.01)
    
    print("ğŸ” æµ‹è¯•ä¿®å¤åçš„æ¢¯åº¦...")
    
    # å‰å‘ä¼ æ’­
    model.train()
    outputs = model(images)
    
    # è®¡ç®—æŸå¤±
    loss, loss_items = loss_fn(outputs, targets, epoch_num=1, step_num=1)
    
    print(f"ä¿®å¤åæŸå¤±è®¡ç®—ç»“æœ:")
    if loss is not None:
        print(f"   æ€»æŸå¤±: {float(loss.numpy()):.6f}")
        if loss_items is not None:
            try:
                loss_values = [float(item.numpy()) for item in loss_items]
                print(f"   æŸå¤±è¯¦æƒ…: {loss_values}")
                print(f"   åˆ†ç±»æŸå¤±: {loss_values[0]:.6f}")
                print(f"   IoUæŸå¤±: {loss_values[1]:.6f}")
                print(f"   DFLæŸå¤±: {loss_values[2]:.6f}")
            except:
                print(f"   æŸå¤±è¯¦æƒ…: {loss_items}")
    else:
        print(f"   âŒ æŸå¤±ä¸ºNone")
        return False
    
    # è®¡ç®—æ¢¯åº¦
    print(f"\nğŸ” åˆ†æä¿®å¤åçš„æ¢¯åº¦...")
    optimizer.zero_grad()
    optimizer.backward(loss)
    
    # æ£€æŸ¥åˆ†ç±»å¤´æ¢¯åº¦
    gradient_normal = True
    max_gradient = 0.0
    
    for name, param in model.named_parameters():
        if 'cls_pred' in name:
            grad = param.opt_grad(optimizer)
            if grad is not None:
                grad_min = float(grad.min().numpy())
                grad_max = float(grad.max().numpy())
                grad_mean = float(grad.mean().numpy())
                grad_std = float(grad.std().numpy())
                grad_abs_max = max(abs(grad_min), abs(grad_max))
                max_gradient = max(max_gradient, grad_abs_max)
                
                print(f"   {name}:")
                print(f"     æ¢¯åº¦èŒƒå›´: [{grad_min:.6f}, {grad_max:.6f}]")
                print(f"     æ¢¯åº¦å‡å€¼: {grad_mean:.6f}")
                print(f"     æ¢¯åº¦æ ‡å‡†å·®: {grad_std:.6f}")
                print(f"     æ¢¯åº¦ç»å¯¹å€¼æœ€å¤§: {grad_abs_max:.6f}")
                
                # æ£€æŸ¥æ¢¯åº¦æ˜¯å¦æ­£å¸¸
                if grad_abs_max > 10.0:
                    print(f"     âŒ æ¢¯åº¦ä»ç„¶è¿‡å¤§ï¼")
                    gradient_normal = False
                elif grad_abs_max > 1.0:
                    print(f"     âš ï¸ æ¢¯åº¦åå¤§ä½†å¯æ¥å—")
                else:
                    print(f"     âœ… æ¢¯åº¦æ­£å¸¸")
                
                # æ£€æŸ¥æ¢¯åº¦æ–¹å‘åˆ†å¸ƒ
                grad_numpy = grad.numpy().flatten()
                positive_ratio = np.sum(grad_numpy > 0) / len(grad_numpy) * 100
                negative_ratio = np.sum(grad_numpy < 0) / len(grad_numpy) * 100
                zero_ratio = np.sum(grad_numpy == 0) / len(grad_numpy) * 100
                
                print(f"     æ¢¯åº¦æ–¹å‘åˆ†å¸ƒ: æ­£å€¼{positive_ratio:.1f}%, è´Ÿå€¼{negative_ratio:.1f}%, é›¶å€¼{zero_ratio:.1f}%")
                
                # æ£€æŸ¥æ¢¯åº¦æ–¹å‘æ˜¯å¦åˆç†
                if positive_ratio == 100.0 or negative_ratio == 100.0:
                    print(f"     âš ï¸ æ¢¯åº¦æ–¹å‘å•ä¸€ï¼Œå¯èƒ½æœ‰é—®é¢˜")
                else:
                    print(f"     âœ… æ¢¯åº¦æ–¹å‘åˆ†å¸ƒåˆç†")
    
    print(f"\nğŸ“Š æ¢¯åº¦ä¿®å¤æ•ˆæœæ€»ç»“:")
    print(f"   æœ€å¤§æ¢¯åº¦ç»å¯¹å€¼: {max_gradient:.6f}")
    
    if gradient_normal and max_gradient < 10.0:
        print(f"   âœ… æ¢¯åº¦çˆ†ç‚¸é—®é¢˜å·²ä¿®å¤ï¼")
        print(f"   ğŸ¯ åˆ†ç±»å¤´ç°åœ¨å¯ä»¥æ­£å¸¸è®­ç»ƒ")
        return True
    elif max_gradient < 100.0:
        print(f"   âš ï¸ æ¢¯åº¦æœ‰æ‰€æ”¹å–„ä½†ä»éœ€ä¼˜åŒ–")
        return None
    else:
        print(f"   âŒ æ¢¯åº¦çˆ†ç‚¸é—®é¢˜ä»ç„¶å­˜åœ¨")
        return False

def test_training_stability():
    """æµ‹è¯•è®­ç»ƒç¨³å®šæ€§"""
    print(f"\nğŸ”§ æµ‹è¯•è®­ç»ƒç¨³å®šæ€§...")
    
    # åˆ›å»ºæ¨¡å‹
    model = create_perfect_gold_yolo_model('gold_yolo-n', num_classes=20)
    
    # é‡æ–°åˆå§‹åŒ–åˆ†ç±»å¤´
    for name, module in model.named_modules():
        if 'cls_pred' in name and isinstance(module, nn.Conv2d):
            jt.init.gauss_(module.weight, mean=0.0, std=0.01)
            if module.bias is not None:
                jt.init.constant_(module.bias, -2.0)
    
    # åˆ›å»ºæŸå¤±å‡½æ•°
    import importlib.util
    losses_file = os.path.join(os.path.dirname(__file__), 'yolov6', 'models', 'losses.py')
    spec = importlib.util.spec_from_file_location("losses", losses_file)
    losses_module = importlib.util.module_from_spec(spec)
    spec.loader.exec_module(losses_module)
    
    loss_fn = losses_module.ComputeLoss(
        num_classes=20,
        ori_img_size=640,
        warmup_epoch=0,
        use_dfl=False,
        reg_max=0,
        iou_type='siou',
        loss_weight={
            'class': 1.0,
            'iou': 2.5,
            'dfl': 0.5
        }
    )
    
    # åˆ›å»ºä¼˜åŒ–å™¨
    optimizer = nn.SGD(model.parameters(), lr=0.01)
    
    # åˆ›å»ºæµ‹è¯•æ•°æ®
    images = jt.randn(1, 3, 640, 640)
    targets = jt.array([[0, 0.5, 0.5, 0.8, 0.8, 0]], dtype='float32')
    
    print(f"è¿›è¡Œ10æ­¥è®­ç»ƒæµ‹è¯•...")
    
    model.train()
    loss_history = []
    cls_output_history = []
    
    for step in range(10):
        # å‰å‘ä¼ æ’­
        outputs = model(images)
        
        # è®°å½•åˆ†ç±»è¾“å‡º
        if isinstance(outputs, (list, tuple)) and len(outputs) >= 2:
            cls_output = outputs[1]  # [1, 8400, 20]
            cls_min = float(cls_output.min().numpy())
            cls_max = float(cls_output.max().numpy())
            cls_range = cls_max - cls_min
            cls_output_history.append(cls_range)
        
        # è®¡ç®—æŸå¤±
        loss, loss_items = loss_fn(outputs, targets, epoch_num=step+1, step_num=1)
        
        if loss is not None:
            loss_value = float(loss.numpy())
            loss_history.append(loss_value)
            
            # åå‘ä¼ æ’­
            optimizer.zero_grad()
            optimizer.backward(loss)
            optimizer.step()
            
            print(f"   æ­¥éª¤{step+1}: æŸå¤±={loss_value:.6f}, åˆ†ç±»èŒƒå›´={cls_range:.6f}")
        else:
            print(f"   æ­¥éª¤{step+1}: æŸå¤±ä¸ºNone")
            return False
    
    # åˆ†æè®­ç»ƒç¨³å®šæ€§
    print(f"\nğŸ“Š è®­ç»ƒç¨³å®šæ€§åˆ†æ:")
    print(f"   åˆå§‹æŸå¤±: {loss_history[0]:.6f}")
    print(f"   æœ€ç»ˆæŸå¤±: {loss_history[-1]:.6f}")
    print(f"   åˆå§‹åˆ†ç±»èŒƒå›´: {cls_output_history[0]:.6f}")
    print(f"   æœ€ç»ˆåˆ†ç±»èŒƒå›´: {cls_output_history[-1]:.6f}")
    
    # æ£€æŸ¥æ˜¯å¦ç¨³å®š
    if cls_output_history[-1] > 0.001:
        print(f"   âœ… åˆ†ç±»è¾“å‡ºä¿æŒå˜åŒ–ï¼Œè®­ç»ƒç¨³å®š")
        return True
    else:
        print(f"   âŒ åˆ†ç±»è¾“å‡ºè¶‹å‘äº0ï¼Œä»æœ‰é—®é¢˜")
        return False

if __name__ == "__main__":
    print("ğŸš€ å¼€å§‹æµ‹è¯•æ¢¯åº¦çˆ†ç‚¸ä¿®å¤æ•ˆæœ...")
    
    # æµ‹è¯•æ¢¯åº¦ä¿®å¤
    gradient_result = test_gradient_fix()
    
    # æµ‹è¯•è®­ç»ƒç¨³å®šæ€§
    stability_result = test_training_stability()
    
    print("\n" + "="*70)
    print("ğŸ‰ æ¢¯åº¦çˆ†ç‚¸ä¿®å¤æµ‹è¯•å®Œæˆï¼")
    print("="*70)
    
    if gradient_result is True and stability_result is True:
        print("âœ… æ¢¯åº¦çˆ†ç‚¸é—®é¢˜å®Œå…¨ä¿®å¤ï¼")
        print("ğŸ¯ åˆ†ç±»å¤´ç°åœ¨å¯ä»¥æ­£å¸¸è®­ç»ƒ")
        print("ğŸ“‹ å»ºè®®è¿›è¡Œå®Œæ•´çš„500è½®è®­ç»ƒéªŒè¯")
    elif gradient_result is not False and stability_result is not False:
        print("âš ï¸ æ¢¯åº¦é—®é¢˜æœ‰æ‰€æ”¹å–„ï¼Œä½†éœ€è¦è¿›ä¸€æ­¥ä¼˜åŒ–")
        print("ğŸ”§ å»ºè®®è°ƒæ•´å­¦ä¹ ç‡æˆ–æŸå¤±æƒé‡")
    else:
        print("âŒ æ¢¯åº¦çˆ†ç‚¸é—®é¢˜ä»ç„¶å­˜åœ¨")
        print("ğŸ”§ éœ€è¦è¿›ä¸€æ­¥ä¿®å¤æŸå¤±å‡½æ•°")
    
    print(f"\nğŸ“Š æµ‹è¯•ç»“æœ:")
    print(f"   æ¢¯åº¦ä¿®å¤: {'âœ… æˆåŠŸ' if gradient_result is True else 'âš ï¸ éƒ¨åˆ†' if gradient_result is None else 'âŒ å¤±è´¥'}")
    print(f"   è®­ç»ƒç¨³å®šæ€§: {'âœ… ç¨³å®š' if stability_result is True else 'âŒ ä¸ç¨³å®š'}")
