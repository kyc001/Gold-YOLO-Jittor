#!/usr/bin/env python3
"""
é«˜æ€§èƒ½æ•°æ®åŠ è½½å™¨ - è§£å†³CPUé«˜GPUä½é—®é¢˜
"""

import os
import cv2
import numpy as np
import jittor as jt
from PIL import Image
import random
import threading
import queue
from concurrent.futures import ThreadPoolExecutor
import time

class FastVOCDataset:
    """é«˜æ€§èƒ½VOCæ•°æ®é›†åŠ è½½å™¨"""
    
    def __init__(self, data_dir, img_size=640, batch_size=8, num_workers=8, prefetch_factor=4):
        self.data_dir = data_dir
        self.img_size = img_size
        self.batch_size = batch_size
        self.num_workers = num_workers
        self.prefetch_factor = prefetch_factor
        
        # è·å–æ‰€æœ‰å›¾åƒæ–‡ä»¶
        self.img_dir = os.path.join(data_dir, 'images')
        self.label_dir = os.path.join(data_dir, 'labels')
        
        self.img_files = []
        self.label_files = []
        
        # æ‰«ææ‰€æœ‰å›¾åƒæ–‡ä»¶
        for img_file in os.listdir(self.img_dir):
            if img_file.lower().endswith(('.jpg', '.jpeg', '.png')):
                img_path = os.path.join(self.img_dir, img_file)
                label_path = os.path.join(self.label_dir, img_file.replace('.jpg', '.txt').replace('.jpeg', '.txt').replace('.png', '.txt'))
                
                if os.path.exists(label_path):
                    self.img_files.append(img_path)
                    self.label_files.append(label_path)
        
        print(f"âœ… é«˜æ€§èƒ½åŠ è½½VOCæ•°æ®é›†: {len(self.img_files)}å¼ å›¾åƒ")
        print(f"   workers: {num_workers}, prefetch: {prefetch_factor}")
        
        # æ‰“ä¹±æ•°æ®
        combined = list(zip(self.img_files, self.label_files))
        random.shuffle(combined)
        self.img_files, self.label_files = zip(*combined)
        
        # é¢„å¤„ç†ç¼“å­˜
        self.cache = {}
        self.cache_size = min(1000, len(self.img_files))  # ç¼“å­˜å‰1000å¼ å›¾åƒ
        
        # é¢„åŠ è½½æ•°æ®
        self._preload_data()
    
    def _preload_data(self):
        """é¢„åŠ è½½éƒ¨åˆ†æ•°æ®åˆ°å†…å­˜"""
        print(f"ğŸš€ é¢„åŠ è½½ {self.cache_size} å¼ å›¾åƒåˆ°å†…å­˜...")
        
        def load_single(idx):
            return idx, self._load_and_process(idx)
        
        with ThreadPoolExecutor(max_workers=self.num_workers) as executor:
            futures = [executor.submit(load_single, i) for i in range(min(self.cache_size, len(self.img_files)))]
            
            for future in futures:
                idx, (img, targets) = future.result()
                self.cache[idx] = (img, targets)
        
        print(f"âœ… é¢„åŠ è½½å®Œæˆ: {len(self.cache)} å¼ å›¾åƒ")
    
    def _load_and_process(self, idx):
        """åŠ è½½å’Œé¢„å¤„ç†å•ä¸ªæ•°æ®é¡¹"""
        img_path = self.img_files[idx]
        label_path = self.label_files[idx]
        
        # åŠ è½½å›¾åƒ - ä½¿ç”¨PILæ›´å¿«
        img = Image.open(img_path).convert('RGB')
        img = img.resize((self.img_size, self.img_size), Image.BILINEAR)
        img = np.array(img, dtype=np.float32) / 255.0
        img = np.transpose(img, (2, 0, 1))  # HWC -> CHW
        
        # åŠ è½½æ ‡ç­¾
        targets = []
        if os.path.exists(label_path):
            with open(label_path, 'r') as f:
                lines = f.readlines()
            
            for line in lines:
                line = line.strip()
                if line:
                    parts = line.split()
                    if len(parts) >= 5:
                        class_id = int(parts[0])
                        x_center = float(parts[1])
                        y_center = float(parts[2])
                        width = float(parts[3])
                        height = float(parts[4])
                        
                        targets.append([class_id, x_center, y_center, width, height])
        
        # è½¬æ¢ä¸ºnumpyæ•°ç»„
        if targets:
            targets = np.array(targets, dtype=np.float32)
        else:
            targets = np.zeros((0, 5), dtype=np.float32)
        
        return img, targets
    
    def __len__(self):
        return len(self.img_files) // self.batch_size
    
    def __iter__(self):
        self.current_idx = 0
        return self
    
    def __next__(self):
        if self.current_idx >= len(self):
            raise StopIteration
        
        return self._get_batch()
    
    def _get_batch(self):
        """å¿«é€Ÿè·å–batchæ•°æ®"""
        batch_imgs = []
        batch_targets = []
        
        start_idx = self.current_idx * self.batch_size
        
        for i in range(self.batch_size):
            idx = start_idx + i
            if idx >= len(self.img_files):
                break
            
            # ä»ç¼“å­˜è·å–æˆ–å®æ—¶åŠ è½½
            if idx in self.cache:
                img, targets = self.cache[idx]
            else:
                img, targets = self._load_and_process(idx)
            
            batch_imgs.append(img)
            
            # æ·»åŠ batchç´¢å¼•
            if len(targets) > 0:
                batch_indices = np.full((len(targets), 1), i, dtype=np.float32)
                targets_with_batch = np.concatenate([batch_indices, targets], axis=1)
                batch_targets.append(targets_with_batch)
        
        # è½¬æ¢ä¸ºjittor tensor
        if batch_imgs:
            batch_imgs = jt.array(np.stack(batch_imgs))
        else:
            batch_imgs = jt.zeros((self.batch_size, 3, self.img_size, self.img_size))
        
        # åˆå¹¶æ‰€æœ‰targets
        if batch_targets:
            batch_targets = jt.array(np.concatenate(batch_targets, axis=0))
        else:
            batch_targets = jt.zeros((0, 6))
        
        self.current_idx += 1
        return batch_imgs, batch_targets

class AsyncDataLoader:
    """å¼‚æ­¥æ•°æ®åŠ è½½å™¨ - è¿›ä¸€æ­¥æå‡æ€§èƒ½"""
    
    def __init__(self, dataset, prefetch_factor=4):
        self.dataset = dataset
        self.prefetch_factor = prefetch_factor
        self.queue = queue.Queue(maxsize=prefetch_factor)
        self.stop_event = threading.Event()
        
        # å¯åŠ¨åå°åŠ è½½çº¿ç¨‹
        self.loader_thread = threading.Thread(target=self._background_loader)
        self.loader_thread.start()
    
    def _background_loader(self):
        """åå°åŠ è½½æ•°æ®"""
        dataset_iter = iter(self.dataset)
        
        while not self.stop_event.is_set():
            try:
                batch = next(dataset_iter)
                self.queue.put(batch, timeout=1)
            except StopIteration:
                # é‡æ–°å¼€å§‹è¿­ä»£
                dataset_iter = iter(self.dataset)
            except queue.Full:
                continue
            except Exception as e:
                print(f"âš ï¸ åå°åŠ è½½é”™è¯¯: {e}")
                break
    
    def __iter__(self):
        return self
    
    def __next__(self):
        try:
            return self.queue.get(timeout=5)
        except queue.Empty:
            raise StopIteration
    
    def __len__(self):
        return len(self.dataset)
    
    def stop(self):
        """åœæ­¢åå°åŠ è½½"""
        self.stop_event.set()
        self.loader_thread.join()

def create_fast_dataloader(data_config, batch_size, is_train=True, num_workers=8):
    """åˆ›å»ºé«˜æ€§èƒ½æ•°æ®åŠ è½½å™¨"""
    
    # æ•°æ®è·¯å¾„
    data_path = data_config['path']
    split = 'train' if is_train else 'val'
    data_dir = os.path.join(data_path, split)
    
    if not os.path.exists(data_dir):
        print(f"âš ï¸ æ•°æ®ç›®å½•ä¸å­˜åœ¨: {data_dir}")
        return None
    
    # åˆ›å»ºé«˜æ€§èƒ½æ•°æ®é›†
    dataset = FastVOCDataset(
        data_dir=data_dir,
        img_size=640,
        batch_size=batch_size,
        num_workers=num_workers,
        prefetch_factor=4
    )
    
    # åˆ›å»ºå¼‚æ­¥æ•°æ®åŠ è½½å™¨
    async_loader = AsyncDataLoader(dataset, prefetch_factor=4)
    
    return async_loader

def benchmark_dataloader():
    """æ•°æ®åŠ è½½å™¨æ€§èƒ½æµ‹è¯•"""
    print("ğŸš€ æ•°æ®åŠ è½½å™¨æ€§èƒ½æµ‹è¯•")
    
    data_config = {
        'path': '/home/kyc/project/GOLD-YOLO/data/voc2012_subset',
        'nc': 20
    }
    
    # æµ‹è¯•åŸå§‹æ•°æ®åŠ è½½å™¨
    print("\nğŸ“Š æµ‹è¯•åŸå§‹æ•°æ®åŠ è½½å™¨...")
    from real_data_loader import VOCDataset

    start_time = time.time()
    old_dataset = VOCDataset('/home/kyc/project/GOLD-YOLO/data/voc2012_subset/train', batch_size=8)
    old_loader = iter(old_dataset)
    
    for i, (images, targets) in enumerate(old_loader):
        if i >= 10:  # åªæµ‹è¯•10ä¸ªbatch
            break
    
    old_time = time.time() - start_time
    print(f"   åŸå§‹åŠ è½½å™¨: {old_time:.2f}ç§’ (10 batches)")
    
    # æµ‹è¯•é«˜æ€§èƒ½æ•°æ®åŠ è½½å™¨
    print("\nğŸ“Š æµ‹è¯•é«˜æ€§èƒ½æ•°æ®åŠ è½½å™¨...")
    
    start_time = time.time()
    fast_loader = create_fast_dataloader(data_config, 8, is_train=True, num_workers=8)
    
    for i, (images, targets) in enumerate(fast_loader):
        if i >= 10:  # åªæµ‹è¯•10ä¸ªbatch
            break
    
    fast_time = time.time() - start_time
    print(f"   é«˜æ€§èƒ½åŠ è½½å™¨: {fast_time:.2f}ç§’ (10 batches)")
    
    speedup = old_time / fast_time
    print(f"\nğŸš€ æ€§èƒ½æå‡: {speedup:.2f}x")
    
    fast_loader.stop()

if __name__ == "__main__":
    benchmark_dataloader()
