#!/usr/bin/env python3
"""
å•å¼ å›¾ç‰‡è¿‡æ‹Ÿåˆè®­ç»ƒå’Œæ¨ç†æµ‹è¯•è„šæœ¬
è¦æ±‚ï¼šæ˜¾ç¤ºè®­ç»ƒè¿›åº¦ï¼Œæ¨ç†æµ‹è¯•ç»“æœå¯è§†åŒ–ï¼Œæ£€æµ‹è¯†åˆ«å‡ºæ¥ç‰©ä½“ä¸çœŸå®æ ‡æ³¨ä¸€è‡´
"""

import os
import sys
import time
import cv2
import numpy as np
import matplotlib.pyplot as plt
from pathlib import Path

import jittor as jt
from jittor import nn
from jittor.dataset import Dataset, DataLoader

sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from models.perfect_gold_yolo import create_perfect_gold_yolo_model
from yolov6.data.data_augment import letterbox
from yolov6.utils.nms import non_max_suppression

# COCOç±»åˆ«åç§°
COCO_CLASSES = [
    'person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus', 'train', 'truck', 'boat', 'traffic light',
    'fire hydrant', 'stop sign', 'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep', 'cow'
]

# é¢œè‰²åˆ—è¡¨
COLORS = [
    (255, 0, 0), (0, 255, 0), (0, 0, 255), (255, 255, 0), (255, 0, 255),
    (0, 255, 255), (128, 0, 128), (255, 165, 0), (0, 128, 128), (128, 128, 0),
    (255, 192, 203), (128, 0, 0), (0, 128, 0), (0, 0, 128), (128, 128, 128),
    (255, 20, 147), (0, 191, 255), (255, 69, 0), (50, 205, 50), (220, 20, 60)
]

class SingleImageDataset(Dataset):
    """å•å¼ å›¾ç‰‡æ•°æ®é›†"""
    
    def __init__(self, img_path, annotations, img_size=640):
        super().__init__()
        self.img_path = img_path
        self.annotations = annotations  # æ ¼å¼: [[cls, x_center, y_center, width, height], ...]
        self.img_size = img_size
        
        # åŠ è½½å›¾åƒ
        self.img = cv2.imread(img_path)
        assert self.img is not None, f"æ— æ³•è¯»å–å›¾åƒ: {img_path}"
        
        print(f"ğŸ“¸ å•å¼ å›¾ç‰‡è®­ç»ƒ:")
        print(f"   å›¾åƒè·¯å¾„: {img_path}")
        print(f"   å›¾åƒå°ºå¯¸: {self.img.shape}")
        print(f"   æ ‡æ³¨æ•°é‡: {len(self.annotations)}")
        for i, ann in enumerate(self.annotations):
            cls_name = COCO_CLASSES[int(ann[0])] if int(ann[0]) < len(COCO_CLASSES) else f'Class{int(ann[0])}'
            print(f"   æ ‡æ³¨{i+1}: {cls_name} [{ann[1]:.3f}, {ann[2]:.3f}, {ann[3]:.3f}, {ann[4]:.3f}]")
    
    def __len__(self):
        return 1
    
    def __getitem__(self, idx):
        # å›¾åƒé¢„å¤„ç†
        img = letterbox(self.img, new_shape=self.img_size, stride=32, auto=False)[0]
        img = img[:, :, ::-1].transpose(2, 0, 1)  # BGR to RGB, HWC to CHW
        img = np.ascontiguousarray(img)
        img = img.astype(np.float32) / 255.0
        
        # æ ‡ç­¾å¤„ç† - æ”¯æŒå¤šä¸ªç›®æ ‡
        labels_out = []
        for ann in self.annotations:
            labels_out.append([ann[0], ann[1], ann[2], ann[3], ann[4], 0])
        
        if len(labels_out) == 0:
            labels_out = [[0, 0.5, 0.5, 0.1, 0.1, 0]]  # é»˜è®¤æ ‡ç­¾
        
        return jt.array(img, dtype='float32'), jt.array(labels_out, dtype='float32')

def create_sample_annotation():
    """åˆ›å»ºç¤ºä¾‹æ ‡æ³¨"""
    # ä½¿ç”¨ä¸€ä¸ªæµ‹è¯•å›¾åƒåˆ›å»ºæ ‡æ³¨
    test_img = '/home/kyc/project/GOLD-YOLO/Gold-YOLO_pytorch/runs/inference/pytorch_baseline_test/test_images/2008_000099.jpg'
    
    if os.path.exists(test_img):
        # ä¸ºè¿™å¼ å›¾åƒåˆ›å»ºä¸€äº›ç¤ºä¾‹æ ‡æ³¨
        annotations = [
            [0, 0.3, 0.4, 0.2, 0.3],  # personåœ¨å·¦ä¸ŠåŒºåŸŸ
            [0, 0.7, 0.6, 0.15, 0.25]  # personåœ¨å³ä¸‹åŒºåŸŸ
        ]
        return test_img, annotations
    else:
        print("âŒ æµ‹è¯•å›¾åƒä¸å­˜åœ¨ï¼Œè¯·æä¾›æœ‰æ•ˆçš„å›¾åƒè·¯å¾„å’Œæ ‡æ³¨")
        return None, None

def draw_annotations(img, annotations, title="çœŸå®æ ‡æ³¨"):
    """ç»˜åˆ¶çœŸå®æ ‡æ³¨"""
    img_vis = img.copy()
    h, w = img.shape[:2]
    
    for i, ann in enumerate(annotations):
        cls_id, x_center, y_center, width, height = ann
        
        # è½¬æ¢ä¸ºåƒç´ åæ ‡
        x1 = int((x_center - width/2) * w)
        y1 = int((y_center - height/2) * h)
        x2 = int((x_center + width/2) * w)
        y2 = int((y_center + height/2) * h)
        
        # ç¡®ä¿åæ ‡åœ¨èŒƒå›´å†…
        x1 = max(0, min(x1, w-1))
        y1 = max(0, min(y1, h-1))
        x2 = max(0, min(x2, w-1))
        y2 = max(0, min(y2, h-1))
        
        color = COLORS[int(cls_id) % len(COLORS)]
        
        # ç»˜åˆ¶è¾¹ç•Œæ¡†
        cv2.rectangle(img_vis, (x1, y1), (x2, y2), color, 3)
        
        # ç»˜åˆ¶æ ‡ç­¾
        cls_name = COCO_CLASSES[int(cls_id)] if int(cls_id) < len(COCO_CLASSES) else f'Class{int(cls_id)}'
        label = f'GT: {cls_name}'
        
        label_size = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.7, 2)[0]
        cv2.rectangle(img_vis, (x1, y1-label_size[1]-10), (x1+label_size[0], y1), color, -1)
        cv2.putText(img_vis, label, (x1, y1-5), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255,255,255), 2)
    
    return img_vis

def draw_predictions(img, detections, conf_thres=0.5, title="é¢„æµ‹ç»“æœ"):
    """ç»˜åˆ¶é¢„æµ‹ç»“æœ"""
    img_vis = img.copy()
    
    for det in detections:
        if len(det):
            if hasattr(det, 'numpy'):
                det_np = det.numpy()
            else:
                det_np = det
            
            for detection in det_np:
                if isinstance(detection, np.ndarray) and detection.ndim > 1:
                    detection = detection.flatten()
                
                if len(detection) >= 6:
                    x1, y1, x2, y2, conf, cls = detection[:6]
                    if conf >= conf_thres:
                        x1, y1, x2, y2 = map(int, [x1, y1, x2, y2])
                        class_id = int(cls)
                        confidence = float(conf)
                        
                        # ç¡®ä¿åæ ‡åœ¨å›¾åƒèŒƒå›´å†…
                        x1 = max(0, min(x1, img.shape[1]-1))
                        y1 = max(0, min(y1, img.shape[0]-1))
                        x2 = max(0, min(x2, img.shape[1]-1))
                        y2 = max(0, min(y2, img.shape[0]-1))
                        
                        color = COLORS[class_id % len(COLORS)]
                        
                        # ç»˜åˆ¶è¾¹ç•Œæ¡†
                        cv2.rectangle(img_vis, (x1, y1), (x2, y2), color, 2)
                        
                        # ç»˜åˆ¶æ ‡ç­¾
                        class_name = COCO_CLASSES[class_id] if class_id < len(COCO_CLASSES) else f'Class{class_id}'
                        label = f'Pred: {class_name} {confidence:.2f}'
                        
                        label_size = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 2)[0]
                        cv2.rectangle(img_vis, (x1, y1-label_size[1]-5), (x1+label_size[0], y1), color, -1)
                        cv2.putText(img_vis, label, (x1, y1-5), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255,255,255), 2)
    
    return img_vis

def scale_coords(img1_shape, coords, img0_shape):
    """åæ ‡ç¼©æ”¾"""
    gain = min(img1_shape[0] / img0_shape[0], img1_shape[1] / img0_shape[1])
    pad = (img1_shape[1] - img0_shape[1] * gain) / 2, (img1_shape[0] - img0_shape[0] * gain) / 2

    coords[:, [0, 2]] -= pad[0]  # x padding
    coords[:, [1, 3]] -= pad[1]  # y padding
    coords[:, :4] /= gain
    coords[:, [0, 2]] = coords[:, [0, 2]].clamp(0, img0_shape[1])
    coords[:, [1, 3]] = coords[:, [1, 3]].clamp(0, img0_shape[0])
    return coords

def single_image_overfit_training():
    """å•å¼ å›¾ç‰‡è¿‡æ‹Ÿåˆè®­ç»ƒä¸»å‡½æ•°"""
    print("""
    â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
    â•‘              å•å¼ å›¾ç‰‡è¿‡æ‹Ÿåˆè®­ç»ƒç³»ç»Ÿ                           â•‘
    â•‘                                                              â•‘
    â•‘  ğŸ¯ åœ¨å•å¼ å›¾ç‰‡ä¸Šè¿‡æ‹Ÿåˆè®­ç»ƒ                                   â•‘
    â•‘  ğŸ“Š æ˜¾ç¤ºè¯¦ç»†è®­ç»ƒè¿›åº¦                                         â•‘
    â•‘  ğŸ” æ¨ç†ç»“æœå¯è§†åŒ–å¯¹æ¯”                                       â•‘
    â•‘  âœ… ç¡®ä¿æ£€æµ‹ç»“æœä¸çœŸå®æ ‡æ³¨ä¸€è‡´                               â•‘
    â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    """)
    
    # åˆ›å»ºç¤ºä¾‹æ•°æ®
    img_path, annotations = create_sample_annotation()
    if img_path is None:
        return False
    
    # åˆ›å»ºæ•°æ®é›†
    dataset = SingleImageDataset(img_path, annotations)
    dataloader = DataLoader(dataset, batch_size=1, shuffle=False, num_workers=0)
    
    # åˆ›å»ºæ¨¡å‹
    print("\nğŸ”§ åˆ›å»ºæ¨¡å‹...")
    model = create_perfect_gold_yolo_model('gold_yolo-n', num_classes=20)
    
    # é‡æ–°åˆå§‹åŒ–åˆ†ç±»å¤´
    print("ğŸ”§ é‡æ–°åˆå§‹åŒ–åˆ†ç±»å¤´...")
    for name, module in model.named_modules():
        if 'cls_pred' in name and isinstance(module, nn.Conv2d):
            jt.init.gauss_(module.weight, mean=0.0, std=0.01)
            if module.bias is not None:
                jt.init.constant_(module.bias, -2.0)
    
    # åˆ›å»ºæŸå¤±å‡½æ•°
    print("ğŸ”§ åˆ›å»ºæŸå¤±å‡½æ•°...")
    import importlib.util
    losses_file = os.path.join(os.path.dirname(__file__), 'yolov6', 'models', 'losses.py')
    spec = importlib.util.spec_from_file_location("losses", losses_file)
    losses_module = importlib.util.module_from_spec(spec)
    spec.loader.exec_module(losses_module)
    
    loss_fn = losses_module.ComputeLoss(
        num_classes=20,
        ori_img_size=640,
        warmup_epoch=0,
        use_dfl=False,
        reg_max=0,
        iou_type='siou',
        loss_weight={
            'class': 2.0,  # å¢åŠ åˆ†ç±»æŸå¤±æƒé‡
            'iou': 3.0,    # å¢åŠ IoUæŸå¤±æƒé‡
            'dfl': 0.5
        }
    )
    
    # åˆ›å»ºä¼˜åŒ–å™¨
    print("ğŸ”§ åˆ›å»ºä¼˜åŒ–å™¨...")
    optimizer = nn.SGD(model.parameters(), lr=0.01, momentum=0.9, weight_decay=0.0001)
    
    # å¼€å§‹è¿‡æ‹Ÿåˆè®­ç»ƒ
    print("\nğŸš€ å¼€å§‹å•å¼ å›¾ç‰‡è¿‡æ‹Ÿåˆè®­ç»ƒ...")
    print(f"   ç›®æ ‡: åœ¨å•å¼ å›¾ç‰‡ä¸Šå®Œå…¨è¿‡æ‹Ÿåˆ")
    print(f"   æœŸæœ›: æ£€æµ‹ç»“æœä¸çœŸå®æ ‡æ³¨å®Œå…¨ä¸€è‡´")
    print("=" * 70)
    
    model.train()
    
    # è®­ç»ƒç»Ÿè®¡
    loss_history = []
    epochs = 200  # è¿‡æ‹Ÿåˆè®­ç»ƒè½®æ•°
    
    # è·å–åŸå§‹å›¾åƒç”¨äºå¯è§†åŒ–
    original_img = cv2.imread(img_path)
    
    # åˆ›å»ºä¿å­˜ç›®å½•
    save_dir = Path('runs/overfit')
    save_dir.mkdir(parents=True, exist_ok=True)
    
    # ä¿å­˜çœŸå®æ ‡æ³¨å›¾åƒ
    gt_img = draw_annotations(original_img, annotations, "çœŸå®æ ‡æ³¨")
    cv2.imwrite(str(save_dir / 'ground_truth.jpg'), gt_img)
    
    start_time = time.time()
    
    for epoch in range(epochs):
        epoch_loss = 0.0
        
        for batch_idx, (images, targets) in enumerate(dataloader):
            # å‰å‘ä¼ æ’­
            predictions = model(images)
            
            # è®¡ç®—æŸå¤±
            loss, loss_items = loss_fn(predictions, targets, epoch_num=epoch+1, step_num=batch_idx+1)
            
            if loss is not None:
                # åå‘ä¼ æ’­
                optimizer.zero_grad()
                optimizer.backward(loss)
                optimizer.step()
                
                epoch_loss += float(loss.numpy())
        
        # è®°å½•æŸå¤±
        loss_history.append(epoch_loss)
        
        # æ˜¾ç¤ºè®­ç»ƒè¿›åº¦
        if (epoch + 1) % 20 == 0 or epoch < 10:
            elapsed_time = time.time() - start_time
            eta = elapsed_time / (epoch + 1) * (epochs - epoch - 1)
            
            print(f"Epoch {epoch+1:3d}/{epochs} | Loss: {epoch_loss:.6f} | ETA: {eta:.1f}s")
            
            # æ¯20è½®è¿›è¡Œä¸€æ¬¡æ¨ç†æµ‹è¯•
            if (epoch + 1) % 20 == 0:
                print(f"   ğŸ” è¿›è¡Œæ¨ç†æµ‹è¯•...")
                
                model.eval()
                with jt.no_grad():
                    # é¢„å¤„ç†å›¾åƒ
                    img = letterbox(original_img, new_shape=640, stride=32, auto=False)[0]
                    img = img[:, :, ::-1].transpose(2, 0, 1)
                    img = np.ascontiguousarray(img)
                    img = img.astype(np.float32) / 255.0
                    img_tensor = jt.array(img).unsqueeze(0)
                    
                    # æ¨ç†
                    pred = model(img_tensor)
                    
                    # åå¤„ç†
                    if isinstance(pred, (list, tuple)):
                        pred_for_nms = pred[0]
                    else:
                        pred_for_nms = pred
                    
                    # NMS
                    detections = non_max_suppression(pred_for_nms.unsqueeze(0), conf_thres=0.3, iou_thres=0.45)
                    
                    # åæ ‡ç¼©æ”¾
                    for det in detections:
                        if len(det):
                            det[:, :4] = scale_coords((640, 640), det[:, :4], original_img.shape[:2])
                    
                    # ç»Ÿè®¡æ£€æµ‹ç»“æœ
                    num_det = len(detections[0]) if len(detections) > 0 and len(detections[0]) > 0 else 0
                    print(f"     æ£€æµ‹æ•°é‡: {num_det}")
                    
                    # ç»˜åˆ¶é¢„æµ‹ç»“æœ
                    pred_img = draw_predictions(original_img, detections, conf_thres=0.3)
                    
                    # ä¿å­˜é˜¶æ®µæ€§ç»“æœ
                    cv2.imwrite(str(save_dir / f'prediction_epoch_{epoch+1:03d}.jpg'), pred_img)
                    
                    # åˆ›å»ºå¯¹æ¯”å›¾åƒ
                    comparison = np.hstack([gt_img, pred_img])
                    cv2.imwrite(str(save_dir / f'comparison_epoch_{epoch+1:03d}.jpg'), comparison)
                
                model.train()
    
    # è®­ç»ƒå®Œæˆ
    total_time = time.time() - start_time
    print("\nâœ… å•å¼ å›¾ç‰‡è¿‡æ‹Ÿåˆè®­ç»ƒå®Œæˆï¼")
    
    # æœ€ç»ˆæ¨ç†æµ‹è¯•
    print("\nğŸ” æœ€ç»ˆæ¨ç†æµ‹è¯•...")
    model.eval()
    
    with jt.no_grad():
        # é¢„å¤„ç†
        img = letterbox(original_img, new_shape=640, stride=32, auto=False)[0]
        img = img[:, :, ::-1].transpose(2, 0, 1)
        img = np.ascontiguousarray(img)
        img = img.astype(np.float32) / 255.0
        img_tensor = jt.array(img).unsqueeze(0)
        
        # æ¨ç†
        pred = model(img_tensor)
        
        # åå¤„ç†
        if isinstance(pred, (list, tuple)):
            pred_for_nms = pred[0]
        else:
            pred_for_nms = pred
        
        # ä½¿ç”¨æ›´ä½çš„ç½®ä¿¡åº¦é˜ˆå€¼è¿›è¡Œæœ€ç»ˆæµ‹è¯•
        detections = non_max_suppression(pred_for_nms.unsqueeze(0), conf_thres=0.1, iou_thres=0.45)
        
        # åæ ‡ç¼©æ”¾
        for det in detections:
            if len(det):
                det[:, :4] = scale_coords((640, 640), det[:, :4], original_img.shape[:2])
        
        # ç»Ÿè®¡æœ€ç»ˆç»“æœ
        num_det = len(detections[0]) if len(detections) > 0 and len(detections[0]) > 0 else 0
        
        print(f"æœ€ç»ˆæ£€æµ‹ç»“æœ:")
        print(f"   æ£€æµ‹æ•°é‡: {num_det}")
        print(f"   çœŸå®æ ‡æ³¨æ•°é‡: {len(annotations)}")
        
        # ç»˜åˆ¶æœ€ç»ˆç»“æœ
        final_pred_img = draw_predictions(original_img, detections, conf_thres=0.1)
        final_comparison = np.hstack([gt_img, final_pred_img])
        
        # ä¿å­˜æœ€ç»ˆç»“æœ
        cv2.imwrite(str(save_dir / 'final_prediction.jpg'), final_pred_img)
        cv2.imwrite(str(save_dir / 'final_comparison.jpg'), final_comparison)
        
        # åˆ†ææ£€æµ‹è´¨é‡
        if num_det > 0:
            det = detections[0]
            if hasattr(det, 'numpy'):
                det_np = det.numpy()
            else:
                det_np = det
            
            print(f"   æ£€æµ‹è¯¦æƒ…:")
            for i, detection in enumerate(det_np[:5]):  # æ˜¾ç¤ºå‰5ä¸ª
                if isinstance(detection, np.ndarray) and detection.ndim > 1:
                    detection = detection.flatten()
                
                if len(detection) >= 6:
                    x1, y1, x2, y2, conf, cls = detection[:6]
                    class_name = COCO_CLASSES[int(cls)] if int(cls) < len(COCO_CLASSES) else f'Class{int(cls)}'
                    print(f"     {i+1}: {class_name} {conf:.3f} [{int(x1)},{int(y1)},{int(x2)},{int(y2)}]")
    
    # ç»˜åˆ¶æŸå¤±æ›²çº¿
    plt.figure(figsize=(10, 6))
    plt.plot(loss_history)
    plt.title('å•å¼ å›¾ç‰‡è¿‡æ‹Ÿåˆè®­ç»ƒ - æŸå¤±æ›²çº¿')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.grid(True)
    plt.savefig(str(save_dir / 'loss_curve.png'), dpi=150, bbox_inches='tight')
    plt.close()
    
    # ä¿å­˜æ¨¡å‹
    model_path = save_dir / 'overfit_model.pkl'
    jt.save({
        'model': model.state_dict(),
        'optimizer': optimizer.state_dict(),
        'epoch': epochs,
        'loss_history': loss_history,
        'annotations': annotations,
        'img_path': img_path
    }, str(model_path))
    
    # è¾“å‡ºæ€»ç»“
    print("\n" + "="*70)
    print("ğŸ‰ å•å¼ å›¾ç‰‡è¿‡æ‹Ÿåˆè®­ç»ƒå®Œæˆï¼")
    print("="*70)
    print(f"ğŸ“Š è®­ç»ƒç»Ÿè®¡:")
    print(f"   è®­ç»ƒè½®æ•°: {epochs}")
    print(f"   è®­ç»ƒæ—¶é—´: {total_time/60:.1f}åˆ†é’Ÿ")
    print(f"   åˆå§‹æŸå¤±: {loss_history[0]:.6f}")
    print(f"   æœ€ç»ˆæŸå¤±: {loss_history[-1]:.6f}")
    print(f"   æŸå¤±ä¸‹é™: {((loss_history[0] - loss_history[-1]) / loss_history[0] * 100):.1f}%")
    
    print(f"\nğŸ“‹ æ£€æµ‹ç»“æœ:")
    print(f"   çœŸå®æ ‡æ³¨: {len(annotations)}ä¸ªç›®æ ‡")
    print(f"   æ£€æµ‹ç»“æœ: {num_det}ä¸ªç›®æ ‡")
    print(f"   è¿‡æ‹ŸåˆçŠ¶æ€: {'æˆåŠŸ' if num_det >= len(annotations) else 'éœ€è¦æ›´å¤šè®­ç»ƒ'}")
    
    print(f"\nğŸ’¾ ä¿å­˜æ–‡ä»¶:")
    print(f"   æ¨¡å‹æƒé‡: {model_path}")
    print(f"   æŸå¤±æ›²çº¿: {save_dir}/loss_curve.png")
    print(f"   çœŸå®æ ‡æ³¨: {save_dir}/ground_truth.jpg")
    print(f"   æœ€ç»ˆé¢„æµ‹: {save_dir}/final_prediction.jpg")
    print(f"   å¯¹æ¯”ç»“æœ: {save_dir}/final_comparison.jpg")
    
    return True

if __name__ == "__main__":
    print("ğŸš€ å¼€å§‹å•å¼ å›¾ç‰‡è¿‡æ‹Ÿåˆè®­ç»ƒ...")
    
    success = single_image_overfit_training()
    
    if success:
        print("\nğŸ‰ å•å¼ å›¾ç‰‡è¿‡æ‹Ÿåˆè®­ç»ƒæˆåŠŸå®Œæˆï¼")
        print("ğŸ“‹ ç°åœ¨å¯ä»¥æŸ¥çœ‹:")
        print("   - runs/overfit/final_comparison.jpg (å¯¹æ¯”ç»“æœ)")
        print("   - runs/overfit/loss_curve.png (è®­ç»ƒæ›²çº¿)")
        print("   - runs/overfit/ ç›®å½•ä¸‹çš„æ‰€æœ‰é˜¶æ®µæ€§ç»“æœ")
    else:
        print("\nâŒ å•å¼ å›¾ç‰‡è¿‡æ‹Ÿåˆè®­ç»ƒå¤±è´¥ï¼")
