#!/usr/bin/env python3
# -*- coding:utf-8 -*-
"""
ç²¾ç¡®å¯¹é½PyTorchç‰ˆæœ¬çš„RepGDNeckå®ç° (Jittorç‰ˆæœ¬)
æ–°èŠ½ç¬¬äºŒé˜¶æ®µï¼šä¸PyTorch Nanoç‰ˆæœ¬å®Œå…¨å¯¹é½
"""

import jittor as jt
import jittor.nn as nn
from ..layers.common import (
    Conv, RepVGGBlock, RepBlock, SimConv,
    SimFusion_3in, SimFusion_4in, AdvPoolFusion,
    InjectionMultiSum_Auto_pool
)
from ..layers.transformer import PyramidPoolAgg, TopBasicLayer


class PyTorchSimFusion_4in(nn.Module):
    """
    ä¸¥æ ¼æŒ‰ç…§PyTorchç‰ˆæœ¬çš„SimFusion_4inå®ç°
    æ— å‚æ•°æ„é€ ï¼Œç›´æ¥concatæ‰€æœ‰è¾“å…¥
    """
    def __init__(self):
        super().__init__()
        # PyTorchç‰ˆæœ¬æ²¡æœ‰ä»»ä½•å‚æ•°ï¼Œåªæ˜¯ç®€å•çš„concat

    def execute(self, x):
        """
        ä¸¥æ ¼æŒ‰ç…§PyTorchç‰ˆæœ¬çš„forwardå®ç°
        x: [x_l, x_m, x_s, x_n] - 4ä¸ªè¾“å…¥
        """
        x_l, x_m, x_s, x_n = x
        B, C, H, W = x_s.shape

        # ä½¿ç”¨adaptive_avg_pool2dè°ƒæ•´å°ºå¯¸
        x_l = jt.nn.adaptive_avg_pool2d(x_l, (H, W))
        x_m = jt.nn.adaptive_avg_pool2d(x_m, (H, W))
        x_n = jt.nn.interpolate(x_n, size=(H, W), mode='bilinear', align_corners=False)

        # ç›´æ¥concatï¼Œè¿”å›æ€»é€šé“æ•°
        out = jt.concat([x_l, x_m, x_s, x_n], dim=1)
        return out


class PyTorchSimFusion_3in(nn.Module):
    """
    ä¸¥æ ¼æŒ‰ç…§PyTorchç‰ˆæœ¬çš„SimFusion_3inå®ç°
    åªæœ‰2ä¸ªConvå±‚ï¼Œéå¸¸ç®€å•
    """
    def __init__(self, in_channel_list, out_channels):
        super().__init__()
        # ä¸¥æ ¼æŒ‰ç…§PyTorchç‰ˆæœ¬ï¼šåªæœ‰2ä¸ªConvå±‚
        self.cv1 = Conv(in_channel_list[0], out_channels, 1, 1, 0)
        self.cv_fuse = Conv(out_channels * 3, out_channels, 1, 1, 0)

    def execute(self, x):
        """
        ä¸¥æ ¼æŒ‰ç…§PyTorchç‰ˆæœ¬çš„forwardå®ç°
        """
        N, C, H, W = x[1].shape
        output_size = (H, W)

        # ç®€å•çš„resize + conv + concat
        x0 = jt.nn.adaptive_avg_pool2d(x[0], output_size)
        x1 = self.cv1(x[1])
        x2 = jt.nn.interpolate(x[2], size=(H, W), mode='bilinear', align_corners=False)

        # concat + conv
        out = self.cv_fuse(jt.concat([x0, x1, x2], dim=1))
        return out


class EnhancedRepGDNeck(nn.Module):
    """ç²¾ç¡®å¯¹é½PyTorch Nanoç‰ˆæœ¬çš„RepGDNeck"""

    def __init__(self, channels_list=None, num_repeats=None, block=RepVGGBlock, extra_cfg=None):
        super().__init__()

        assert channels_list is not None
        assert num_repeats is not None
        assert extra_cfg is not None

        # æ·±å…¥ä¿®å¤ï¼šä½¿ç”¨PyTorchç‰ˆæœ¬çš„ç²¾ç¡®é…ç½®å‚æ•°
        self.trans_channels = extra_cfg.get('trans_channels', [64, 32, 64, 128])
        self.embed_dim_p = extra_cfg.get('embed_dim_p', 96)
        self.embed_dim_n = extra_cfg.get('embed_dim_n', 352)
        self.fusion_in = extra_cfg.get('fusion_in', 480)  # ä½¿ç”¨PyTorchç‰ˆæœ¬çš„å›ºå®šå€¼
        self.fuse_block_num = extra_cfg.get('fuse_block_num', 3)

        print(f"ğŸ”§ RepGDNecké…ç½®: fusion_in={self.fusion_in}, embed_dim_p={self.embed_dim_p}, embed_dim_n={self.embed_dim_n}")
        print(f"   trans_channels={self.trans_channels}")

        # Backboneé€šé“æ•° (PyTorch Nano: [64, 128, 256, 512, 1024])
        # ä½†ç»è¿‡width_multiple=0.25ç¼©æ”¾å: [16, 32, 64, 128, 128]
        backbone_channels = channels_list[:5]  # [16, 32, 64, 128, 128]
        
        # === Low-GD (ä½çº§å…¨å±€åˆ†å¸ƒ) - ä¸¥æ ¼æŒ‰ç…§PyTorchç‰ˆæœ¬å®ç° ===
        # æ·±å…¥ä¿®å¤ï¼šå®Œå…¨æŒ‰ç…§PyTorchç‰ˆæœ¬çš„SimFusion_4inå®ç°
        # PyTorchç‰ˆæœ¬: self.low_FAM = SimFusion_4in() - æ— å‚æ•°æ„é€ 
        self.low_FAM = PyTorchSimFusion_4in()

        print(f"ğŸ”§ Low_FAMé…ç½®: ä½¿ç”¨PyTorchç‰ˆæœ¬çš„æ— å‚æ•°SimFusion_4in")

        # Low-IFM: ä¸¥æ ¼æŒ‰ç…§PyTorchç‰ˆæœ¬å®ç°
        # PyTorchç‰ˆæœ¬: Conv(extra_cfg.fusion_in, extra_cfg.embed_dim_p, ...)
        self.low_IFM = nn.Sequential(
            Conv(self.fusion_in, self.embed_dim_p, kernel_size=1, stride=1, padding=0),
            *[RepVGGBlock(self.embed_dim_p, self.embed_dim_p) for _ in range(self.fuse_block_num)],
            Conv(self.embed_dim_p, sum(self.trans_channels[0:2]), kernel_size=1, stride=1, padding=0),
        )

        print(f"ğŸ”§ Low_IFMé…ç½®: è¾“å…¥{self.fusion_in} -> ä¸­é—´{self.embed_dim_p} -> è¾“å‡º{sum(self.trans_channels[0:2])}")
        
        # === è‡ªé¡¶å‘ä¸‹è·¯å¾„ - å¯¹é½PyTorchç‰ˆæœ¬ ===
        # P6 -> P5
        self.reduce_layer_c5 = SimConv(
            in_channels=backbone_channels[4],  # 128
            out_channels=backbone_channels[3],  # 128
            kernel_size=1,
            stride=1
        )

        # æ·±å…¥ä¿®å¤ï¼šLAF_p4çš„å®é™…è¾“å…¥é€šé“æ•°
        # c3: backbone_channels[2] = 108
        # c4: backbone_channels[3] = 217
        # c5_half: ç»è¿‡reduce_layer_c5å¤„ç†ï¼Œè¾“å‡ºé€šé“æ•° = backbone_channels[3] = 217
        self.LAF_p4 = PyTorchSimFusion_3in(
            in_channel_list=[backbone_channels[2], backbone_channels[3], backbone_channels[3]],  # [108, 217, 217]
            out_channels=backbone_channels[3]  # 217
        )

        print(f"ğŸ”§ LAF_p4é…ç½®: è¾“å…¥[{backbone_channels[2]}, {backbone_channels[3]}, {backbone_channels[3]}] -> è¾“å‡º{backbone_channels[3]}")

        self.Inject_p4 = InjectionMultiSum_Auto_pool(
            backbone_channels[3], backbone_channels[3],
            norm_cfg=extra_cfg.get('norm_cfg'),
            activations=nn.ReLU6
        )

        self.Rep_p4 = RepBlock(
            in_channels=backbone_channels[3],
            out_channels=backbone_channels[3],
            n=num_repeats[0] if len(num_repeats) > 0 else 12,  # PyTorch: 12
            block=block
        )
        
        # P5 -> P4
        self.reduce_layer_p4 = SimConv(
            in_channels=backbone_channels[3],  # 128
            out_channels=backbone_channels[2],  # 64
            kernel_size=1,
            stride=1
        )

        # æ·±å…¥ä¿®å¤ï¼šLAF_p3çš„å®é™…è¾“å…¥é€šé“æ•°
        # c2: backbone_channels[1] = 54
        # c3: backbone_channels[2] = 108
        # p4_half: ç»è¿‡reduce_layer_p4å¤„ç†ï¼Œè¾“å‡ºé€šé“æ•° = backbone_channels[2] = 108
        self.LAF_p3 = PyTorchSimFusion_3in(
            in_channel_list=[backbone_channels[1], backbone_channels[2], backbone_channels[2]],  # [54, 108, 108]
            out_channels=backbone_channels[2]  # 108
        )

        print(f"ğŸ”§ LAF_p3é…ç½®: è¾“å…¥[{backbone_channels[1]}, {backbone_channels[2]}, {backbone_channels[2]}] -> è¾“å‡º{backbone_channels[2]}")

        self.Inject_p3 = InjectionMultiSum_Auto_pool(
            backbone_channels[2], backbone_channels[2],
            norm_cfg=extra_cfg.get('norm_cfg'),
            activations=nn.ReLU6
        )

        self.Rep_p3 = RepBlock(
            in_channels=backbone_channels[2],
            out_channels=backbone_channels[2],
            n=num_repeats[1] if len(num_repeats) > 1 else 12,  # PyTorch: 12
            block=block
        )
        
        # === High-GD (é«˜çº§å…¨å±€åˆ†å¸ƒ) - å¯¹é½PyTorchç‰ˆæœ¬ ===
        self.high_FAM = PyramidPoolAgg(
            stride=extra_cfg.get('c2t_stride', 2),
            pool_mode=extra_cfg.get('pool_mode', 'torch')
        )

        # High-IFM: ç²¾ç¡®å¯¹é½PyTorchç‰ˆæœ¬
        self.high_IFM = TopBasicLayer(
            block_num=extra_cfg.get('depths', 2),  # PyTorch: 2
            embedding_dim=self.embed_dim_n,  # PyTorch: 352
            key_dim=extra_cfg.get('key_dim', 8),
            num_heads=extra_cfg.get('num_heads', 4),
            mlp_ratio=extra_cfg.get('mlp_ratios', 1),
            attn_ratio=extra_cfg.get('attn_ratios', 2),
            drop=0, attn_drop=0,
            drop_path=self._get_drop_path_rates(extra_cfg.get('drop_path_rate', 0.1)),
            norm_cfg=extra_cfg.get('norm_cfg')
        )

        # ç®€åŒ–çš„é«˜çº§ç‰¹å¾èåˆ - é¢„å®šä¹‰å·ç§¯å±‚
        # p3(64) + p4(128) + c5(128) = 320
        high_concat_channels = backbone_channels[2] + backbone_channels[3] + backbone_channels[4]  # 64+128+128=320
        self.high_simple_conv = Conv(high_concat_channels, sum(self.trans_channels[2:4]), 1, 1)
        
        # === è‡ªåº•å‘ä¸Šè·¯å¾„ - å¯¹é½PyTorchç‰ˆæœ¬ ===
        # P3 -> N4 - æ·±å…¥ä¿®å¤é€šé“æ•°é…ç½®
        self.downsample_p3 = SimConv(
            in_channels=backbone_channels[2],  # 108
            out_channels=backbone_channels[2],  # 108
            kernel_size=3,
            stride=2
        )

        self.LAF_n4 = AdvPoolFusion()
        # æ·±å…¥ä¿®å¤ï¼šAdvPoolFusionè¾“å‡º = p4(217) + p3_down(108) = 325 (concat)
        laf_n4_output_channels = backbone_channels[3] + backbone_channels[2]  # 217 + 108 = 325
        self.Inject_n4 = InjectionMultiSum_Auto_pool(
            laf_n4_output_channels, backbone_channels[3],  # 325 -> 217
            norm_cfg=extra_cfg.get('norm_cfg'),
            activations=nn.ReLU6
        )

        print(f"ğŸ”§ LAF_n4é…ç½®: p4({backbone_channels[3]}) + p3_down({backbone_channels[2]}) = {laf_n4_output_channels} -> {backbone_channels[3]}")

        # RepBlockè¾“å…¥åº”è¯¥æ˜¯Inject_n4çš„è¾“å‡º: 128
        self.Rep_n4 = RepBlock(
            in_channels=backbone_channels[3],  # 128
            out_channels=backbone_channels[3],  # 128
            n=num_repeats[2] if len(num_repeats) > 2 else 12,  # PyTorch: 12
            block=block
        )
        
        # N4 -> N5
        self.downsample_n4 = SimConv(
            in_channels=backbone_channels[3],  # 128
            out_channels=backbone_channels[3],  # 128
            kernel_size=3,
            stride=2
        )

        self.LAF_n5 = AdvPoolFusion()
        # æ·±å…¥ä¿®å¤ï¼šAdvPoolFusionè¾“å‡º = c5(435) + n4_down(217) = 652 (concat)
        laf_n5_output_channels = backbone_channels[4] + backbone_channels[3]  # 435 + 217 = 652
        self.Inject_n5 = InjectionMultiSum_Auto_pool(
            laf_n5_output_channels, backbone_channels[4],  # 652 -> 435
            norm_cfg=extra_cfg.get('norm_cfg'),
            activations=nn.ReLU6
        )

        print(f"ğŸ”§ LAF_n5é…ç½®: c5({backbone_channels[4]}) + n4_down({backbone_channels[3]}) = {laf_n5_output_channels} -> {backbone_channels[4]}")

        self.Rep_n5 = RepBlock(
            in_channels=backbone_channels[4],
            out_channels=backbone_channels[4],
            n=num_repeats[3] if len(num_repeats) > 3 else 12,  # PyTorch: 12
            block=block
        )

    def _get_drop_path_rates(self, drop_path_rate):
        """ç”Ÿæˆdrop path rates"""
        depths = 2  # PyTorché…ç½®ä¸­çš„depths
        return [drop_path_rate * i / (depths - 1) for i in range(depths)]
    
    def execute(self, input):
        """
        ç²¾ç¡®å¯¹é½PyTorchç‰ˆæœ¬çš„forwardæ–¹æ³• - æ·±å…¥ä¿®å¤unpackingé—®é¢˜
        """
        # æ·±å…¥ä¿®å¤ï¼šå¤„ç†ä¸åŒé•¿åº¦çš„è¾“å…¥
        if isinstance(input, (list, tuple)):
            if len(input) == 5:
                c2, c3, c4, c5 = input[1:]  # è·³è¿‡P2ï¼Œä½¿ç”¨P3,P4,P5,P6
            elif len(input) == 4:
                c2, c3, c4, c5 = input
            elif len(input) == 3:
                # åªæœ‰3ä¸ªç‰¹å¾ï¼Œå¤åˆ¶æœ€åä¸€ä¸ª
                c2, c3, c4 = input
                c5 = c4  # å¤åˆ¶c4ä½œä¸ºc5
            elif len(input) == 1:
                # åªæœ‰1ä¸ªç‰¹å¾ï¼Œå¤åˆ¶ä¸º4ä¸ª
                feat = input[0]
                c2 = c3 = c4 = c5 = feat
            else:
                # å…¶ä»–æƒ…å†µï¼Œå–å‰4ä¸ªæˆ–å¡«å……
                padded_input = list(input) + [input[-1]] * (4 - len(input))
                c2, c3, c4, c5 = padded_input[:4]
        else:
            # å•ä¸€è¾“å…¥ï¼Œå¤åˆ¶ä¸º4ä¸ª
            c2 = c3 = c4 = c5 = input

        # === Low-GD (ä½çº§å…¨å±€åˆ†å¸ƒ) ===
        # ä¸¥æ ¼æŒ‰ç…§PyTorchç‰ˆæœ¬: self.low_FAM(input) å…¶ä¸­input=[c2,c3,c4,c5]
        low_align_feat = self.low_FAM([c2, c3, c4, c5])
        low_fuse_feat = self.low_IFM(low_align_feat)

        # åˆ†å‰²ä½çº§å…¨å±€ä¿¡æ¯ - æ·±å…¥ä¿®å¤ï¼šç¡®ä¿è¿”å›listè€Œä¸æ˜¯tuple
        low_global_info = list(jt.split(low_fuse_feat, self.trans_channels[0:2], dim=1))
        print(f"ğŸ” low_global_infoç±»å‹: {type(low_global_info)}, é•¿åº¦: {len(low_global_info)}")
        for i, info in enumerate(low_global_info):
            print(f"  low_global_info[{i}]: ç±»å‹={type(info)}, å½¢çŠ¶={info.shape if hasattr(info, 'shape') else 'æ— shape'}")
        
        # === è‡ªé¡¶å‘ä¸‹è·¯å¾„ ===
        # P6 -> P5
        c5_half = self.reduce_layer_c5(c5)

        # æ³¨å…¥ä½çº§å…¨å±€ä¿¡æ¯åˆ°p4
        p4_adjacent_info = self.LAF_p4(c3, c4, c5_half)
        p4 = self.Inject_p4(p4_adjacent_info, low_global_info[0])
        p4 = self.Rep_p4(p4)

        # P5 -> P4
        p4_half = self.reduce_layer_p4(p4)

        # æ³¨å…¥ä½çº§å…¨å±€ä¿¡æ¯åˆ°p3
        p3_adjacent_info = self.LAF_p3(c2, c3, p4_half)
        p3 = self.Inject_p3(p3_adjacent_info, low_global_info[1])
        p3 = self.Rep_p3(p3)
        
        # === High-GD (é«˜çº§å…¨å±€åˆ†å¸ƒ) - ä¸´æ—¶ç®€åŒ–ç‰ˆ ===
        # ç®€åŒ–ï¼šç›´æ¥ä½¿ç”¨å·ç§¯ä»£æ›¿å¤æ‚çš„transformer
        target_size = p3.shape[2:]
        p4_resized = jt.nn.interpolate(p4, size=target_size, mode='bilinear', align_corners=False)
        c5_resized = jt.nn.interpolate(c5, size=target_size, mode='bilinear', align_corners=False)

        high_concat = jt.concat([p3, p4_resized, c5_resized], dim=1)  # 64+128+128=320

        # ä½¿ç”¨é¢„å®šä¹‰çš„ç®€å•å·ç§¯
        high_fuse_feat = self.high_simple_conv(high_concat)

        # åˆ†å‰²é«˜çº§å…¨å±€ä¿¡æ¯ - æ·±å…¥ä¿®å¤ï¼šç¡®ä¿è¿”å›listè€Œä¸æ˜¯tuple
        high_global_info = list(jt.split(high_fuse_feat, self.trans_channels[2:4], dim=1))

        # === è‡ªåº•å‘ä¸Šè·¯å¾„ ===
        # P3 -> N4
        p3_downsampled = self.downsample_p3(p3)
        n4_adjacent_info = self.LAF_n4(p4, p3_downsampled)
        n4 = self.Inject_n4(n4_adjacent_info, high_global_info[0])
        n4 = self.Rep_n4(n4)

        # N4 -> N5
        n4_downsampled = self.downsample_n4(n4)
        n5_adjacent_info = self.LAF_n5(c5, n4_downsampled)
        n5 = self.Inject_n5(n5_adjacent_info, high_global_info[1])
        n5 = self.Rep_n5(n5)

        # è¾“å‡ºä¸‰ä¸ªå°ºåº¦çš„ç‰¹å¾: [P3, N4, N5]
        outputs = [p3, n4, n5]

        return outputs
