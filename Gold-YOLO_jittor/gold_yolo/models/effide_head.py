#!/usr/bin/env python3
# -*- coding:utf-8 -*-
"""
ç²¾ç¡®å¯¹é½PyTorchç‰ˆæœ¬çš„EffiDeHeadå®ç° (Jittorç‰ˆæœ¬)
æ–°èŠ½ç¬¬äºŒé˜¶æ®µï¼šä¸PyTorch Nanoç‰ˆæœ¬å®Œå…¨å¯¹é½
"""

import jittor as jt
import jittor.nn as nn
import math
from ..layers.common import Conv


class EffiDeHead(nn.Module):
    """ç²¾ç¡®å¯¹é½PyTorch Nanoç‰ˆæœ¬çš„EffiDeHead"""
    
    def __init__(self, num_classes=20, in_channels=[128, 256, 512], num_layers=3,
                 anchors=3, use_dfl=False, reg_max=0, **kwargs):
        super().__init__()

        self.nc = num_classes
        self.no = num_classes + 5  # number of outputs per anchor
        self.nl = num_layers  # number of detection layers
        self.na = anchors  # number of anchors
        self.use_dfl = use_dfl
        self.reg_max = reg_max

        # ğŸ”§ æ·±å…¥ä¿®å¤ï¼šæ·»åŠ ç¼ºå¤±çš„prior_probå±æ€§
        self.prior_prob = 1e-2  # å¯¹é½PyTorchç‰ˆæœ¬

        # æ­¥é•¿é…ç½® (å¯¹é½PyTorch)
        self.stride = jt.array([8, 16, 32])

        # è¾“å…¥é€šé“æ•° (å¯¹é½PyTorch Nanoé…ç½®)
        # PyTorché…ç½®: in_channels=[128, 256, 512]
        # ä½†ç»è¿‡width_multiple=0.25ç¼©æ”¾åå®é™…æ˜¯: [32, 64, 128]
        # æˆ‘ä»¬çš„neckè¾“å‡ºæ˜¯: [64, 128, 128] (P3, N4, N5)
        self.in_channels = in_channels
        
        # åˆå§‹åŒ–è§£è€¦å¤´éƒ¨
        self.stems = nn.ModuleList()
        self.cls_convs = nn.ModuleList()
        self.reg_convs = nn.ModuleList()
        self.cls_preds = nn.ModuleList()
        self.reg_preds = nn.ModuleList()
        
        # ä¸ºæ¯ä¸ªæ£€æµ‹å±‚æ„å»ºå¤´éƒ¨ (ç²¾ç¡®å¯¹é½PyTorch)
        for i in range(num_layers):
            # ä½¿ç”¨å®é™…çš„neckè¾“å‡ºé€šé“æ•° - ä¿®å¤é€šé“æ•°ä¸åŒ¹é…
            ch = in_channels[i] if i < len(in_channels) else in_channels[-1]


            # Stemå±‚ (å¯¹é½PyTorch)
            self.stems.append(Conv(ch, ch, kernel_size=1, stride=1))

            # åˆ†ç±»åˆ†æ”¯ (å¯¹é½PyTorch)
            self.cls_convs.append(Conv(ch, ch, kernel_size=3, stride=1))

            # å›å½’åˆ†æ”¯ (å¯¹é½PyTorch)
            self.reg_convs.append(Conv(ch, ch, kernel_size=3, stride=1))

            # åˆ†ç±»é¢„æµ‹ (å¯¹é½PyTorch)
            self.cls_preds.append(nn.Conv2d(ch, num_classes * anchors, kernel_size=1))

            # å›å½’é¢„æµ‹ (å¯¹é½PyTorch)
            if use_dfl and reg_max > 0:
                self.reg_preds.append(nn.Conv2d(ch, 4 * (reg_max + 1) * anchors, kernel_size=1))
            else:
                self.reg_preds.append(nn.Conv2d(ch, 4 * anchors, kernel_size=1))

        # DFLç›¸å…³ (å¯¹é½PyTorch) - æ·±å…¥ä¿®å¤Parameterè­¦å‘Š
        if use_dfl and reg_max > 0:
            # åœ¨Jittorä¸­ï¼Œç›´æ¥åˆ›å»ºå˜é‡ï¼Œä¸éœ€è¦ParameteråŒ…è£…
            self.proj = jt.linspace(0, reg_max, reg_max + 1)
            self.proj_conv = nn.Conv2d(reg_max + 1, 1, 1, bias=False)
            # ç›´æ¥è®¾ç½®æƒé‡ï¼Œä¸éœ€è¦ParameteråŒ…è£… - æ·±å…¥ä¿®å¤copy_æ–¹æ³•
            with jt.no_grad():
                # åœ¨Jittorä¸­ä½¿ç”¨assignè€Œä¸æ˜¯copy_
                proj_weight = self.proj.view(1, reg_max + 1, 1, 1)
                self.proj_conv.weight.assign(proj_weight)
    
    def initialize_biases(self):
        """åˆå§‹åŒ–åç½® - æ·±å…¥ä¿®å¤ç¡®ä¿æ‰€æœ‰å‚æ•°æ­£ç¡®åˆå§‹åŒ–"""
        print(f"ğŸ”§ å¼€å§‹åˆå§‹åŒ–Headå±‚åç½®...")

        # åˆ†ç±»å¤´åç½®åˆå§‹åŒ– - æ·±å…¥ä¿®å¤
        print(f"ğŸ”§ åˆå§‹åŒ–åˆ†ç±»é¢„æµ‹å±‚åç½®...")
        for i, conv in enumerate(self.cls_preds):
            if hasattr(conv, 'bias') and conv.bias is not None:
                # åœ¨Jittorä¸­ï¼Œç›´æ¥æ“ä½œbiaså’Œweight
                bias_value = -math.log((1 - self.prior_prob) / self.prior_prob)
                print(f"  åˆ†ç±»å±‚{i}: è®¾ç½®åç½®ä¸º{bias_value:.6f}")

                # ä½¿ç”¨Jittorçš„æ­£ç¡®æ–¹å¼åˆå§‹åŒ–
                conv.bias.data = jt.full_like(conv.bias, bias_value)
                # æƒé‡åˆå§‹åŒ–ä¸ºå°çš„éšæœºå€¼è€Œä¸æ˜¯0ï¼Œç¡®ä¿æ¢¯åº¦ä¼ æ’­
                conv.weight.data = jt.randn_like(conv.weight) * 0.01
                print(f"  åˆ†ç±»å±‚{i}: æƒé‡å½¢çŠ¶{conv.weight.shape}, åç½®å½¢çŠ¶{conv.bias.shape}")

        # å›å½’å¤´åç½®åˆå§‹åŒ– - æ·±å…¥ä¿®å¤
        print(f"ğŸ”§ åˆå§‹åŒ–å›å½’é¢„æµ‹å±‚åç½®...")
        for i, conv in enumerate(self.reg_preds):
            if hasattr(conv, 'bias') and conv.bias is not None:
                print(f"  å›å½’å±‚{i}: è®¾ç½®åç½®ä¸º1.0")

                # ä½¿ç”¨Jittorçš„æ­£ç¡®æ–¹å¼åˆå§‹åŒ–
                conv.bias.data = jt.ones_like(conv.bias)
                # æƒé‡åˆå§‹åŒ–ä¸ºå°çš„éšæœºå€¼è€Œä¸æ˜¯0ï¼Œç¡®ä¿æ¢¯åº¦ä¼ æ’­
                conv.weight.data = jt.randn_like(conv.weight) * 0.01
                print(f"  å›å½’å±‚{i}: æƒé‡å½¢çŠ¶{conv.weight.shape}, åç½®å½¢çŠ¶{conv.bias.shape}")

        # åˆ†ç±»å·ç§¯å±‚åˆå§‹åŒ– - æ·±å…¥ä¿®å¤ï¼šç¡®ä¿è¿™äº›å±‚ä¹Ÿè¢«æ­£ç¡®åˆå§‹åŒ–
        print(f"ğŸ”§ åˆå§‹åŒ–åˆ†ç±»å·ç§¯å±‚...")
        for i, conv_module in enumerate(self.cls_convs):
            if hasattr(conv_module, 'conv') and hasattr(conv_module.conv, 'weight'):
                # Convæ¨¡å—å†…éƒ¨çš„å·ç§¯å±‚
                conv_module.conv.weight.data = jt.randn_like(conv_module.conv.weight) * 0.01
                if hasattr(conv_module.conv, 'bias') and conv_module.conv.bias is not None:
                    conv_module.conv.bias.data = jt.zeros_like(conv_module.conv.bias)
                print(f"  åˆ†ç±»å·ç§¯{i}: æƒé‡å½¢çŠ¶{conv_module.conv.weight.shape}")

        # å›å½’å·ç§¯å±‚åˆå§‹åŒ– - æ·±å…¥ä¿®å¤ï¼šç¡®ä¿è¿™äº›å±‚ä¹Ÿè¢«æ­£ç¡®åˆå§‹åŒ–
        print(f"ğŸ”§ åˆå§‹åŒ–å›å½’å·ç§¯å±‚...")
        for i, conv_module in enumerate(self.reg_convs):
            if hasattr(conv_module, 'conv') and hasattr(conv_module.conv, 'weight'):
                # Convæ¨¡å—å†…éƒ¨çš„å·ç§¯å±‚
                conv_module.conv.weight.data = jt.randn_like(conv_module.conv.weight) * 0.01
                if hasattr(conv_module.conv, 'bias') and conv_module.conv.bias is not None:
                    conv_module.conv.bias.data = jt.zeros_like(conv_module.conv.bias)
                print(f"  å›å½’å·ç§¯{i}: æƒé‡å½¢çŠ¶{conv_module.conv.weight.shape}")

        # DFLæŠ•å½±å‚æ•°åˆå§‹åŒ–
        if self.use_dfl and hasattr(self, 'proj_conv'):
            print(f"ğŸ”§ åˆå§‹åŒ–DFLæŠ•å½±å±‚...")
            # ç›´æ¥åˆ›å»ºå˜é‡ï¼Œä¸éœ€è¦ParameteråŒ…è£…
            self.proj = jt.linspace(0, self.reg_max, self.reg_max + 1)
            proj_weight = self.proj.view(1, self.reg_max + 1, 1, 1)
            self.proj_conv.weight.data = proj_weight
            print(f"  DFLæŠ•å½±å±‚: æƒé‡å½¢çŠ¶{self.proj_conv.weight.shape}")

        print(f"âœ… Headå±‚åç½®åˆå§‹åŒ–å®Œæˆ")
    
    def execute(self, x):
        """
        å‰å‘ä¼ æ’­
        x: [P3, N4, N5] ä¸‰ä¸ªå°ºåº¦çš„ç‰¹å¾
        """
        if self.training:
            return self._forward_train(x)
        else:
            return self._forward_eval(x)
    
    def _forward_train(self, x):
        """è®­ç»ƒæ—¶çš„å‰å‘ä¼ æ’­ - æ·±å…¥ä¿®å¤ç¡®ä¿æ‰€æœ‰å±‚å‚ä¸è®¡ç®—"""
        cls_score_list = []
        reg_distri_list = []

        print(f"ğŸ”§ Headå±‚å‰å‘ä¼ æ’­å¼€å§‹ï¼Œè¾“å…¥ç‰¹å¾æ•°é‡: {len(x)}")

        for i in range(self.nl):
            print(f"ğŸ”§ å¤„ç†ç¬¬{i}å±‚ï¼Œè¾“å…¥å½¢çŠ¶: {x[i].shape}")

            # ğŸ”§ æ·±å…¥ä¿®å¤ï¼šç¡®ä¿æ¯ä¸€å±‚éƒ½è¢«æ­£ç¡®è°ƒç”¨å’Œä½¿ç”¨

            # Stemå¤„ç† - ç¡®ä¿å‚ä¸æ¢¯åº¦è®¡ç®—
            feat = self.stems[i](x[i])
            print(f"  Stemè¾“å‡ºå½¢çŠ¶: {feat.shape}")

            # åˆ†ç±»åˆ†æ”¯ - æ·±å…¥ä¿®å¤ï¼šç¡®ä¿æ‰€æœ‰åˆ†ç±»å±‚éƒ½å‚ä¸è®¡ç®—
            cls_feat = self.cls_convs[i](feat)  # è¿™é‡Œè°ƒç”¨cls_convs
            print(f"  åˆ†ç±»å·ç§¯è¾“å‡ºå½¢çŠ¶: {cls_feat.shape}")

            cls_output = self.cls_preds[i](cls_feat)  # è¿™é‡Œè°ƒç”¨cls_preds
            print(f"  åˆ†ç±»é¢„æµ‹è¾“å‡ºå½¢çŠ¶: {cls_output.shape}")

            # ä¸è¦åœ¨è®­ç»ƒæ—¶åº”ç”¨sigmoidï¼Œè®©æŸå¤±å‡½æ•°å¤„ç†
            # cls_output = jt.sigmoid(cls_output)  # æ³¨é‡Šæ‰ï¼Œè®©æŸå¤±å‡½æ•°å¤„ç†

            # å›å½’åˆ†æ”¯ - æ·±å…¥ä¿®å¤ï¼šç¡®ä¿æ‰€æœ‰å›å½’å±‚éƒ½å‚ä¸è®¡ç®—
            reg_feat = self.reg_convs[i](feat)  # è¿™é‡Œè°ƒç”¨reg_convs
            print(f"  å›å½’å·ç§¯è¾“å‡ºå½¢çŠ¶: {reg_feat.shape}")

            reg_output = self.reg_preds[i](reg_feat)  # è¿™é‡Œè°ƒç”¨reg_preds
            print(f"  å›å½’é¢„æµ‹è¾“å‡ºå½¢çŠ¶: {reg_output.shape}")

            # é‡å¡‘è¾“å‡ºæ ¼å¼ - ç¡®ä¿æ¢¯åº¦è¿æ¥
            # flatten(2) = flatten(start_dim=2) å°†H,Wç»´åº¦å±•å¹³
            cls_reshaped = cls_output.flatten(2).permute(0, 2, 1)  # [B, C, H*W] -> [B, H*W, C]
            reg_reshaped = reg_output.flatten(2).permute(0, 2, 1)  # [B, C, H*W] -> [B, H*W, C]

            print(f"  åˆ†ç±»é‡å¡‘åå½¢çŠ¶: {cls_reshaped.shape}")
            print(f"  å›å½’é‡å¡‘åå½¢çŠ¶: {reg_reshaped.shape}")

            cls_score_list.append(cls_reshaped)
            reg_distri_list.append(reg_reshaped)
        
        # æ‹¼æ¥æ‰€æœ‰å°ºåº¦ - æ·±åº¦ä¿®å¤ç¡®ä¿è¿”å›å¼ é‡
        print(f"ğŸ”§ Headå±‚è¾“å‡ºåˆå¹¶å‰æ£€æŸ¥:")
        print(f"  cls_score_listé•¿åº¦: {len(cls_score_list)}")
        print(f"  reg_distri_listé•¿åº¦: {len(reg_distri_list)}")

        # ç¡®ä¿æ‰€æœ‰è¾“å‡ºéƒ½æ˜¯å¼ é‡
        for i, (cls, reg) in enumerate(zip(cls_score_list, reg_distri_list)):
            print(f"  å°ºåº¦{i}: clsç±»å‹={type(cls)}, regç±»å‹={type(reg)}")
            if hasattr(cls, 'shape'):
                print(f"    clså½¢çŠ¶={cls.shape}")
            if hasattr(reg, 'shape'):
                print(f"    regå½¢çŠ¶={reg.shape}")

        try:
            cls_score_list = jt.concat(cls_score_list, dim=1)
            reg_distri_list = jt.concat(reg_distri_list, dim=1)

            print(f"âœ… Headå±‚è¾“å‡ºåˆå¹¶æˆåŠŸ:")
            print(f"  cls_score_listå½¢çŠ¶: {cls_score_list.shape}")
            print(f"  reg_distri_listå½¢çŠ¶: {reg_distri_list.shape}")
            print(f"  xç±»å‹: {type(x)}, é•¿åº¦: {len(x) if isinstance(x, list) else 'N/A'}")

            return x, cls_score_list, reg_distri_list

        except Exception as e:
            print(f"âŒ Headå±‚è¾“å‡ºåˆå¹¶å¤±è´¥: {e}")
            print(f"âŒ é”™è¯¯è¯¦æƒ…: {str(e)}")

            # ğŸ”§ æ·±å…¥ä¿®å¤ï¼šä¸è¦åˆ›å»ºéšæœºè¾“å‡ºï¼Œè€Œæ˜¯å°è¯•ä¿®å¤åˆå¹¶é—®é¢˜
            print(f"ğŸ”§ å°è¯•é€ä¸ªæ£€æŸ¥è¾“å‡ºå¼ é‡...")

            # æ£€æŸ¥æ¯ä¸ªè¾“å‡ºå¼ é‡çš„æœ‰æ•ˆæ€§
            valid_cls_list = []
            valid_reg_list = []

            for i, (cls, reg) in enumerate(zip(cls_score_list, reg_distri_list)):
                if hasattr(cls, 'shape') and hasattr(reg, 'shape'):
                    try:
                        # æµ‹è¯•å¼ é‡æ˜¯å¦å¯ç”¨
                        _ = cls.sum()
                        _ = reg.sum()
                        valid_cls_list.append(cls)
                        valid_reg_list.append(reg)
                        print(f"  âœ… å°ºåº¦{i}è¾“å‡ºæœ‰æ•ˆ")
                    except Exception as tensor_error:
                        print(f"  âŒ å°ºåº¦{i}è¾“å‡ºæ— æ•ˆ: {tensor_error}")

            if len(valid_cls_list) > 0 and len(valid_reg_list) > 0:
                try:
                    cls_concat = jt.concat(valid_cls_list, dim=1)
                    reg_concat = jt.concat(valid_reg_list, dim=1)
                    print(f"âœ… ä½¿ç”¨æœ‰æ•ˆè¾“å‡ºåˆå¹¶æˆåŠŸ")
                    return x, cls_concat, reg_concat
                except Exception as concat_error:
                    print(f"âŒ æœ‰æ•ˆè¾“å‡ºåˆå¹¶ä¹Ÿå¤±è´¥: {concat_error}")

            # å¦‚æœæ‰€æœ‰å°è¯•éƒ½å¤±è´¥ï¼ŒæŠ›å‡ºé”™è¯¯è€Œä¸æ˜¯åˆ›å»ºå‡æ•°æ®
            raise RuntimeError(f"Headå±‚è¾“å‡ºåˆå¹¶å®Œå…¨å¤±è´¥: {e}")
    
    def _forward_eval(self, x):
        """æ¨ç†æ—¶çš„å‰å‘ä¼ æ’­"""
        cls_score_list = []
        reg_dist_list = []
        
        # ç”Ÿæˆanchor points (ç®€åŒ–ç‰ˆ)
        anchor_points, stride_tensor = self._generate_anchors(x)
        
        for i in range(self.nl):
            b, _, h, w = x[i].shape
            l = h * w
            
            # Stemå¤„ç†
            feat = self.stems[i](x[i])
            
            # åˆ†ç±»åˆ†æ”¯
            cls_feat = self.cls_convs[i](feat)
            cls_output = self.cls_preds[i](cls_feat)
            cls_output = jt.sigmoid(cls_output)
            
            # å›å½’åˆ†æ”¯
            reg_feat = self.reg_convs[i](feat)
            reg_output = self.reg_preds[i](reg_feat)
            
            # DFLå¤„ç†
            if self.use_dfl:
                reg_output = reg_output.reshape(b, 4, self.reg_max + 1, l).permute(0, 2, 1, 3)
                reg_output = self.proj_conv(jt.nn.softmax(reg_output, dim=1))
            
            # é‡å¡‘è¾“å‡º
            cls_score_list.append(cls_output.reshape(b, self.nc, l))
            reg_dist_list.append(reg_output.reshape(b, 4, l))
        
        # æ‹¼æ¥æ‰€æœ‰å°ºåº¦
        cls_score_list = jt.concat(cls_score_list, dim=-1).permute(0, 2, 1)
        reg_dist_list = jt.concat(reg_dist_list, dim=-1).permute(0, 2, 1)
        
        # è½¬æ¢ä¸ºè¾¹ç•Œæ¡† (ç®€åŒ–ç‰ˆ)
        pred_bboxes = self._dist2bbox(reg_dist_list, anchor_points)
        pred_bboxes *= stride_tensor
        
        # æ‹¼æ¥æœ€ç»ˆè¾“å‡º
        b = pred_bboxes.shape[0]
        objectness = jt.ones((b, pred_bboxes.shape[1], 1))
        
        return jt.concat([pred_bboxes, objectness, cls_score_list], dim=-1)
    
    def _generate_anchors(self, x):
        """ç”Ÿæˆanchor points (ç®€åŒ–ç‰ˆ)"""
        anchor_points = []
        stride_tensor = []
        
        for i, feat in enumerate(x):
            h, w = feat.shape[2:]
            stride = self.stride[i]
            
            # ç”Ÿæˆç½‘æ ¼ç‚¹
            shift_x = jt.arange(0, w) + self.grid_cell_offset
            shift_y = jt.arange(0, h) + self.grid_cell_offset
            shift_y, shift_x = jt.meshgrid(shift_y, shift_x)
            
            # è½¬æ¢ä¸ºanchor points
            anchor_point = jt.stack([shift_x, shift_y], dim=-1).reshape(-1, 2)
            anchor_point *= stride
            
            anchor_points.append(anchor_point)
            stride_tensor.append(jt.full((anchor_point.shape[0], 1), stride))
        
        anchor_points = jt.concat(anchor_points, dim=0)
        stride_tensor = jt.concat(stride_tensor, dim=0)
        
        return anchor_points, stride_tensor
    
    def _dist2bbox(self, distance, anchor_points):
        """è·ç¦»è½¬è¾¹ç•Œæ¡† (ç®€åŒ–ç‰ˆ)"""
        lt, rb = jt.split(distance, 2, dim=-1)
        x1y1 = anchor_points - lt
        x2y2 = anchor_points + rb
        return jt.concat([x1y1, x2y2], dim=-1)


def build_effide_head(neck_channels, num_classes=20, use_dfl=False, reg_max=0):
    """æ„å»ºç²¾ç¡®å¯¹é½çš„EffiDeHead"""
    return EffiDeHead(
        num_classes=num_classes,
        in_channels=neck_channels,  # [64, 128, 128] (P3, N4, N5)
        num_layers=3,
        anchors=3,  # å¯¹é½PyTorché…ç½®
        use_dfl=use_dfl,
        reg_max=reg_max
    )
