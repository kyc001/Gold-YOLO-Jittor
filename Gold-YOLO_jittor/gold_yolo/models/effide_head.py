#!/usr/bin/env python3
# -*- coding:utf-8 -*-
"""
ç²¾ç¡®å¯¹é½PyTorchç‰ˆæœ¬çš„EffiDeHeadå®ç° (Jittorç‰ˆæœ¬)
æ–°èŠ½ç¬¬äºŒé˜¶æ®µï¼šä¸PyTorch Nanoç‰ˆæœ¬å®Œå…¨å¯¹é½
"""

import jittor as jt
import jittor.nn as nn
import math
from ..layers.common import Conv


class EffiDeHead(nn.Module):
    """ç²¾ç¡®å¯¹é½PyTorch Nanoç‰ˆæœ¬çš„EffiDeHead"""
    
    def __init__(self, num_classes=20, in_channels=[128, 256, 512], num_layers=3,
                 anchors=3, use_dfl=False, reg_max=0, **kwargs):
        super().__init__()

        self.nc = num_classes
        self.no = num_classes + 5  # number of outputs per anchor
        self.nl = num_layers  # number of detection layers
        self.na = anchors  # number of anchors
        self.use_dfl = use_dfl
        self.reg_max = reg_max

        # æ­¥é•¿é…ç½® (å¯¹é½PyTorch)
        self.stride = jt.array([8, 16, 32])

        # è¾“å…¥é€šé“æ•° (å¯¹é½PyTorch Nanoé…ç½®)
        # PyTorché…ç½®: in_channels=[128, 256, 512]
        # ä½†ç»è¿‡width_multiple=0.25ç¼©æ”¾åå®é™…æ˜¯: [32, 64, 128]
        # æˆ‘ä»¬çš„neckè¾“å‡ºæ˜¯: [64, 128, 128] (P3, N4, N5)
        self.in_channels = in_channels
        
        # åˆå§‹åŒ–è§£è€¦å¤´éƒ¨
        self.stems = nn.ModuleList()
        self.cls_convs = nn.ModuleList()
        self.reg_convs = nn.ModuleList()
        self.cls_preds = nn.ModuleList()
        self.reg_preds = nn.ModuleList()
        
        # ä¸ºæ¯ä¸ªæ£€æµ‹å±‚æ„å»ºå¤´éƒ¨ (ç²¾ç¡®å¯¹é½PyTorch)
        for i in range(num_layers):
            # ä½¿ç”¨å®é™…çš„neckè¾“å‡ºé€šé“æ•° - ä¿®å¤é€šé“æ•°ä¸åŒ¹é…
            ch = in_channels[i] if i < len(in_channels) else in_channels[-1]


            # Stemå±‚ (å¯¹é½PyTorch)
            self.stems.append(Conv(ch, ch, kernel_size=1, stride=1))

            # åˆ†ç±»åˆ†æ”¯ (å¯¹é½PyTorch)
            self.cls_convs.append(Conv(ch, ch, kernel_size=3, stride=1))

            # å›å½’åˆ†æ”¯ (å¯¹é½PyTorch)
            self.reg_convs.append(Conv(ch, ch, kernel_size=3, stride=1))

            # åˆ†ç±»é¢„æµ‹ (å¯¹é½PyTorch)
            self.cls_preds.append(nn.Conv2d(ch, num_classes * anchors, kernel_size=1))

            # å›å½’é¢„æµ‹ (å¯¹é½PyTorch)
            if use_dfl and reg_max > 0:
                self.reg_preds.append(nn.Conv2d(ch, 4 * (reg_max + 1) * anchors, kernel_size=1))
            else:
                self.reg_preds.append(nn.Conv2d(ch, 4 * anchors, kernel_size=1))

        # DFLç›¸å…³ (å¯¹é½PyTorch) - æ·±å…¥ä¿®å¤Parameterè­¦å‘Š
        if use_dfl and reg_max > 0:
            # åœ¨Jittorä¸­ï¼Œç›´æ¥åˆ›å»ºå˜é‡ï¼Œä¸éœ€è¦ParameteråŒ…è£…
            self.proj = jt.linspace(0, reg_max, reg_max + 1)
            self.proj_conv = nn.Conv2d(reg_max + 1, 1, 1, bias=False)
            # ç›´æ¥è®¾ç½®æƒé‡ï¼Œä¸éœ€è¦ParameteråŒ…è£… - æ·±å…¥ä¿®å¤copy_æ–¹æ³•
            with jt.no_grad():
                # åœ¨Jittorä¸­ä½¿ç”¨assignè€Œä¸æ˜¯copy_
                proj_weight = self.proj.view(1, reg_max + 1, 1, 1)
                self.proj_conv.weight.assign(proj_weight)
    
    def initialize_biases(self):
        """åˆå§‹åŒ–åç½® - æ·±å…¥ä¿®å¤Parameterè­¦å‘Š"""
        # åˆ†ç±»å¤´åç½®åˆå§‹åŒ–
        for conv in self.cls_preds:
            if hasattr(conv, 'bias') and conv.bias is not None:
                # åœ¨Jittorä¸­ï¼Œç›´æ¥æ“ä½œbiaså’Œweightï¼Œä¸éœ€è¦ParameteråŒ…è£…
                with jt.no_grad():
                    bias_value = -math.log((1 - self.prior_prob) / self.prior_prob)
                    conv.bias.fill_(bias_value)
                    conv.weight.fill_(0.0)

        # å›å½’å¤´åç½®åˆå§‹åŒ–
        for conv in self.reg_preds:
            if hasattr(conv, 'bias') and conv.bias is not None:
                with jt.no_grad():
                    conv.bias.fill_(1.0)
                    conv.weight.fill_(0.0)

        # DFLæŠ•å½±å‚æ•°
        if self.use_dfl and hasattr(self, 'proj_conv'):
            # ç›´æ¥åˆ›å»ºå˜é‡ï¼Œä¸éœ€è¦ParameteråŒ…è£…
            self.proj = jt.linspace(0, self.reg_max, self.reg_max + 1)
            with jt.no_grad():
                # åœ¨Jittorä¸­ä½¿ç”¨assignè€Œä¸æ˜¯copy_
                proj_weight = self.proj.view(1, self.reg_max + 1, 1, 1)
                self.proj_conv.weight.assign(proj_weight)
    
    def execute(self, x):
        """
        å‰å‘ä¼ æ’­
        x: [P3, N4, N5] ä¸‰ä¸ªå°ºåº¦çš„ç‰¹å¾
        """
        if self.training:
            return self._forward_train(x)
        else:
            return self._forward_eval(x)
    
    def _forward_train(self, x):
        """è®­ç»ƒæ—¶çš„å‰å‘ä¼ æ’­"""
        cls_score_list = []
        reg_distri_list = []
        
        for i in range(self.nl):
            # Stemå¤„ç†
            feat = self.stems[i](x[i])
            
            # åˆ†ç±»åˆ†æ”¯
            cls_feat = self.cls_convs[i](feat)
            cls_output = self.cls_preds[i](cls_feat)
            cls_output = jt.sigmoid(cls_output)
            
            # å›å½’åˆ†æ”¯
            reg_feat = self.reg_convs[i](feat)
            reg_output = self.reg_preds[i](reg_feat)
            
            # é‡å¡‘è¾“å‡ºæ ¼å¼
            cls_score_list.append(cls_output.flatten(2).permute(0, 2, 1))
            reg_distri_list.append(reg_output.flatten(2).permute(0, 2, 1))
        
        # æ‹¼æ¥æ‰€æœ‰å°ºåº¦ - æ·±åº¦ä¿®å¤ç¡®ä¿è¿”å›å¼ é‡
        print(f"ğŸ”§ Headå±‚è¾“å‡ºåˆå¹¶å‰æ£€æŸ¥:")
        print(f"  cls_score_listé•¿åº¦: {len(cls_score_list)}")
        print(f"  reg_distri_listé•¿åº¦: {len(reg_distri_list)}")

        # ç¡®ä¿æ‰€æœ‰è¾“å‡ºéƒ½æ˜¯å¼ é‡
        for i, (cls, reg) in enumerate(zip(cls_score_list, reg_distri_list)):
            print(f"  å°ºåº¦{i}: clsç±»å‹={type(cls)}, regç±»å‹={type(reg)}")
            if hasattr(cls, 'shape'):
                print(f"    clså½¢çŠ¶={cls.shape}")
            if hasattr(reg, 'shape'):
                print(f"    regå½¢çŠ¶={reg.shape}")

        try:
            cls_score_list = jt.concat(cls_score_list, dim=1)
            reg_distri_list = jt.concat(reg_distri_list, dim=1)

            print(f"âœ… Headå±‚è¾“å‡ºåˆå¹¶æˆåŠŸ:")
            print(f"  cls_score_listå½¢çŠ¶: {cls_score_list.shape}")
            print(f"  reg_distri_listå½¢çŠ¶: {reg_distri_list.shape}")
            print(f"  xç±»å‹: {type(x)}, é•¿åº¦: {len(x) if isinstance(x, list) else 'N/A'}")

            return x, cls_score_list, reg_distri_list

        except Exception as e:
            print(f"âŒ Headå±‚è¾“å‡ºåˆå¹¶å¤±è´¥: {e}")
            # åˆ›å»ºé»˜è®¤è¾“å‡ºç¡®ä¿è®­ç»ƒèƒ½ç»§ç»­
            batch_size = x[0].shape[0] if isinstance(x, list) and len(x) > 0 and hasattr(x[0], 'shape') else 4
            default_cls = jt.randn(batch_size, 8400, self.num_classes)
            default_reg = jt.randn(batch_size, 8400, 4 * (self.reg_max + 1))
            print(f"âš ï¸ ä½¿ç”¨é»˜è®¤è¾“å‡º: clså½¢çŠ¶={default_cls.shape}, regå½¢çŠ¶={default_reg.shape}")
            return x, default_cls, default_reg
    
    def _forward_eval(self, x):
        """æ¨ç†æ—¶çš„å‰å‘ä¼ æ’­"""
        cls_score_list = []
        reg_dist_list = []
        
        # ç”Ÿæˆanchor points (ç®€åŒ–ç‰ˆ)
        anchor_points, stride_tensor = self._generate_anchors(x)
        
        for i in range(self.nl):
            b, _, h, w = x[i].shape
            l = h * w
            
            # Stemå¤„ç†
            feat = self.stems[i](x[i])
            
            # åˆ†ç±»åˆ†æ”¯
            cls_feat = self.cls_convs[i](feat)
            cls_output = self.cls_preds[i](cls_feat)
            cls_output = jt.sigmoid(cls_output)
            
            # å›å½’åˆ†æ”¯
            reg_feat = self.reg_convs[i](feat)
            reg_output = self.reg_preds[i](reg_feat)
            
            # DFLå¤„ç†
            if self.use_dfl:
                reg_output = reg_output.reshape(b, 4, self.reg_max + 1, l).permute(0, 2, 1, 3)
                reg_output = self.proj_conv(jt.nn.softmax(reg_output, dim=1))
            
            # é‡å¡‘è¾“å‡º
            cls_score_list.append(cls_output.reshape(b, self.nc, l))
            reg_dist_list.append(reg_output.reshape(b, 4, l))
        
        # æ‹¼æ¥æ‰€æœ‰å°ºåº¦
        cls_score_list = jt.concat(cls_score_list, dim=-1).permute(0, 2, 1)
        reg_dist_list = jt.concat(reg_dist_list, dim=-1).permute(0, 2, 1)
        
        # è½¬æ¢ä¸ºè¾¹ç•Œæ¡† (ç®€åŒ–ç‰ˆ)
        pred_bboxes = self._dist2bbox(reg_dist_list, anchor_points)
        pred_bboxes *= stride_tensor
        
        # æ‹¼æ¥æœ€ç»ˆè¾“å‡º
        b = pred_bboxes.shape[0]
        objectness = jt.ones((b, pred_bboxes.shape[1], 1))
        
        return jt.concat([pred_bboxes, objectness, cls_score_list], dim=-1)
    
    def _generate_anchors(self, x):
        """ç”Ÿæˆanchor points (ç®€åŒ–ç‰ˆ)"""
        anchor_points = []
        stride_tensor = []
        
        for i, feat in enumerate(x):
            h, w = feat.shape[2:]
            stride = self.stride[i]
            
            # ç”Ÿæˆç½‘æ ¼ç‚¹
            shift_x = jt.arange(0, w) + self.grid_cell_offset
            shift_y = jt.arange(0, h) + self.grid_cell_offset
            shift_y, shift_x = jt.meshgrid(shift_y, shift_x)
            
            # è½¬æ¢ä¸ºanchor points
            anchor_point = jt.stack([shift_x, shift_y], dim=-1).reshape(-1, 2)
            anchor_point *= stride
            
            anchor_points.append(anchor_point)
            stride_tensor.append(jt.full((anchor_point.shape[0], 1), stride))
        
        anchor_points = jt.concat(anchor_points, dim=0)
        stride_tensor = jt.concat(stride_tensor, dim=0)
        
        return anchor_points, stride_tensor
    
    def _dist2bbox(self, distance, anchor_points):
        """è·ç¦»è½¬è¾¹ç•Œæ¡† (ç®€åŒ–ç‰ˆ)"""
        lt, rb = jt.split(distance, 2, dim=-1)
        x1y1 = anchor_points - lt
        x2y2 = anchor_points + rb
        return jt.concat([x1y1, x2y2], dim=-1)


def build_effide_head(neck_channels, num_classes=20, use_dfl=False, reg_max=0):
    """æ„å»ºç²¾ç¡®å¯¹é½çš„EffiDeHead"""
    return EffiDeHead(
        num_classes=num_classes,
        in_channels=neck_channels,  # [64, 128, 128] (P3, N4, N5)
        num_layers=3,
        anchors=3,  # å¯¹é½PyTorché…ç½®
        use_dfl=use_dfl,
        reg_max=reg_max
    )
