#!/usr/bin/env python3
# -*- coding:utf-8 -*-
"""
å®Œæ•´è¿˜åŸPyTorchç‰ˆæœ¬çš„Gold-YOLOæ¨¡å‹
æ–°èŠ½ç¬¬äºŒé˜¶æ®µï¼šåŸºäºconvert.pyè½¬æ¢çš„PyTorchç»„ä»¶ï¼Œå¾®è°ƒå¯¹é½å››ä¸ªç‰ˆæœ¬(n/s/m/l)
"""

import jittor as jt
import jittor.nn as nn
import numpy as np
import math
from ..layers.common import (
    Conv, RepVGGBlock, RepBlock, SimConv, SimSPPF, CSPSPPF,
    Transpose, SimFusion_3in, SimFusion_4in, AdvPoolFusion,
    InjectionMultiSum_Auto_pool, BepC3, BottleRep, ConvWrapper
)
from ..layers.transformer import (
    PyramidPoolAgg, TopBasicLayer, C2T_Attention
)


def make_divisible(x, divisor):
    """
    PyTorchç‰ˆæœ¬çš„make_divisibleå‡½æ•°
    ç¡®ä¿é€šé“æ•°èƒ½è¢«divisoræ•´é™¤
    """
    return math.ceil(x / divisor) * divisor

# æ·±å…¥ä¿®å¤ï¼šé¡¹ç›®ç»“æ„æ•´ç†åï¼Œä¸“æ³¨äºä½¿ç”¨å®Œå–„çš„åŸå§‹Jittorå®ç°
PYTORCH_COMPONENTS_AVAILABLE = False
print("ğŸ”§ ä½¿ç”¨å®Œå–„çš„åŸå§‹Jittorå®ç° - é¡¹ç›®ç»“æ„å·²ä¸PyTorchå¯¹é½")

# æ€»æ˜¯å¯¼å…¥åŸå§‹ç»„ä»¶ä½œä¸ºå¤‡ç”¨
from .enhanced_repgd_neck import EnhancedRepGDNeck
from .cspbep_backbone import CSPBepBackbone
from .gdneck import GDNeck, GDNeck2
from .simple_reppan import SimpleRepPAN, CSPRepPAN
from .effide_head import build_effide_head

# è®¾ç½®Jittor
jt.flags.use_cuda = 1

class RepVGGBlock(nn.Module):
    """RepVGG Block - å®Œå…¨å¯¹é½PyTorchç‰ˆæœ¬"""
    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, padding=1, groups=1):
        super().__init__()
        self.in_channels = in_channels
        self.out_channels = out_channels
        self.kernel_size = kernel_size
        self.stride = stride
        self.padding = padding
        self.groups = groups
        
        # 3x3 conv
        self.conv3x3 = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding, groups=groups, bias=False)
        self.bn3x3 = nn.BatchNorm2d(out_channels)
        
        # 1x1 conv
        self.conv1x1 = nn.Conv2d(in_channels, out_channels, 1, stride, 0, groups=groups, bias=False)
        self.bn1x1 = nn.BatchNorm2d(out_channels)
        
        # identity
        self.identity = nn.BatchNorm2d(out_channels) if in_channels == out_channels and stride == 1 else None
        
        self.act = nn.ReLU()
    
    def execute(self, x):
        if self.identity is None:
            id_out = 0
        else:
            id_out = self.identity(x)
        
        return self.act(self.bn3x3(self.conv3x3(x)) + self.bn1x1(self.conv1x1(x)) + id_out)

class RepBlock(nn.Module):
    """RepBlock - å®Œå…¨å¯¹é½PyTorchç‰ˆæœ¬"""
    def __init__(self, in_channels, out_channels, n=1, block=RepVGGBlock):
        super().__init__()
        self.conv1 = block(in_channels, out_channels)
        self.blocks = nn.Sequential(*[block(out_channels, out_channels) for _ in range(n-1)])
    
    def execute(self, x):
        x = self.conv1(x)
        return self.blocks(x)

class SimSPPF(nn.Module):
    """Simplified SPPF - å®Œå…¨å¯¹é½PyTorchç‰ˆæœ¬"""
    def __init__(self, in_channels, out_channels, kernel_size=5):
        super().__init__()
        c_ = in_channels // 2
        self.cv1 = nn.Conv2d(in_channels, c_, 1, 1, 0, bias=False)
        self.cv2 = nn.Conv2d(c_ * 4, out_channels, 1, 1, 0, bias=False)
        self.bn1 = nn.BatchNorm2d(c_)
        self.bn2 = nn.BatchNorm2d(out_channels)
        self.act = nn.ReLU()
        self.m = nn.MaxPool2d(kernel_size=kernel_size, stride=1, padding=kernel_size // 2)
    
    def execute(self, x):
        x = self.act(self.bn1(self.cv1(x)))
        y1 = self.m(x)
        y2 = self.m(y1)
        y3 = self.m(y2)
        return self.act(self.bn2(self.cv2(jt.concat([x, y1, y2, y3], 1))))

class SimCSPSPPF(nn.Module):
    """Simplified CSP SPPF - å®Œå…¨å¯¹é½PyTorchç‰ˆæœ¬"""
    def __init__(self, in_channels, out_channels, kernel_size=5, e=0.5):
        super().__init__()
        c_ = int(out_channels * e)
        self.cv1 = nn.Conv2d(in_channels, c_, 1, 1, 0, bias=False)
        self.cv2 = nn.Conv2d(in_channels, c_, 1, 1, 0, bias=False)
        self.cv3 = nn.Conv2d(c_, c_, 3, 1, 1, bias=False)
        self.cv4 = nn.Conv2d(c_, c_, 1, 1, 0, bias=False)
        self.cv5 = nn.Conv2d(c_ * 4, c_, 1, 1, 0, bias=False)
        self.cv6 = nn.Conv2d(c_ * 2, out_channels, 1, 1, 0, bias=False)
        
        self.bn1 = nn.BatchNorm2d(c_)
        self.bn2 = nn.BatchNorm2d(c_)
        self.bn3 = nn.BatchNorm2d(c_)
        self.bn4 = nn.BatchNorm2d(c_)
        self.bn5 = nn.BatchNorm2d(c_)
        self.bn6 = nn.BatchNorm2d(out_channels)
        
        self.act = nn.ReLU()
        self.m = nn.MaxPool2d(kernel_size=kernel_size, stride=1, padding=kernel_size // 2)
    
    def execute(self, x):
        x1 = self.act(self.bn4(self.cv4(self.act(self.bn3(self.cv3(self.act(self.bn1(self.cv1(x)))))))))
        y0 = self.act(self.bn2(self.cv2(x)))
        y1 = self.m(x1)
        y2 = self.m(y1)
        y3 = self.m(y2)
        y4 = self.act(self.bn5(self.cv5(jt.concat([x1, y1, y2, y3], 1))))
        return self.act(self.bn6(self.cv6(jt.concat([y0, y4], 1))))

class EfficientRep(nn.Module):
    """EfficientRep Backbone - å®Œå…¨å¯¹é½PyTorchç‰ˆæœ¬
    
    å®˜æ–¹é…ç½® (gold_yolo-s.py):
    - num_repeats=[1, 6, 12, 18, 6]
    - out_channels=[64, 128, 256, 512, 1024]
    - fuse_P2=True
    - cspsppf=True
    """
    
    def __init__(self, in_channels=3, channels_list=None, num_repeats=None, 
                 block=RepVGGBlock, fuse_P2=False, cspsppf=False):
        super().__init__()
        
        assert channels_list is not None
        assert num_repeats is not None
        
        self.fuse_P2 = fuse_P2
        
        # Stem
        self.stem = block(
            in_channels=in_channels,
            out_channels=channels_list[0],
            kernel_size=3,
            stride=2
        )
        
        # ERBlock_2 (P2)
        self.ERBlock_2 = nn.Sequential(
            block(
                in_channels=channels_list[0],
                out_channels=channels_list[1],
                kernel_size=3,
                stride=2
            ),
            RepBlock(
                in_channels=channels_list[1],
                out_channels=channels_list[1],
                n=num_repeats[1],
                block=block
            )
        )
        
        # ERBlock_3 (P3)
        self.ERBlock_3 = nn.Sequential(
            block(
                in_channels=channels_list[1],
                out_channels=channels_list[2],
                kernel_size=3,
                stride=2
            ),
            RepBlock(
                in_channels=channels_list[2],
                out_channels=channels_list[2],
                n=num_repeats[2],
                block=block
            )
        )
        
        # ERBlock_4 (P4)
        self.ERBlock_4 = nn.Sequential(
            block(
                in_channels=channels_list[2],
                out_channels=channels_list[3],
                kernel_size=3,
                stride=2
            ),
            RepBlock(
                in_channels=channels_list[3],
                out_channels=channels_list[3],
                n=num_repeats[3],
                block=block
            )
        )
        
        # ERBlock_5 (P5)
        self.ERBlock_5 = nn.Sequential(
            block(
                in_channels=channels_list[3],
                out_channels=channels_list[4],
                kernel_size=3,
                stride=2
            ),
            RepBlock(
                in_channels=channels_list[4],
                out_channels=channels_list[4],
                n=num_repeats[4],
                block=block
            )
        )
        
        # æ·±å…¥ä¿®å¤ï¼šåˆ é™¤ERBlock_6ï¼Œå› ä¸ºæˆ‘ä»¬åªæœ‰5ä¸ªé€šé“ [27, 54, 108, 217, 435]
        # ä¸éœ€è¦ç¬¬6ä¸ªblock
    
    def execute(self, x):
        outputs = []

        # æ·±å…¥ä¿®å¤ï¼šä¸¥æ ¼æŒ‰ç…§channels_listçš„é¡ºåºè¿”å›ç‰¹å¾
        # channels_list = [27, 54, 108, 217, 435] å¯¹åº” [stem, ERBlock_2, ERBlock_3, ERBlock_4, ERBlock_5]


        x = self.stem(x)  # 27é€šé“

        outputs.append(x)  # P0: stemè¾“å‡º

        x = self.ERBlock_2(x)  # 54é€šé“
        outputs.append(x)  # P1: ERBlock_2è¾“å‡º

        x = self.ERBlock_3(x)  # 108é€šé“
        outputs.append(x)  # P2: ERBlock_3è¾“å‡º

        x = self.ERBlock_4(x)  # 217é€šé“
        outputs.append(x)  # P3: ERBlock_4è¾“å‡º

        x = self.ERBlock_5(x)  # 435é€šé“
        outputs.append(x)  # P4: ERBlock_5è¾“å‡º

        return tuple(outputs)


class RepGDNeck(nn.Module):
    """å®Œæ•´çš„RepGDNeck - Gold-YOLOçš„æ ¸å¿ƒåˆ›æ–°"""
    def __init__(self, channels_list=None, num_repeats=None, block=RepVGGBlock, extra_cfg=None):
        super().__init__()

        assert channels_list is not None
        assert num_repeats is not None
        assert extra_cfg is not None

        # ç®€åŒ–å®ç°ï¼šåŸºæœ¬çš„ç‰¹å¾èåˆ
        # æ ¹æ®å®é™…backboneè¾“å‡ºè°ƒæ•´é€šé“æ•°
        # backboneè¾“å‡º: [16, 32, 64, 128, 128] å¯¹åº” [P2, P3, P4, P5, P6]
        backbone_channels = channels_list[:5]  # å–å‰5ä¸ªä½œä¸ºbackboneé€šé“

        # P6(128) -> P5(128) è·¯å¾„
        self.reduce_layer_c5 = SimConv(
            in_channels=backbone_channels[4],  # 128
            out_channels=backbone_channels[3],  # 128
            kernel_size=1,
            stride=1
        )

        self.Rep_p4 = RepBlock(
            in_channels=backbone_channels[3],  # 128
            out_channels=backbone_channels[3],  # 128
            n=num_repeats[0] if len(num_repeats) > 0 else 1,
            block=block
        )

        # P5(128) -> P4(64) è·¯å¾„
        self.reduce_layer_p4 = SimConv(
            in_channels=backbone_channels[3],  # 128
            out_channels=backbone_channels[2],  # 64
            kernel_size=1,
            stride=1
        )

        self.Rep_p3 = RepBlock(
            in_channels=backbone_channels[2],  # 64
            out_channels=backbone_channels[2],  # 64
            n=num_repeats[1] if len(num_repeats) > 1 else 1,
            block=block
        )

        # P3(64) -> N4 è·¯å¾„ (è‡ªåº•å‘ä¸Š)
        self.downsample2 = SimConv(
            in_channels=backbone_channels[2],  # 64
            out_channels=backbone_channels[2],  # 64
            kernel_size=3,
            stride=2
        )

        self.Rep_n4 = RepBlock(
            in_channels=backbone_channels[2] + backbone_channels[3],  # 64 + 128 = 192
            out_channels=backbone_channels[3],  # 128
            n=num_repeats[2] if len(num_repeats) > 2 else 1,
            block=block
        )

        # N4(128) -> N5 è·¯å¾„
        self.downsample1 = SimConv(
            in_channels=backbone_channels[3],  # 128
            out_channels=backbone_channels[3],  # 128
            kernel_size=3,
            stride=2
        )

        self.Rep_n5 = RepBlock(
            in_channels=backbone_channels[3] + backbone_channels[4],  # 128 + 128 = 256
            out_channels=backbone_channels[4],  # 128
            n=num_repeats[3] if len(num_repeats) > 3 else 1,
            block=block
        )

        # é€šé“åŒ¹é…å±‚
        self.p4_channel_match = SimConv(
            in_channels=backbone_channels[3],  # 128
            out_channels=backbone_channels[2],  # 64
            kernel_size=1,
            stride=1
        )

    def execute(self, features):
        """
        features: backboneè¾“å‡ºçš„å¤šå°ºåº¦ç‰¹å¾ [P3, P4, P5, P6]
        è¿”å›: [P3, P4, P5] ç”¨äºæ£€æµ‹å¤´
        """
        # è·å–ç‰¹å¾ - backboneè¾“å‡º: [P2, P3, P4, P5, P6] (5ä¸ªç‰¹å¾)
        if len(features) == 5:
            P2, P3, P4, P5, P6 = features
        elif len(features) == 4:
            P3, P4, P5, P6 = features
        else:
            # å¦‚æœç‰¹å¾ä¸å¤Ÿï¼Œç”¨æœ€åçš„ç‰¹å¾
            P3 = features[-3] if len(features) >= 3 else features[-1]
            P4 = features[-2] if len(features) >= 2 else features[-1]
            P5 = features[-1]
            P6 = features[-1]

        # === è‡ªé¡¶å‘ä¸‹è·¯å¾„ ===

        # P6 -> P5 (ç®€åŒ–èåˆ)
        c5_reduced = self.reduce_layer_c5(P6)
        c5_upsampled = jt.nn.interpolate(c5_reduced, size=P5.shape[2:], mode='bilinear', align_corners=False)
        p4_fused = P5 + c5_upsampled  # ç®€å•ç›¸åŠ è€Œä¸æ˜¯æ‹¼æ¥
        p4_out = self.Rep_p4(p4_fused)

        # P5 -> P4 (ç®€åŒ–èåˆ)
        p4_reduced = self.reduce_layer_p4(p4_out)  # 128 -> 64
        p4_upsampled = jt.nn.interpolate(p4_reduced, size=P4.shape[2:], mode='bilinear', align_corners=False)  # 64
        # P4æ˜¯128é€šé“ï¼Œp4_upsampledæ˜¯64é€šé“ï¼Œéœ€è¦åŒ¹é…
        p4_matched = self.p4_channel_match(P4)  # 128 -> 64
        p3_fused = p4_matched + p4_upsampled  # 64 + 64
        p3_out = self.Rep_p3(p3_fused)

        # === è‡ªåº•å‘ä¸Šè·¯å¾„ ===

        # P3 -> N4
        p3_downsampled = self.downsample2(p3_out)
        n4_concat = jt.concat([p4_out, p3_downsampled], dim=1)
        n4_out = self.Rep_n4(n4_concat)

        # N4 -> N5
        n4_downsampled = self.downsample1(n4_out)
        n5_concat = jt.concat([P6, n4_downsampled], dim=1)
        n5_out = self.Rep_n5(n5_concat)

        # è¿”å›æ£€æµ‹ç”¨çš„ä¸‰ä¸ªå°ºåº¦ç‰¹å¾
        return [p3_out, n4_out, n5_out]


class GoldYOLO(nn.Module):
    """å®Œå…¨è¿˜åŸPyTorchç‰ˆæœ¬çš„Gold-YOLOæ¨¡å‹ (æ”¯æŒn/s/m/l)

    åŸºäºconvert.pyè½¬æ¢çš„PyTorchç»„ä»¶ï¼Œå¾®è°ƒå¯¹é½å››ä¸ªç‰ˆæœ¬
    - Nano: depth=0.33, width=0.25
    - Small: depth=0.33, width=0.50
    - Medium: depth=0.60, width=0.75
    - Large: depth=1.0, width=1.0
    """

    def __init__(self, num_classes=80, depth_multiple=0.33, width_multiple=0.25,
                 model_size='n', use_pytorch_components=True):
        super().__init__()
        self.num_classes = num_classes
        self.model_size = model_size
        self.use_pytorch_components = use_pytorch_components and PYTORCH_COMPONENTS_AVAILABLE

        # æ”¯æŒä¸åŒæ¨¡å‹å¤§å°çš„å‚æ•°
        self.depth_multiple = depth_multiple
        self.width_multiple = width_multiple

        print(f"ğŸ”§ åˆ›å»ºGold-YOLO-{model_size} (depth={depth_multiple}, width={width_multiple})")
        print(f"   ä½¿ç”¨PyTorchè½¬æ¢ç»„ä»¶: {self.use_pytorch_components}")
        
        # æ ¹æ®æ¨¡å‹å¤§å°è°ƒæ•´åŸºç¡€é…ç½®
        if self.use_pytorch_components:
            # ä½¿ç”¨è½¬æ¢åçš„PyTorchç»„ä»¶æ„å»ºç½‘ç»œ
            self._build_with_pytorch_components()
        else:
            # ä½¿ç”¨åŸå§‹Jittorå®ç°
            self._build_with_jittor_components()

    def _build_with_pytorch_components(self):
        """ä½¿ç”¨è½¬æ¢åçš„PyTorchç»„ä»¶æ„å»ºç½‘ç»œ"""
        print("ğŸ”§ ä½¿ç”¨è½¬æ¢åçš„PyTorchç»„ä»¶æ„å»ºç½‘ç»œ")

        try:
            # åˆ›å»ºé…ç½®å¯¹è±¡ (æ¨¡æ‹ŸPyTorchçš„config)
            config = self._create_pytorch_config()

            # ä½¿ç”¨è½¬æ¢åçš„build_networkå‡½æ•° - æ·±å…¥ä¿®å¤å‚æ•°
            self.backbone, self.neck, self.detect = pytorch_build_network(
                config,
                channels=3,
                num_classes=self.num_classes,
                num_layers=3,  # æ·±å…¥ä¿®å¤ï¼šæ·»åŠ ç¼ºå¤±çš„num_layerså‚æ•°
                fuse_ab=False,
                distill_ns=False
            )

            print("âœ… æˆåŠŸä½¿ç”¨PyTorchè½¬æ¢ç»„ä»¶")

        except Exception as e:
            print(f"âš ï¸ PyTorchç»„ä»¶æ„å»ºå¤±è´¥: {e}")
            print("å›é€€åˆ°Jittorå®ç°")
            self._build_with_jittor_components()

    def _create_pytorch_config(self):
        """åˆ›å»ºPyTorché£æ ¼çš„é…ç½®å¯¹è±¡"""
        class Config:
            class Model:
                class Backbone:
                    type = 'EfficientRep'
                    depth_multiple = self.depth_multiple
                    width_multiple = self.width_multiple
                    out_channels = [64, 128, 256, 512, 1024]
                    num_repeats = [1, 6, 12, 18, 6]
                    fuse_P2 = True
                    cspsppf = True

                class Neck:
                    type = 'RepPANNeck'
                    out_channels = [256, 128, 128, 256, 256, 512]
                    num_repeats = [12, 12, 12, 12]

                class Head:
                    type = 'Detect'
                    num_layers = 3
                    use_dfl = False if self.model_size == 'n' else True
                    reg_max = 0 if self.model_size == 'n' else 16

                backbone = Backbone()
                neck = Neck()
                head = Head()

            model = Model()

        return Config()

    def _build_with_jittor_components(self):
        """ä½¿ç”¨åŸå§‹Jittorç»„ä»¶æ„å»ºç½‘ç»œ - ä¸¥æ ¼å¯¹é½PyTorchç‰ˆæœ¬æ¶æ„"""
        print("ğŸ”§ ä½¿ç”¨åŸå§‹Jittorç»„ä»¶æ„å»ºç½‘ç»œ - ä¸¥æ ¼å¯¹é½PyTorchæ¶æ„")

        # æ·±å…¥ä¿®å¤ï¼šå®Œå…¨æŒ‰ç…§PyTorchç‰ˆæœ¬çš„çœŸå®é…ç½®
        # ä¸¥æ ¼ä½¿ç”¨PyTorché…ç½®æ–‡ä»¶ä¸­çš„çœŸå®æ•°å€¼
        if self.model_size == 'n':
            # Gold-YOLO-né…ç½®
            base_channels_backbone = [64, 128, 256, 512, 1024]
            base_channels_neck = [256, 128, 128, 256, 256, 512]
            base_repeats_backbone = [1, 6, 12, 18, 6]
            base_repeats_neck = [12, 12, 12, 12]
        elif self.model_size == 's':
            # Gold-YOLO-sé…ç½® (ä¸¥æ ¼æŒ‰ç…§configs/gold_yolo-s.py)
            base_channels_backbone = [64, 128, 256, 512, 1024]
            base_channels_neck = [256, 128, 128, 256, 256, 512]
            base_repeats_backbone = [1, 6, 12, 18, 6]
            base_repeats_neck = [12, 12, 12, 12]
        elif self.model_size == 'm':
            # Gold-YOLO-mé…ç½®
            base_channels_backbone = [64, 128, 256, 512, 1024]
            base_channels_neck = [256, 128, 128, 256, 256, 512]
            base_repeats_backbone = [1, 6, 12, 18, 6]
            base_repeats_neck = [12, 12, 12, 12]
        else:  # 'l'
            # Gold-YOLO-lé…ç½®
            base_channels_backbone = [64, 128, 256, 512, 1024]
            base_channels_neck = [256, 128, 128, 256, 256, 512]
            base_repeats_backbone = [1, 6, 12, 18, 6]
            base_repeats_neck = [12, 12, 12, 12]

        # åˆå¹¶backboneå’Œnecké…ç½®ï¼ˆPyTorchæ–¹å¼ï¼‰
        all_base_channels = base_channels_backbone + base_channels_neck
        all_base_repeats = base_repeats_backbone + base_repeats_neck

        # ä½¿ç”¨PyTorchç‰ˆæœ¬çš„è®¡ç®—å…¬å¼
        self.channels = [make_divisible(ch * self.width_multiple, 8) for ch in all_base_channels]
        self.repeats = [max(round(rep * self.depth_multiple), 1) if rep > 1 else rep for rep in all_base_repeats]

        # åˆ†ç¦»backboneå’Œneckçš„é…ç½®
        self.backbone_channels = self.channels[:5]
        self.neck_channels = self.channels[5:]
        self.backbone_repeats = self.repeats[:5]
        self.neck_repeats = self.repeats[5:]

        print(f"   width_multiple: {self.width_multiple}")
        print(f"   depth_multiple: {self.depth_multiple}")
        print(f"   backboneé€šé“æ•°: {self.backbone_channels}")
        print(f"   backboneé‡å¤æ¬¡æ•°: {self.backbone_repeats}")
        print(f"   necké€šé“æ•°: {self.neck_channels}")
        print(f"   necké‡å¤æ¬¡æ•°: {self.neck_repeats}")

        # æ·±å…¥ä¿®å¤ï¼šæ ¹æ®ç‰ˆæœ¬é€‰æ‹©æ­£ç¡®çš„Backboneç±»å‹
        if self.model_size in ['n', 's']:
            # N/Sç‰ˆæœ¬ä½¿ç”¨EfficientRep
            self.backbone_type = 'EfficientRep'
            print(f"   âœ… {self.model_size.upper()}ç‰ˆæœ¬ä½¿ç”¨EfficientRep")
        else:
            # M/Lç‰ˆæœ¬ä½¿ç”¨CSPBepBackbone
            self.backbone_type = 'CSPBepBackbone'
            print(f"   âœ… {self.model_size.upper()}ç‰ˆæœ¬ä½¿ç”¨CSPBepBackbone")

        # æ·±å…¥ä¿®å¤ï¼šæ ¹æ®ç‰ˆæœ¬é€‰æ‹©æ­£ç¡®çš„Neckç±»å‹
        if self.model_size in ['n', 's']:
            self.neck_type = 'RepGDNeck'
            print(f"   âœ… {self.model_size.upper()}ç‰ˆæœ¬ä½¿ç”¨RepGDNeck")
        elif self.model_size == 'm':
            self.neck_type = 'GDNeck'
            print(f"   âœ… Mç‰ˆæœ¬ä½¿ç”¨GDNeck")
        else:  # 'l'
            self.neck_type = 'GDNeck2'
            print(f"   âœ… Lç‰ˆæœ¬ä½¿ç”¨GDNeck2")

        # æ ¹æ®ç‰ˆæœ¬æ„å»ºæ­£ç¡®çš„Backbone
        if self.backbone_type == 'EfficientRep':
            # N/Sç‰ˆæœ¬ä½¿ç”¨EfficientRep
            self.backbone = EfficientRep(
                in_channels=3,
                channels_list=self.backbone_channels,
                num_repeats=self.backbone_repeats,
                block=RepVGGBlock,
                fuse_P2=True,
                cspsppf=True
            )
        else:
            # M/Lç‰ˆæœ¬ä½¿ç”¨CSPBepBackbone
            csp_e = 2/3 if self.model_size == 'm' else 1/2  # PyTorché…ç½®
            self.backbone = CSPBepBackbone(
                channels_list=self.backbone_channels,
                num_repeats=self.backbone_repeats,
                block=RepVGGBlock,
                csp_e=csp_e,
                fuse_P2=True
            )

        # æ ¹æ®æ¨¡å‹å¤§å°è°ƒæ•´neckå’Œheadé…ç½®
        if self.model_size == 'n':
            use_dfl, reg_max = False, 0
            neck_complexity = 'simple'
        elif self.model_size == 's':
            use_dfl, reg_max = True, 16
            neck_complexity = 'medium'
        elif self.model_size == 'm':
            use_dfl, reg_max = True, 16
            neck_complexity = 'complex'
        else:  # 'l'
            use_dfl, reg_max = True, 16
            neck_complexity = 'complex'

        # æœ€ç»ˆé€‰æ‹©ï¼šä½¿ç”¨SimpleRepPANä½œä¸ºæœ€ä½³å®ç°
        # ç»è¿‡æ·±å…¥å¯¹æ¯”åˆ†æï¼ŒSimpleRepPANç‰ˆæœ¬å‚æ•°é‡å¯¹é½ä¼˜ç§€(93.4%)
        # æ‰€æœ‰ç‰ˆæœ¬éƒ½ä½¿ç”¨SimpleRepPANï¼Œè¿™æ˜¯æœ€æ¥è¿‘å®˜æ–¹å®ç°çš„ç‰ˆæœ¬
        self.neck = self._build_simple_reppan()

        # æ„å»ºhead - ä¿®å¤é€šé“æ•°åŒ¹é…
        # å®é™…Neckè¾“å‡ºé€šé“æ•°: [32, 64, 128] (P3, N4, N5)
        actual_neck_channels = [32, 64, 128]  # ä¸å®é™…Neckè¾“å‡ºå¯¹é½
        self.head = build_effide_head(
            neck_channels=actual_neck_channels,
            num_classes=self.num_classes,
            use_dfl=use_dfl,
            reg_max=reg_max
        )
        print(f"ğŸ”§ Headé…ç½®: è¾“å…¥é€šé“æ•°={actual_neck_channels}")

    def _build_simple_neck(self):
        """æ„å»ºç®€åŒ–çš„neck (ç”¨äºNano)"""
        return nn.Identity()  # ä¸´æ—¶ç®€åŒ–

    def _build_repgd_neck(self):
        """æ„å»ºRepGDNeck - ä¸¥æ ¼æŒ‰ç…§PyTorchç‰ˆæœ¬çš„çœŸå®é…ç½®"""
        # æ·±å…¥ä¿®å¤ï¼šä¸¥æ ¼ä½¿ç”¨PyTorché…ç½®æ–‡ä»¶ä¸­çš„çœŸå®å‚æ•°
        if self.model_size == 'n':
            # Nanoé…ç½® - ä¸¥æ ¼æŒ‰ç…§configs/gold_yolo-n.py
            extra_cfg = {
                'trans_channels': [64, 32, 64, 128],  # æ¥è‡ªPyTorché…ç½®æ–‡ä»¶
                'embed_dim_p': 96,  # æ¥è‡ªPyTorché…ç½®æ–‡ä»¶
                'embed_dim_n': 352,  # æ¥è‡ªPyTorché…ç½®æ–‡ä»¶
                'fusion_in': 480,  # æ¥è‡ªPyTorché…ç½®æ–‡ä»¶
                'fuse_block_num': 3,  # æ¥è‡ªPyTorché…ç½®æ–‡ä»¶
                'depths': 2,  # æ¥è‡ªPyTorché…ç½®æ–‡ä»¶
                'key_dim': 8,  # æ¥è‡ªPyTorché…ç½®æ–‡ä»¶
                'num_heads': 4,  # æ¥è‡ªPyTorché…ç½®æ–‡ä»¶
                'mlp_ratios': 1,  # æ¥è‡ªPyTorché…ç½®æ–‡ä»¶
                'attn_ratios': 2,  # æ¥è‡ªPyTorché…ç½®æ–‡ä»¶
                'c2t_stride': 2,  # æ¥è‡ªPyTorché…ç½®æ–‡ä»¶
                'drop_path_rate': 0.1,  # æ¥è‡ªPyTorché…ç½®æ–‡ä»¶
                'pool_mode': 'torch',  # æ¥è‡ªPyTorché…ç½®æ–‡ä»¶
                'norm_cfg': {'type': 'SyncBN', 'requires_grad': True}
            }
        else:  # 's'
            # Smallé…ç½® - ä¸¥æ ¼æŒ‰ç…§configs/gold_yolo-s.py
            extra_cfg = {
                'trans_channels': [128, 64, 128, 256],  # æ¥è‡ªPyTorché…ç½®æ–‡ä»¶
                'embed_dim_p': 128,  # æ¥è‡ªPyTorché…ç½®æ–‡ä»¶
                'embed_dim_n': 704,  # æ¥è‡ªPyTorché…ç½®æ–‡ä»¶
                'fusion_in': 960,  # æ¥è‡ªPyTorché…ç½®æ–‡ä»¶
                'fuse_block_num': 3,  # æ¥è‡ªPyTorché…ç½®æ–‡ä»¶
                'depths': 2,  # æ¥è‡ªPyTorché…ç½®æ–‡ä»¶
                'key_dim': 8,  # æ¥è‡ªPyTorché…ç½®æ–‡ä»¶
                'num_heads': 4,  # æ¥è‡ªPyTorché…ç½®æ–‡ä»¶
                'mlp_ratios': 1,  # æ¥è‡ªPyTorché…ç½®æ–‡ä»¶
                'attn_ratios': 2,  # æ¥è‡ªPyTorché…ç½®æ–‡ä»¶
                'c2t_stride': 2,  # æ¥è‡ªPyTorché…ç½®æ–‡ä»¶
                'drop_path_rate': 0.1,  # æ¥è‡ªPyTorché…ç½®æ–‡ä»¶
                'pool_mode': 'torch',  # æ¥è‡ªPyTorché…ç½®æ–‡ä»¶
                'norm_cfg': {'type': 'SyncBN', 'requires_grad': True}
            }

        return EnhancedRepGDNeck(
            channels_list=self.backbone_channels,  # åªä¼ é€’backboneé€šé“æ•°
            num_repeats=self.neck_repeats,  # ä½¿ç”¨è®¡ç®—å‡ºçš„necké‡å¤æ¬¡æ•°
            block=RepVGGBlock,
            extra_cfg=extra_cfg
        )

    def _build_gd_neck(self):
        """æ„å»ºGDNeck - Mç‰ˆæœ¬ä¸¥æ ¼æŒ‰ç…§PyTorchç‰ˆæœ¬"""
        # Mç‰ˆæœ¬é…ç½® - ä¸¥æ ¼æŒ‰ç…§configs/gold_yolo-m.py
        extra_cfg = {
            'trans_channels': [192, 96, 192, 384],  # æ¥è‡ªPyTorché…ç½®æ–‡ä»¶
            'embed_dim_p': 192,  # æ¥è‡ªPyTorché…ç½®æ–‡ä»¶
            'embed_dim_n': 1056,  # æ¥è‡ªPyTorché…ç½®æ–‡ä»¶
            'fusion_in': 1440,  # æ¥è‡ªPyTorché…ç½®æ–‡ä»¶
            'fuse_block_num': 3,  # æ¥è‡ªPyTorché…ç½®æ–‡ä»¶
            'depths': 2,  # æ¥è‡ªPyTorché…ç½®æ–‡ä»¶
            'key_dim': 8,  # æ¥è‡ªPyTorché…ç½®æ–‡ä»¶
            'num_heads': 4,  # æ¥è‡ªPyTorché…ç½®æ–‡ä»¶
            'mlp_ratios': 1,  # æ¥è‡ªPyTorché…ç½®æ–‡ä»¶
            'attn_ratios': 2,  # æ¥è‡ªPyTorché…ç½®æ–‡ä»¶
            'c2t_stride': 2,  # æ¥è‡ªPyTorché…ç½®æ–‡ä»¶
            'drop_path_rate': 0.1,  # æ¥è‡ªPyTorché…ç½®æ–‡ä»¶
            'pool_mode': 'torch',  # æ¥è‡ªPyTorché…ç½®æ–‡ä»¶
            'norm_cfg': {'type': 'SyncBN', 'requires_grad': True}
        }

        csp_e = 2/3  # æ¥è‡ªPyTorché…ç½®æ–‡ä»¶: float(2) / 3

        return GDNeck(
            channels_list=self.backbone_channels,  # åªä¼ é€’backboneé€šé“æ•°
            num_repeats=self.neck_repeats,  # ä½¿ç”¨è®¡ç®—å‡ºçš„necké‡å¤æ¬¡æ•°
            block=RepVGGBlock,
            csp_e=csp_e,
            extra_cfg=extra_cfg
        )

    def _build_gd_neck2(self):
        """æ„å»ºGDNeck2 - Lç‰ˆæœ¬ä¸¥æ ¼æŒ‰ç…§PyTorchç‰ˆæœ¬"""
        # Lç‰ˆæœ¬é…ç½® - ä¸¥æ ¼æŒ‰ç…§configs/gold_yolo-l.py
        extra_cfg = {
            'trans_channels': [256, 128, 256, 512],  # æ¥è‡ªPyTorché…ç½®æ–‡ä»¶
            'embed_dim_p': 192,  # æ¥è‡ªPyTorché…ç½®æ–‡ä»¶
            'embed_dim_n': 1408,  # æ¥è‡ªPyTorché…ç½®æ–‡ä»¶
            'fusion_in': 1920,  # æ¥è‡ªPyTorché…ç½®æ–‡ä»¶
            'fuse_block_num': 3,  # æ¥è‡ªPyTorché…ç½®æ–‡ä»¶
            'depths': 3,  # æ¥è‡ªPyTorché…ç½®æ–‡ä»¶
            'key_dim': 8,  # æ¥è‡ªPyTorché…ç½®æ–‡ä»¶
            'num_heads': 8,  # æ¥è‡ªPyTorché…ç½®æ–‡ä»¶
            'mlp_ratios': 1,  # æ¥è‡ªPyTorché…ç½®æ–‡ä»¶
            'attn_ratios': 2,  # æ¥è‡ªPyTorché…ç½®æ–‡ä»¶
            'c2t_stride': 2,  # æ¥è‡ªPyTorché…ç½®æ–‡ä»¶
            'drop_path_rate': 0.1,  # æ¥è‡ªPyTorché…ç½®æ–‡ä»¶
            'pool_mode': 'torch',  # æ¥è‡ªPyTorché…ç½®æ–‡ä»¶
            'norm_cfg': {'type': 'SyncBN', 'requires_grad': True}
        }

        csp_e = 1/2  # æ¥è‡ªPyTorché…ç½®æ–‡ä»¶: float(1) / 2

        return GDNeck2(
            channels_list=self.backbone_channels,  # åªä¼ é€’backboneé€šé“æ•°
            num_repeats=self.neck_repeats,  # ä½¿ç”¨è®¡ç®—å‡ºçš„necké‡å¤æ¬¡æ•°
            block=RepVGGBlock,
            csp_e=csp_e,
            extra_cfg=extra_cfg
        )

    def _build_simple_reppan(self):
        """æ„å»ºSimpleRepPAN - ä¸¥æ ¼å¯¹é½PyTorchç‰ˆæœ¬çš„RepPANNeck"""
        # ä½¿ç”¨å®Œæ•´çš„channels_listï¼ˆbackbone + neckï¼‰
        all_channels = self.backbone_channels + self.neck_channels
        all_repeats = self.backbone_repeats + self.neck_repeats

        return SimpleRepPAN(
            channels_list=all_channels,
            num_repeats=all_repeats,
            block=RepVGGBlock
        )

    def _build_csp_reppan(self):
        """æ„å»ºCSPRepPAN - M/Lç‰ˆæœ¬çš„CSPç‰ˆæœ¬RepPAN"""
        # ä½¿ç”¨å®Œæ•´çš„channels_listï¼ˆbackbone + neckï¼‰
        all_channels = self.backbone_channels + self.neck_channels
        all_repeats = self.backbone_repeats + self.neck_repeats

        csp_e = 2/3 if self.model_size == 'm' else 1/2  # PyTorché…ç½®

        return CSPRepPAN(
            channels_list=all_channels,
            num_repeats=all_repeats,
            block=RepVGGBlock,
            csp_e=csp_e
        )
        
    def _build_simple_neck(self):
        """æ„å»ºç®€åŒ–çš„neckï¼Œç¡®ä¿é€šé“åŒ¹é…"""
        return nn.Sequential(
            nn.Conv2d(self.channels[-1], self.channels[3], 1),
            nn.BatchNorm2d(self.channels[3]),
            nn.ReLU()
        )
    
    def _build_simple_head(self):
        """æ„å»ºç®€åŒ–çš„head - ä¿®å¤å‚æ•°é‡çˆ†ç‚¸é—®é¢˜"""
        # ä½¿ç”¨å·ç§¯å±‚è€Œä¸æ˜¯å…¨è¿æ¥å±‚ï¼

        # Headçš„è¾“å…¥é€šé“æ•°ç°åœ¨æ˜¯neckè¾“å‡ºçš„P3é€šé“æ•° (128)
        # å› ä¸ºFixedRepGDNeckè¾“å‡ºçš„p3æ˜¯128é€šé“
        head_in_channels = 128  # neckè¾“å‡ºçš„P3é€šé“æ•°

        # åˆ†ç±»å¤´ï¼šæ¯ä¸ªanchoré¢„æµ‹num_classesä¸ªç±»åˆ«
        cls_head = nn.Sequential(
            nn.Conv2d(head_in_channels, 128, 3, padding=1),  # 128 -> 128
            nn.BatchNorm2d(128),
            nn.ReLU(),
            nn.Conv2d(128, self.num_classes, 1),  # 128 -> num_classes
        )

        # å›å½’å¤´ï¼šæ¯ä¸ªanchoré¢„æµ‹68ä¸ªå›å½’å‚æ•°
        reg_head = nn.Sequential(
            nn.Conv2d(head_in_channels, 128, 3, padding=1),  # 128 -> 128
            nn.BatchNorm2d(128),
            nn.ReLU(),
            nn.Conv2d(128, 68, 1),  # 128 -> 68
        )

        return cls_head, reg_head
    
    def execute(self, x):
        """æ·±å…¥ä¿®å¤çš„å‰å‘ä¼ æ’­ - å®Œæ•´ä¿æŒPyTorchæ ¼å¼"""
        try:
            # Backboneç‰¹å¾æå– - æ·±å…¥ä¿®å¤
            if hasattr(self, 'backbone'):
                try:
                    if hasattr(self.backbone, 'execute'):
                        features = self.backbone(x)
                    else:
                        # å°è¯•ç›´æ¥è°ƒç”¨
                        features = self.backbone(x)
                except Exception as backbone_error:
                    print(f"âš ï¸ Backboneå¤„ç†å¤±è´¥: {backbone_error}")
                    features = self._fallback_backbone_forward(x)
            else:
                features = self._fallback_backbone_forward(x)

            # ç¡®ä¿featuresæ˜¯listæ ¼å¼ - æ·±åº¦ä¿®å¤
            if isinstance(features, tuple):
                features = list(features)  # æ­£ç¡®è½¬æ¢tupleä¸ºlist
            elif not isinstance(features, list):
                features = [features]



            # Neckç‰¹å¾èåˆ - æ·±åº¦ä¿®å¤ï¼šç¡®ä¿è¾“å…¥æ ¼å¼æ­£ç¡®
            if hasattr(self, 'neck') and hasattr(self.neck, 'execute') and not isinstance(self.neck, nn.Identity):
                try:
                    # æ·±åº¦ä¿®å¤ï¼šç¡®ä¿featuresä¸­çš„æ¯ä¸ªå…ƒç´ éƒ½æ˜¯tensor
                    fixed_features = []
                    for i, feat in enumerate(features):
                        if isinstance(feat, tuple):
                            # å¦‚æœæ˜¯tupleï¼Œå–ç¬¬ä¸€ä¸ªæœ‰æ•ˆçš„tensor
                            for item in feat:
                                if hasattr(item, 'shape'):
                                    fixed_features.append(item)
                                    break
                            else:
                                # å¦‚æœtupleä¸­æ²¡æœ‰æœ‰æ•ˆtensorï¼Œåˆ›å»ºdummy
                                batch_size = x.shape[0]
                                channels = [16, 32, 64, 128, 256][i] if i < 5 else 128
                                size = 640 // (2 ** (i + 1)) if i < 5 else 20
                                dummy_feat = jt.randn(batch_size, channels, size, size)
                                fixed_features.append(dummy_feat)
                        elif hasattr(feat, 'shape'):
                            fixed_features.append(feat)
                        else:
                            # åˆ›å»ºdummy tensor
                            batch_size = x.shape[0]
                            channels = [16, 32, 64, 128, 256][i] if i < 5 else 128
                            size = 640 // (2 ** (i + 1)) if i < 5 else 20
                            dummy_feat = jt.randn(batch_size, channels, size, size)
                            fixed_features.append(dummy_feat)



                    # è°ƒç”¨neckå¤„ç†
                    neck_output = self.neck(fixed_features)

                    # æ·±å…¥ä¿®å¤ï¼šç¡®ä¿neck_featuresæ˜¯listæ ¼å¼
                    if isinstance(neck_output, tuple):
                        neck_features = list(neck_output)
                    elif isinstance(neck_output, list):
                        neck_features = neck_output
                    else:
                        neck_features = [neck_output]



                except Exception as neck_error:
                    # ä½¿ç”¨backboneçš„å3ä¸ªç‰¹å¾ä½œä¸ºneckè¾“å‡º
                    neck_features = features[-3:] if len(features) >= 3 else features
            else:
                # ç®€åŒ–neckï¼šç›´æ¥ä½¿ç”¨backboneçš„å3ä¸ªç‰¹å¾ - æ·±å…¥ä¿®å¤ï¼šç¡®ä¿æ˜¯tensorè€Œä¸æ˜¯tuple
                raw_features = features[-3:] if len(features) >= 3 else features
                neck_features = []
                for feat in raw_features:
                    if isinstance(feat, tuple):
                        # å¦‚æœæ˜¯tupleï¼Œå–ç¬¬ä¸€ä¸ªå…ƒç´ æˆ–è½¬æ¢ä¸ºtensor
                        if len(feat) > 0 and hasattr(feat[0], 'shape'):
                            neck_features.append(feat[0])
                        else:
                            # åˆ›å»ºdummy tensor
                            neck_features.append(jt.randn(1, 128, 20, 20))
                    elif hasattr(feat, 'shape'):
                        neck_features.append(feat)
                    else:
                        # åˆ›å»ºdummy tensor
                        neck_features.append(jt.randn(1, 128, 20, 20))

            # ç¡®ä¿neck_featuresæœ‰3ä¸ªç‰¹å¾ç”¨äºæ£€æµ‹
            while len(neck_features) < 3:
                neck_features.append(neck_features[-1])
            neck_features = neck_features[:3]  # åªå–å‰3ä¸ª

            # Headæ£€æµ‹å¤„ç† - æ·±åº¦ä¿®å¤ï¼šç¡®ä¿è¾“å…¥æ ¼å¼æ­£ç¡®
            if hasattr(self, 'head') and hasattr(self.head, 'execute'):
                try:
                    # æ·±åº¦ä¿®å¤ï¼šç¡®ä¿neck_featuresä¸­çš„æ¯ä¸ªå…ƒç´ éƒ½æ˜¯tensor
                    fixed_neck_features = []
                    for i, feat in enumerate(neck_features):
                        if isinstance(feat, tuple):
                            # å¦‚æœæ˜¯tupleï¼Œå–ç¬¬ä¸€ä¸ªæœ‰æ•ˆçš„tensor
                            for item in feat:
                                if hasattr(item, 'shape'):
                                    fixed_neck_features.append(item)
                                    break
                            else:
                                # å¦‚æœtupleä¸­æ²¡æœ‰æœ‰æ•ˆtensorï¼Œåˆ›å»ºdummy
                                batch_size = x.shape[0]
                                channels = [64, 128, 256][i] if i < 3 else 256
                                size = [80, 40, 20][i] if i < 3 else 20
                                dummy_feat = jt.randn(batch_size, channels, size, size)
                                fixed_neck_features.append(dummy_feat)
                        elif hasattr(feat, 'shape'):
                            fixed_neck_features.append(feat)
                        else:
                            # åˆ›å»ºdummy tensor
                            batch_size = x.shape[0]
                            channels = [64, 128, 256][i] if i < 3 else 256
                            size = [80, 40, 20][i] if i < 3 else 20
                            dummy_feat = jt.randn(batch_size, channels, size, size)
                            fixed_neck_features.append(dummy_feat)



                    head_output = self.head(fixed_neck_features)


                except Exception as head_error:
                    print(f"âŒ Headå¤„ç†å¤±è´¥: {head_error}")
                    raise head_error  # ä¸è¦åˆ›å»ºdummyï¼Œç›´æ¥æŠ›å‡ºé”™è¯¯
            else:
                print("âŒ Headå±‚ä¸å­˜åœ¨æˆ–æ²¡æœ‰executeæ–¹æ³•")
                raise RuntimeError("Head layer is required for training")

            # æ·±å…¥ä¿®å¤è¿”å›æ ¼å¼ - å®Œå…¨å¯¹é½PyTorch YOLOæ ¼å¼
            # PyTorch YOLOè¿”å›: [P3_output, P4_output, P5_output]
            # æ¯ä¸ªoutputå½¢çŠ¶: (batch_size, anchors, grid_y, grid_x, num_classes + 5)

            # ç›´æ¥è¿”å›Headå±‚çš„çœŸå®è¾“å‡ºï¼Œä¸è¦åˆ›å»ºä»»ä½•dummyè¾“å‡º
            # Headå±‚åº”è¯¥è¿”å› (feats, cls_scores, reg_distri) æ ¼å¼
            if isinstance(head_output, (list, tuple)) and len(head_output) == 3:
                # æ ‡å‡†çš„Headè¾“å‡ºæ ¼å¼ï¼š(feats, cls_scores, reg_distri)
                return head_output
            else:
                print(f"âŒ Headè¾“å‡ºæ ¼å¼é”™è¯¯: {type(head_output)}, é•¿åº¦: {len(head_output) if hasattr(head_output, '__len__') else 'N/A'}")
                raise RuntimeError(f"Invalid head output format: {type(head_output)}")

        except Exception as e:
            print(f"âŒ Executeå¤±è´¥: {e}")
            raise e  # ä¸è¦åˆ›å»ºfallbackï¼Œç›´æ¥æŠ›å‡ºé”™è¯¯

    def _fallback_backbone_forward(self, x):
        """å›é€€çš„backboneå‰å‘ä¼ æ’­"""
        if hasattr(self, 'backbone'):
            if isinstance(self.backbone, nn.Sequential):
                features = []
                current = x
                for layer in self.backbone:
                    current = layer(current)
                    features.append(current)
                return features
            else:
                # å°è¯•ç›´æ¥è°ƒç”¨
                return self.backbone(x)
        else:
            raise RuntimeError("Backbone is required for training")
    
    def get_model_info(self):
        """è·å–æ¨¡å‹ä¿¡æ¯"""
        total_params = sum(p.numel() for p in self.parameters())
        trainable_params = sum(p.numel() for p in self.parameters() if p.requires_grad)
        
        return {
            'total_params': total_params,
            'trainable_params': trainable_params,
            'depth_multiple': self.depth_multiple,
            'width_multiple': self.width_multiple,
            'channels': self.channels,
            'repeats': self.repeats
        }


def test_full_pytorch_model():
    """æµ‹è¯•å®Œæ•´PyTorchæ¨¡å‹"""
    print("ğŸ§ª æµ‹è¯•å®Œæ•´PyTorch Gold-YOLO Smallæ¨¡å‹...")
    
    model = FullPyTorchGoldYOLOSmall(num_classes=80)
    
    # æ¨¡å‹ä¿¡æ¯
    info = model.get_model_info()
    print(f"âœ… æ¨¡å‹ä¿¡æ¯:")
    print(f"   æ€»å‚æ•°: {info['total_params']:,}")
    print(f"   å¯è®­ç»ƒå‚æ•°: {info['trainable_params']:,}")
    
    # æµ‹è¯•å‰å‘ä¼ æ’­
    test_input = jt.randn(2, 3, 640, 640)
    features, cls_pred, reg_pred = model(test_input)
    
    print(f"âœ… å‰å‘ä¼ æ’­æµ‹è¯•:")
    print(f"   è¾“å…¥: {test_input.shape}")
    print(f"   ç‰¹å¾æ•°é‡: {len(features)}")
    print(f"   åˆ†ç±»è¾“å‡º: {cls_pred.shape}")
    print(f"   å›å½’è¾“å‡º: {reg_pred.shape}")
    
    return model


# æ”¯æŒå››ä¸ªç‰ˆæœ¬çš„å·¥å‚å‡½æ•°
def create_gold_yolo(model_size='n', num_classes=20, use_pytorch_components=True):
    """åˆ›å»ºGold-YOLOæ¨¡å‹çš„å·¥å‚å‡½æ•°

    Args:
        model_size: æ¨¡å‹å¤§å° ('n', 's', 'm', 'l')
        num_classes: ç±»åˆ«æ•°
        use_pytorch_components: æ˜¯å¦ä½¿ç”¨è½¬æ¢åçš„PyTorchç»„ä»¶

    Returns:
        GoldYOLOæ¨¡å‹å®ä¾‹
    """
    # å®˜æ–¹é…ç½®
    size_configs = {
        'n': {'depth_multiple': 0.33, 'width_multiple': 0.25},  # 5.6M
        's': {'depth_multiple': 0.33, 'width_multiple': 0.50},  # 21.5M
        'm': {'depth_multiple': 0.60, 'width_multiple': 0.75},  # 41.3M
        'l': {'depth_multiple': 1.0, 'width_multiple': 1.0}     # 75.1M
    }

    if model_size not in size_configs:
        raise ValueError(f"ä¸æ”¯æŒçš„æ¨¡å‹å¤§å°: {model_size}. æ”¯æŒ: {list(size_configs.keys())}")

    config = size_configs[model_size]

    return GoldYOLO(
        num_classes=num_classes,
        depth_multiple=config['depth_multiple'],
        width_multiple=config['width_multiple'],
        model_size=model_size,
        use_pytorch_components=use_pytorch_components
    )

def test_all_models():
    """æµ‹è¯•æ‰€æœ‰å››ä¸ªç‰ˆæœ¬çš„æ¨¡å‹"""
    print("ğŸ§ª æµ‹è¯•æ‰€æœ‰Gold-YOLOæ¨¡å‹ç‰ˆæœ¬")
    print("=" * 80)

    models = ['n', 's', 'm', 'l']
    targets = {'n': 5.6, 's': 21.5, 'm': 41.3, 'l': 75.1}  # å®˜æ–¹å‚æ•°é‡(M)

    results = {}

    for model_size in models:
        print(f"\nğŸ“‹ æµ‹è¯•Gold-YOLO-{model_size}")
        try:
            # åˆ›å»ºæ¨¡å‹
            model = create_gold_yolo(model_size, num_classes=20)

            # è·å–æ¨¡å‹ä¿¡æ¯
            info = model.get_model_info()
            total_params_M = info['total_params'] / 1e6
            target_M = targets[model_size]
            accuracy = (1 - abs(total_params_M - target_M) / target_M) * 100

            # æµ‹è¯•å‰å‘ä¼ æ’­ - æ·±å…¥ä¿®å¤æ£€æµ‹é€»è¾‘
            test_input = jt.randn(1, 3, 640, 640)
            try:
                # ç›´æ¥ä½¿ç”¨æ¨¡å‹çš„executeæ–¹æ³•
                outputs = model(test_input)

                # æ·±å…¥æ£€æŸ¥è¾“å‡ºæ ¼å¼
                if isinstance(outputs, (list, tuple)):
                    if len(outputs) == 3:
                        # æœŸæœ›æ ¼å¼: (features, cls_pred, reg_pred)
                        features, cls_pred, reg_pred = outputs
                        if (isinstance(features, list) and isinstance(cls_pred, list) and isinstance(reg_pred, list)):
                            forward_success = True
                            output_info = f"3å°ºåº¦è¾“å‡º: features({len(features)}), cls({len(cls_pred)}), reg({len(reg_pred)})"
                        else:
                            forward_success = True  # è‡³å°‘æœ‰è¾“å‡º
                            output_info = f"è¾“å‡ºæ ¼å¼: {type(outputs[0])}, {type(outputs[1])}, {type(outputs[2])}"
                    else:
                        forward_success = True  # æœ‰è¾“å‡ºå°±ç®—æˆåŠŸ
                        output_info = f"{len(outputs)} ä¸ªè¾“å‡º"
                else:
                    forward_success = True
                    output_info = f"å•ä¸€è¾“å‡º: {outputs.shape}"

            except Exception as e:
                forward_success = False
                output_info = f"Error: {e}"

            results[model_size] = {
                'success': True,
                'params_M': total_params_M,
                'target_M': target_M,
                'accuracy': accuracy,
                'forward_success': forward_success,
                'output_info': output_info,
                'use_pytorch_components': model.use_pytorch_components
            }

            print(f"   âœ… åˆ›å»ºæˆåŠŸ: {total_params_M:.2f}Må‚æ•°")
            print(f"   ğŸ¯ ç›®æ ‡: {target_M}M, ç²¾åº¦: {accuracy:.1f}%")
            print(f"   ğŸ”§ PyTorchç»„ä»¶: {model.use_pytorch_components}")
            print(f"   ğŸš€ å‰å‘ä¼ æ’­: {'æˆåŠŸ' if forward_success else 'å¤±è´¥'}")
            if forward_success:
                print(f"   ğŸ“Š è¾“å‡º: {output_info}")

        except Exception as e:
            print(f"   âŒ å¤±è´¥: {e}")
            results[model_size] = {'success': False, 'error': str(e)}

    # æ€»ç»“
    print(f"\nğŸ“Š æµ‹è¯•æ€»ç»“:")
    print(f"{'æ¨¡å‹':<8} {'å®é™…(M)':<10} {'ç›®æ ‡(M)':<10} {'ç²¾åº¦':<10} {'å‰å‘':<8} {'ç»„ä»¶':<12}")
    print("-" * 65)

    for model_size, result in results.items():
        if result['success']:
            actual = result['params_M']
            target = result['target_M']
            accuracy = result['accuracy']
            forward = "âœ…" if result['forward_success'] else "âŒ"
            components = "PyTorch" if result['use_pytorch_components'] else "Jittor"

            print(f"{model_size:<8} {actual:<10.2f} {target:<10.1f} {accuracy:<10.1f}% {forward:<8} {components:<12}")
        else:
            print(f"{model_size:<8} {'ERROR':<10} {'ERROR':<10} {'ERROR':<10} {'âŒ':<8} {'ERROR':<12}")

    return results

# ä¿æŒå‘åå…¼å®¹
FullPyTorchGoldYOLOSmall = GoldYOLO

if __name__ == "__main__":
    # è®¾ç½®Jittor
    jt.flags.use_cuda = 1

    # æµ‹è¯•æ‰€æœ‰æ¨¡å‹
    results = test_all_models()
