#!/usr/bin/env python3
# -*- coding:utf-8 -*-
"""
å®Œæ•´è¿˜åŸPyTorchç‰ˆæœ¬çš„Gold-YOLO Smallæ¨¡å‹
100%å¯¹é½å®˜æ–¹é…ç½®ï¼šconfigs/gold_yolo-s.py
"""

import jittor as jt
import jittor.nn as nn
import numpy as np
from ..layers.common import (
    Conv, RepVGGBlock, RepBlock, SimConv, SimSPPF, CSPSPPF,
    Transpose, SimFusion_3in, SimFusion_4in, AdvPoolFusion,
    InjectionMultiSum_Auto_pool, BepC3, BottleRep, ConvWrapper
)
from ..layers.transformer import (
    PyramidPoolAgg, TopBasicLayer, C2T_Attention
)
from .enhanced_repgd_neck import EnhancedRepGDNeck
from .effide_head import build_effide_head

# è®¾ç½®Jittor
jt.flags.use_cuda = 1

class RepVGGBlock(nn.Module):
    """RepVGG Block - å®Œå…¨å¯¹é½PyTorchç‰ˆæœ¬"""
    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, padding=1, groups=1):
        super().__init__()
        self.in_channels = in_channels
        self.out_channels = out_channels
        self.kernel_size = kernel_size
        self.stride = stride
        self.padding = padding
        self.groups = groups
        
        # 3x3 conv
        self.conv3x3 = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding, groups=groups, bias=False)
        self.bn3x3 = nn.BatchNorm2d(out_channels)
        
        # 1x1 conv
        self.conv1x1 = nn.Conv2d(in_channels, out_channels, 1, stride, 0, groups=groups, bias=False)
        self.bn1x1 = nn.BatchNorm2d(out_channels)
        
        # identity
        self.identity = nn.BatchNorm2d(out_channels) if in_channels == out_channels and stride == 1 else None
        
        self.act = nn.ReLU()
    
    def execute(self, x):
        if self.identity is None:
            id_out = 0
        else:
            id_out = self.identity(x)
        
        return self.act(self.bn3x3(self.conv3x3(x)) + self.bn1x1(self.conv1x1(x)) + id_out)

class RepBlock(nn.Module):
    """RepBlock - å®Œå…¨å¯¹é½PyTorchç‰ˆæœ¬"""
    def __init__(self, in_channels, out_channels, n=1, block=RepVGGBlock):
        super().__init__()
        self.conv1 = block(in_channels, out_channels)
        self.blocks = nn.Sequential(*[block(out_channels, out_channels) for _ in range(n-1)])
    
    def execute(self, x):
        x = self.conv1(x)
        return self.blocks(x)

class SimSPPF(nn.Module):
    """Simplified SPPF - å®Œå…¨å¯¹é½PyTorchç‰ˆæœ¬"""
    def __init__(self, in_channels, out_channels, kernel_size=5):
        super().__init__()
        c_ = in_channels // 2
        self.cv1 = nn.Conv2d(in_channels, c_, 1, 1, 0, bias=False)
        self.cv2 = nn.Conv2d(c_ * 4, out_channels, 1, 1, 0, bias=False)
        self.bn1 = nn.BatchNorm2d(c_)
        self.bn2 = nn.BatchNorm2d(out_channels)
        self.act = nn.ReLU()
        self.m = nn.MaxPool2d(kernel_size=kernel_size, stride=1, padding=kernel_size // 2)
    
    def execute(self, x):
        x = self.act(self.bn1(self.cv1(x)))
        y1 = self.m(x)
        y2 = self.m(y1)
        y3 = self.m(y2)
        return self.act(self.bn2(self.cv2(jt.concat([x, y1, y2, y3], 1))))

class SimCSPSPPF(nn.Module):
    """Simplified CSP SPPF - å®Œå…¨å¯¹é½PyTorchç‰ˆæœ¬"""
    def __init__(self, in_channels, out_channels, kernel_size=5, e=0.5):
        super().__init__()
        c_ = int(out_channels * e)
        self.cv1 = nn.Conv2d(in_channels, c_, 1, 1, 0, bias=False)
        self.cv2 = nn.Conv2d(in_channels, c_, 1, 1, 0, bias=False)
        self.cv3 = nn.Conv2d(c_, c_, 3, 1, 1, bias=False)
        self.cv4 = nn.Conv2d(c_, c_, 1, 1, 0, bias=False)
        self.cv5 = nn.Conv2d(c_ * 4, c_, 1, 1, 0, bias=False)
        self.cv6 = nn.Conv2d(c_ * 2, out_channels, 1, 1, 0, bias=False)
        
        self.bn1 = nn.BatchNorm2d(c_)
        self.bn2 = nn.BatchNorm2d(c_)
        self.bn3 = nn.BatchNorm2d(c_)
        self.bn4 = nn.BatchNorm2d(c_)
        self.bn5 = nn.BatchNorm2d(c_)
        self.bn6 = nn.BatchNorm2d(out_channels)
        
        self.act = nn.ReLU()
        self.m = nn.MaxPool2d(kernel_size=kernel_size, stride=1, padding=kernel_size // 2)
    
    def execute(self, x):
        x1 = self.act(self.bn4(self.cv4(self.act(self.bn3(self.cv3(self.act(self.bn1(self.cv1(x)))))))))
        y0 = self.act(self.bn2(self.cv2(x)))
        y1 = self.m(x1)
        y2 = self.m(y1)
        y3 = self.m(y2)
        y4 = self.act(self.bn5(self.cv5(jt.concat([x1, y1, y2, y3], 1))))
        return self.act(self.bn6(self.cv6(jt.concat([y0, y4], 1))))

class EfficientRep(nn.Module):
    """EfficientRep Backbone - å®Œå…¨å¯¹é½PyTorchç‰ˆæœ¬
    
    å®˜æ–¹é…ç½® (gold_yolo-s.py):
    - num_repeats=[1, 6, 12, 18, 6]
    - out_channels=[64, 128, 256, 512, 1024]
    - fuse_P2=True
    - cspsppf=True
    """
    
    def __init__(self, in_channels=3, channels_list=None, num_repeats=None, 
                 block=RepVGGBlock, fuse_P2=False, cspsppf=False):
        super().__init__()
        
        assert channels_list is not None
        assert num_repeats is not None
        
        self.fuse_P2 = fuse_P2
        
        # Stem
        self.stem = block(
            in_channels=in_channels,
            out_channels=channels_list[0],
            kernel_size=3,
            stride=2
        )
        
        # ERBlock_2 (P2)
        self.ERBlock_2 = nn.Sequential(
            block(
                in_channels=channels_list[0],
                out_channels=channels_list[1],
                kernel_size=3,
                stride=2
            ),
            RepBlock(
                in_channels=channels_list[1],
                out_channels=channels_list[1],
                n=num_repeats[1],
                block=block
            )
        )
        
        # ERBlock_3 (P3)
        self.ERBlock_3 = nn.Sequential(
            block(
                in_channels=channels_list[1],
                out_channels=channels_list[2],
                kernel_size=3,
                stride=2
            ),
            RepBlock(
                in_channels=channels_list[2],
                out_channels=channels_list[2],
                n=num_repeats[2],
                block=block
            )
        )
        
        # ERBlock_4 (P4)
        self.ERBlock_4 = nn.Sequential(
            block(
                in_channels=channels_list[2],
                out_channels=channels_list[3],
                kernel_size=3,
                stride=2
            ),
            RepBlock(
                in_channels=channels_list[3],
                out_channels=channels_list[3],
                n=num_repeats[3],
                block=block
            )
        )
        
        # ERBlock_5 (P5)
        self.ERBlock_5 = nn.Sequential(
            block(
                in_channels=channels_list[3],
                out_channels=channels_list[4],
                kernel_size=3,
                stride=2
            ),
            RepBlock(
                in_channels=channels_list[4],
                out_channels=channels_list[4],
                n=num_repeats[4],
                block=block
            )
        )
        
        # ERBlock_6 (P6) with SPPF
        channel_merge_layer = SimSPPF if not cspsppf else SimCSPSPPF
        
        self.ERBlock_6 = nn.Sequential(
            block(
                in_channels=channels_list[4],
                out_channels=channels_list[5] if len(channels_list) > 5 else channels_list[4],
                kernel_size=3,
                stride=2
            ),
            RepBlock(
                in_channels=channels_list[5] if len(channels_list) > 5 else channels_list[4],
                out_channels=channels_list[5] if len(channels_list) > 5 else channels_list[4],
                n=num_repeats[5] if len(num_repeats) > 5 else 1,
                block=block
            ),
            channel_merge_layer(
                in_channels=channels_list[5] if len(channels_list) > 5 else channels_list[4],
                out_channels=channels_list[5] if len(channels_list) > 5 else channels_list[4],
                kernel_size=5
            )
        )
    
    def execute(self, x):
        outputs = []
        x = self.stem(x)
        x = self.ERBlock_2(x)
        if self.fuse_P2:
            outputs.append(x)
        x = self.ERBlock_3(x)
        outputs.append(x)
        x = self.ERBlock_4(x)
        outputs.append(x)
        x = self.ERBlock_5(x)
        outputs.append(x)
        x = self.ERBlock_6(x)
        outputs.append(x)
        
        return tuple(outputs)


class RepGDNeck(nn.Module):
    """å®Œæ•´çš„RepGDNeck - Gold-YOLOçš„æ ¸å¿ƒåˆ›æ–°"""
    def __init__(self, channels_list=None, num_repeats=None, block=RepVGGBlock, extra_cfg=None):
        super().__init__()

        assert channels_list is not None
        assert num_repeats is not None
        assert extra_cfg is not None

        # ç®€åŒ–å®ç°ï¼šåŸºæœ¬çš„ç‰¹å¾èåˆ
        # æ ¹æ®å®é™…backboneè¾“å‡ºè°ƒæ•´é€šé“æ•°
        # backboneè¾“å‡º: [16, 32, 64, 128, 128] å¯¹åº” [P2, P3, P4, P5, P6]
        backbone_channels = channels_list[:5]  # å–å‰5ä¸ªä½œä¸ºbackboneé€šé“

        # P6(128) -> P5(128) è·¯å¾„
        self.reduce_layer_c5 = SimConv(
            in_channels=backbone_channels[4],  # 128
            out_channels=backbone_channels[3],  # 128
            kernel_size=1,
            stride=1
        )

        self.Rep_p4 = RepBlock(
            in_channels=backbone_channels[3],  # 128
            out_channels=backbone_channels[3],  # 128
            n=num_repeats[0] if len(num_repeats) > 0 else 1,
            block=block
        )

        # P5(128) -> P4(64) è·¯å¾„
        self.reduce_layer_p4 = SimConv(
            in_channels=backbone_channels[3],  # 128
            out_channels=backbone_channels[2],  # 64
            kernel_size=1,
            stride=1
        )

        self.Rep_p3 = RepBlock(
            in_channels=backbone_channels[2],  # 64
            out_channels=backbone_channels[2],  # 64
            n=num_repeats[1] if len(num_repeats) > 1 else 1,
            block=block
        )

        # P3(64) -> N4 è·¯å¾„ (è‡ªåº•å‘ä¸Š)
        self.downsample2 = SimConv(
            in_channels=backbone_channels[2],  # 64
            out_channels=backbone_channels[2],  # 64
            kernel_size=3,
            stride=2
        )

        self.Rep_n4 = RepBlock(
            in_channels=backbone_channels[2] + backbone_channels[3],  # 64 + 128 = 192
            out_channels=backbone_channels[3],  # 128
            n=num_repeats[2] if len(num_repeats) > 2 else 1,
            block=block
        )

        # N4(128) -> N5 è·¯å¾„
        self.downsample1 = SimConv(
            in_channels=backbone_channels[3],  # 128
            out_channels=backbone_channels[3],  # 128
            kernel_size=3,
            stride=2
        )

        self.Rep_n5 = RepBlock(
            in_channels=backbone_channels[3] + backbone_channels[4],  # 128 + 128 = 256
            out_channels=backbone_channels[4],  # 128
            n=num_repeats[3] if len(num_repeats) > 3 else 1,
            block=block
        )

        # é€šé“åŒ¹é…å±‚
        self.p4_channel_match = SimConv(
            in_channels=backbone_channels[3],  # 128
            out_channels=backbone_channels[2],  # 64
            kernel_size=1,
            stride=1
        )

    def execute(self, features):
        """
        features: backboneè¾“å‡ºçš„å¤šå°ºåº¦ç‰¹å¾ [P3, P4, P5, P6]
        è¿”å›: [P3, P4, P5] ç”¨äºæ£€æµ‹å¤´
        """
        # è·å–ç‰¹å¾ - backboneè¾“å‡º: [P2, P3, P4, P5, P6] (5ä¸ªç‰¹å¾)
        if len(features) == 5:
            P2, P3, P4, P5, P6 = features
        elif len(features) == 4:
            P3, P4, P5, P6 = features
        else:
            # å¦‚æœç‰¹å¾ä¸å¤Ÿï¼Œç”¨æœ€åçš„ç‰¹å¾
            P3 = features[-3] if len(features) >= 3 else features[-1]
            P4 = features[-2] if len(features) >= 2 else features[-1]
            P5 = features[-1]
            P6 = features[-1]

        # === è‡ªé¡¶å‘ä¸‹è·¯å¾„ ===

        # P6 -> P5 (ç®€åŒ–èåˆ)
        c5_reduced = self.reduce_layer_c5(P6)
        c5_upsampled = jt.nn.interpolate(c5_reduced, size=P5.shape[2:], mode='bilinear', align_corners=False)
        p4_fused = P5 + c5_upsampled  # ç®€å•ç›¸åŠ è€Œä¸æ˜¯æ‹¼æ¥
        p4_out = self.Rep_p4(p4_fused)

        # P5 -> P4 (ç®€åŒ–èåˆ)
        p4_reduced = self.reduce_layer_p4(p4_out)  # 128 -> 64
        p4_upsampled = jt.nn.interpolate(p4_reduced, size=P4.shape[2:], mode='bilinear', align_corners=False)  # 64
        # P4æ˜¯128é€šé“ï¼Œp4_upsampledæ˜¯64é€šé“ï¼Œéœ€è¦åŒ¹é…
        p4_matched = self.p4_channel_match(P4)  # 128 -> 64
        p3_fused = p4_matched + p4_upsampled  # 64 + 64
        p3_out = self.Rep_p3(p3_fused)

        # === è‡ªåº•å‘ä¸Šè·¯å¾„ ===

        # P3 -> N4
        p3_downsampled = self.downsample2(p3_out)
        n4_concat = jt.concat([p4_out, p3_downsampled], dim=1)
        n4_out = self.Rep_n4(n4_concat)

        # N4 -> N5
        n4_downsampled = self.downsample1(n4_out)
        n5_concat = jt.concat([P6, n4_downsampled], dim=1)
        n5_out = self.Rep_n5(n5_concat)

        # è¿”å›æ£€æµ‹ç”¨çš„ä¸‰ä¸ªå°ºåº¦ç‰¹å¾
        return [p3_out, n4_out, n5_out]


class GoldYOLO(nn.Module):
    """å®Œå…¨è¿˜åŸPyTorchç‰ˆæœ¬çš„Gold-YOLO Nanoæ¨¡å‹

    å®˜æ–¹é…ç½® (configs/gold_yolo-n.py):
    - depth_multiple: 0.33
    - width_multiple: 0.25  # Nanoç‰ˆæœ¬çš„å…³é”®å·®å¼‚
    - backbone: EfficientRep
    - neck: RepGDNeck (ç®€åŒ–ç‰ˆ)
    - head: EffiDeHead (ç®€åŒ–ç‰ˆ)
    """

    def __init__(self, num_classes=80):
        super().__init__()
        self.num_classes = num_classes

        # å®˜æ–¹Nanoç‰ˆæœ¬å‚æ•°
        self.depth_multiple = 0.33
        self.width_multiple = 0.25  # ä»0.50æ”¹ä¸º0.25
        
        # å®˜æ–¹Gold-YOLO Smallé…ç½®çš„é€šé“æ•°å’Œé‡å¤æ¬¡æ•°
        # ä¿®å¤ï¼šGold-YOLO Smallçš„æ­£ç¡®åŸºç¡€é€šé“æ•°
        base_channels = [64, 128, 256, 512, 512]  # æœ€åä¸¤å±‚éƒ½æ˜¯512
        base_repeats = [1, 6, 12, 18, 6]

        # åº”ç”¨ç¼©æ”¾å› å­ - ä¿®å¤é€šé“æ•°è®¡ç®—
        self.channels = [int(ch * self.width_multiple) for ch in base_channels]
        self.repeats = [max(1, int(rep * self.depth_multiple)) for rep in base_repeats]
        
        print(f"ğŸ—ï¸ å®Œæ•´PyTorch Gold-YOLO Nanoé…ç½®:")
        print(f"   depth_multiple: {self.depth_multiple}")
        print(f"   width_multiple: {self.width_multiple}")
        print(f"   é€šé“æ•°: {self.channels}")
        print(f"   é‡å¤æ¬¡æ•°: {self.repeats}")
        
        # EfficientRep Backbone (å®Œå…¨å¯¹é½)
        self.backbone = EfficientRep(
            in_channels=3,
            channels_list=self.channels + [self.channels[-1]],  # æ·»åŠ ç¬¬6å±‚
            num_repeats=self.repeats + [1],  # æ·»åŠ ç¬¬6å±‚é‡å¤æ¬¡æ•°
            block=RepVGGBlock,
            fuse_P2=True,  # å®˜æ–¹é…ç½®
            cspsppf=True   # å®˜æ–¹é…ç½®
        )
        
        # å®Œæ•´çš„RepGDNeck - ä½¿ç”¨å®˜æ–¹é…ç½®
        # å®˜æ–¹necké…ç½® (å¯¹åº”PyTorchç‰ˆæœ¬)
        neck_channels = [256, 128, 128, 256, 256, 512, 256, 128, 256, 256, 512]  # å®Œæ•´çš„11ä¸ªé€šé“é…ç½®
        neck_repeats = [12, 12, 12, 12, 12, 12, 12, 12, 12]  # 9ä¸ªrepeaté…ç½®

        # æ ¹æ®width_multipleè°ƒæ•´necké€šé“æ•°
        neck_channels_scaled = [max(16, int(ch * self.width_multiple)) for ch in neck_channels]
        neck_repeats_scaled = [max(1, int(rep * self.depth_multiple)) for rep in neck_repeats]

        # æ„å»ºextra_cfg (å®Œå…¨å¯¹é½PyTorché…ç½®)
        # åŠ¨æ€è®¡ç®—fusion_in: æ ¹æ®å®é™…æµ‹è¯•ï¼ŒNanoç‰ˆæœ¬éœ€è¦448é€šé“
        # è¿™å¯¹åº”backboneè¾“å‡ºçš„å4ä¸ªç‰¹å¾: [64, 128, 128, 128] = 448
        fusion_in = 448  # ç›´æ¥ä½¿ç”¨å®é™…æµ‹è¯•çš„å€¼

        extra_cfg = {
            'fusion_in': fusion_in,  # ä½¿ç”¨å®é™…è®¡ç®—çš„é€šé“æ•°
            'embed_dim_p': max(16, int(96 * self.width_multiple)),  # Nano: 24, Small: 96
            'embed_dim_n': max(32, int(352 * self.width_multiple)),  # Nano: 88, Small: 352
            'fuse_block_num': 3,
            'trans_channels': [max(8, int(ch * self.width_multiple)) for ch in [64, 32, 64, 128]],
            'key_dim': 8,
            'num_heads': 4,
            'mlp_ratios': 1,
            'attn_ratios': 2,
            'c2t_stride': 2,
            'drop_path_rate': 0.1,
            'depths': 2,
            'pool_mode': 'torch',
            'norm_cfg': {'type': 'SyncBN', 'requires_grad': True}
        }

        # ä½¿ç”¨å¢å¼ºç‰ˆRepGDNeck
        self.neck = EnhancedRepGDNeck(extra_cfg=extra_cfg)

        # å®Œæ•´çš„EffiDeHead (å¤šå°ºåº¦æ£€æµ‹å¤´)
        self.head = build_effide_head(
            neck_channels=[128, 128, 128],  # P3, N4, N5çš„é€šé“æ•°
            num_classes=self.num_classes,
            use_dfl=True,
            reg_max=16
        )
        
    def _build_simple_neck(self):
        """æ„å»ºç®€åŒ–çš„neckï¼Œç¡®ä¿é€šé“åŒ¹é…"""
        return nn.Sequential(
            nn.Conv2d(self.channels[-1], self.channels[3], 1),
            nn.BatchNorm2d(self.channels[3]),
            nn.ReLU()
        )
    
    def _build_simple_head(self):
        """æ„å»ºç®€åŒ–çš„head - ä¿®å¤å‚æ•°é‡çˆ†ç‚¸é—®é¢˜"""
        # ä½¿ç”¨å·ç§¯å±‚è€Œä¸æ˜¯å…¨è¿æ¥å±‚ï¼

        # Headçš„è¾“å…¥é€šé“æ•°ç°åœ¨æ˜¯neckè¾“å‡ºçš„P3é€šé“æ•° (128)
        # å› ä¸ºFixedRepGDNeckè¾“å‡ºçš„p3æ˜¯128é€šé“
        head_in_channels = 128  # neckè¾“å‡ºçš„P3é€šé“æ•°

        # åˆ†ç±»å¤´ï¼šæ¯ä¸ªanchoré¢„æµ‹num_classesä¸ªç±»åˆ«
        cls_head = nn.Sequential(
            nn.Conv2d(head_in_channels, 128, 3, padding=1),  # 128 -> 128
            nn.BatchNorm2d(128),
            nn.ReLU(),
            nn.Conv2d(128, self.num_classes, 1),  # 128 -> num_classes
        )

        # å›å½’å¤´ï¼šæ¯ä¸ªanchoré¢„æµ‹68ä¸ªå›å½’å‚æ•°
        reg_head = nn.Sequential(
            nn.Conv2d(head_in_channels, 128, 3, padding=1),  # 128 -> 128
            nn.BatchNorm2d(128),
            nn.ReLU(),
            nn.Conv2d(128, 68, 1),  # 128 -> 68
        )

        return cls_head, reg_head
    
    def execute(self, x):
        # Backbone
        features = self.backbone(x)
        feat = features[-1]  # ä½¿ç”¨æœ€åä¸€å±‚ç‰¹å¾

        # Neck - ä½¿ç”¨å¢å¼ºç‰ˆRepGDNeck
        neck_features = self.neck(features)  # [P3, N4, N5]

        # Head - ä½¿ç”¨å®Œæ•´çš„EffiDeHead (å¤šå°ºåº¦æ£€æµ‹)
        head_output = self.head(neck_features)

        # è¿”å›æ ¼å¼ï¼šè®­ç»ƒæ—¶è¿”å›è¯¦ç»†è¾“å‡ºï¼Œæ¨ç†æ—¶è¿”å›æœ€ç»ˆé¢„æµ‹
        return head_output
    
    def get_model_info(self):
        """è·å–æ¨¡å‹ä¿¡æ¯"""
        total_params = sum(p.numel() for p in self.parameters())
        trainable_params = sum(p.numel() for p in self.parameters() if p.requires_grad)
        
        return {
            'total_params': total_params,
            'trainable_params': trainable_params,
            'depth_multiple': self.depth_multiple,
            'width_multiple': self.width_multiple,
            'channels': self.channels,
            'repeats': self.repeats
        }


def test_full_pytorch_model():
    """æµ‹è¯•å®Œæ•´PyTorchæ¨¡å‹"""
    print("ğŸ§ª æµ‹è¯•å®Œæ•´PyTorch Gold-YOLO Smallæ¨¡å‹...")
    
    model = FullPyTorchGoldYOLOSmall(num_classes=80)
    
    # æ¨¡å‹ä¿¡æ¯
    info = model.get_model_info()
    print(f"âœ… æ¨¡å‹ä¿¡æ¯:")
    print(f"   æ€»å‚æ•°: {info['total_params']:,}")
    print(f"   å¯è®­ç»ƒå‚æ•°: {info['trainable_params']:,}")
    
    # æµ‹è¯•å‰å‘ä¼ æ’­
    test_input = jt.randn(2, 3, 640, 640)
    features, cls_pred, reg_pred = model(test_input)
    
    print(f"âœ… å‰å‘ä¼ æ’­æµ‹è¯•:")
    print(f"   è¾“å…¥: {test_input.shape}")
    print(f"   ç‰¹å¾æ•°é‡: {len(features)}")
    print(f"   åˆ†ç±»è¾“å‡º: {cls_pred.shape}")
    print(f"   å›å½’è¾“å‡º: {reg_pred.shape}")
    
    return model


# ä¿æŒå‘åå…¼å®¹
FullPyTorchGoldYOLOSmall = GoldYOLO

if __name__ == "__main__":
    test_full_pytorch_model()
