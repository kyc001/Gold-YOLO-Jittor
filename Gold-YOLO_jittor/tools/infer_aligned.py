#!/usr/bin/env python3
# -*- coding:utf-8 -*-
"""
å®Œå…¨å¯¹é½PyTorchç‰ˆæœ¬çš„Gold-YOLOæ¨ç†è„šæœ¬
ä¸¥æ ¼æŒ‰ç…§PyTorchç‰ˆæœ¬çš„æ¨ç†é€»è¾‘å’Œåå¤„ç†
"""

import argparse
import os
import sys
import time
import yaml
import os.path as osp
from pathlib import Path

import jittor as jt
import jittor.nn as nn
import numpy as np
import cv2
from PIL import Image, ImageDraw, ImageFont

# è®¾ç½®Jittor
jt.flags.use_cuda = 1

# æ·»åŠ é¡¹ç›®è·¯å¾„
ROOT = os.getcwd()
if str(ROOT) not in sys.path:
    sys.path.append(str(ROOT))

# å¯¼å…¥Gold-YOLOç»„ä»¶
from gold_yolo.models.gold_yolo import create_gold_yolo
from gold_yolo.utils.general import increment_name, letterbox, scale_coords
from gold_yolo.utils.events import LOGGER
from gold_yolo.utils.postprocess import non_max_suppression


def get_args_parser(add_help=True):
    """å®Œå…¨å¯¹é½PyTorchç‰ˆæœ¬çš„æ¨ç†å‚æ•°è§£æå™¨"""
    parser = argparse.ArgumentParser(description='Gold-YOLO Jittor Inference', add_help=add_help)
    
    # æ¨¡å‹å’Œæ•°æ®å‚æ•°
    parser.add_argument('--weights', type=str, default='weights/gold_yolo_s.pt', help='model path(s) for inference')
    parser.add_argument('--source', type=str, default='data/images', help='the source path, e.g. image-file/dir')
    parser.add_argument('--yaml', type=str, default='data/coco.yaml', help='data yaml file')
    parser.add_argument('--img-size', nargs='+', type=int, default=[640, 640], help='the image-size(h,w) in inference size')
    
    # æ¨ç†å‚æ•°
    parser.add_argument('--conf-thres', type=float, default=0.4, help='confidence threshold for inference')
    parser.add_argument('--iou-thres', type=float, default=0.45, help='NMS IoU threshold for inference')
    parser.add_argument('--max-det', type=int, default=1000, help='maximal inferences per image')
    
    # è®¾å¤‡å‚æ•°
    parser.add_argument('--device', default='0', help='device to run our model i.e. 0 or 0,1,2,3 or cpu')
    parser.add_argument('--half', action='store_true', help='whether to use FP16 half-precision inference')
    
    # è¾“å‡ºå‚æ•°
    parser.add_argument('--save-txt', action='store_true', help='save results to *.txt')
    parser.add_argument('--not-save-img', action='store_true', help='do not save visuallized inference results')
    parser.add_argument('--save-dir', type=str, help='directory to save predictions in')
    parser.add_argument('--view-img', action='store_true', help='show inference results')
    
    # è¿‡æ»¤å‚æ•°
    parser.add_argument('--classes', nargs='+', type=int, help='filter by classes, e.g. --classes 0, or --classes 0 2 3')
    parser.add_argument('--agnostic-nms', action='store_true', help='class-agnostic NMS')
    
    # é¡¹ç›®å‚æ•°
    parser.add_argument('--project', default='runs/inference', help='save inference results to project/name')
    parser.add_argument('--name', default='exp', help='save inference results to project/name')
    parser.add_argument('--hide-labels', default=False, action='store_true', help='hide labels')
    parser.add_argument('--hide-conf', default=False, action='store_true', help='hide confidences')
    
    return parser


class AlignedInferer:
    """å®Œå…¨å¯¹é½PyTorchç‰ˆæœ¬çš„æ¨ç†å™¨"""
    
    def __init__(self, args):
        self.args = args
        
        # è®¾ç½®è®¾å¤‡
        if args.device == 'cpu':
            jt.flags.use_cuda = 0
        else:
            jt.flags.use_cuda = 1
            
        # å¤„ç†å›¾åƒå°ºå¯¸
        if len(args.img_size) == 1:
            self.img_size = [args.img_size[0], args.img_size[0]]
        else:
            self.img_size = args.img_size[:2]
            
        # åˆ›å»ºä¿å­˜ç›®å½•
        if args.save_dir:
            self.save_dir = args.save_dir
        else:
            self.save_dir = str(increment_name(osp.join(args.project, args.name)))
        os.makedirs(self.save_dir, exist_ok=True)
        
        # åŠ è½½ç±»åˆ«åç§°
        self.class_names = self.load_class_names()
        
        LOGGER.info(f'ğŸ¯ å¯¹é½æ¨ç†å™¨åˆå§‹åŒ–å®Œæˆ')
        LOGGER.info(f'   ä¿å­˜ç›®å½•: {self.save_dir}')
        LOGGER.info(f'   è®¾å¤‡: {"CUDA" if jt.flags.use_cuda else "CPU"}')
        LOGGER.info(f'   å›¾åƒå°ºå¯¸: {self.img_size}')
        
    def load_class_names(self):
        """åŠ è½½ç±»åˆ«åç§° - å¯¹é½PyTorchç‰ˆæœ¬"""
        if os.path.exists(self.args.yaml):
            with open(self.args.yaml, 'r') as f:
                data_cfg = yaml.safe_load(f)
                return data_cfg.get('names', [f'class_{i}' for i in range(80)])
        else:
            # COCOé»˜è®¤ç±»åˆ«
            return [
                'person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus', 'train', 'truck', 'boat', 'traffic light',
                'fire hydrant', 'stop sign', 'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep', 'cow',
                'elephant', 'bear', 'zebra', 'giraffe', 'backpack', 'umbrella', 'handbag', 'tie', 'suitcase', 'frisbee',
                'skis', 'snowboard', 'sports ball', 'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard',
                'tennis racket', 'bottle', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl', 'banana', 'apple',
                'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'chair', 'couch',
                'potted plant', 'bed', 'dining table', 'toilet', 'tv', 'laptop', 'mouse', 'remote', 'keyboard',
                'cell phone', 'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'book', 'clock', 'vase',
                'scissors', 'teddy bear', 'hair drier', 'toothbrush'
            ]
            
    def load_model(self):
        """åŠ è½½æ¨¡å‹ - å¯¹é½PyTorchç‰ˆæœ¬"""
        # ä»æƒé‡æ–‡ä»¶åæ¨æ–­æ¨¡å‹ç‰ˆæœ¬
        model_version = 's'  # é»˜è®¤
        if 'gold_yolo_n' in self.args.weights or 'gold-yolo-n' in self.args.weights:
            model_version = 'n'
        elif 'gold_yolo_m' in self.args.weights or 'gold-yolo-m' in self.args.weights:
            model_version = 'm'
        elif 'gold_yolo_l' in self.args.weights or 'gold-yolo-l' in self.args.weights:
            model_version = 'l'
            
        # åˆ›å»ºæ¨¡å‹
        self.model = create_gold_yolo(
            model_version,
            num_classes=len(self.class_names),
            use_pytorch_components=False
        )
        
        # åŠ è½½æƒé‡
        if os.path.exists(self.args.weights):
            checkpoint = jt.load(self.args.weights)
            if 'model_state_dict' in checkpoint:
                self.model.load_state_dict(checkpoint['model_state_dict'])
            else:
                self.model.load_state_dict(checkpoint)
            LOGGER.info(f'âœ… æƒé‡åŠ è½½å®Œæˆ: {self.args.weights}')
        else:
            LOGGER.warning(f'âš ï¸ æƒé‡æ–‡ä»¶ä¸å­˜åœ¨: {self.args.weights}ï¼Œä½¿ç”¨éšæœºåˆå§‹åŒ–æƒé‡')
            
        self.model.eval()
        
        # è®¡ç®—å‚æ•°é‡
        total_params = sum(p.numel() for p in self.model.parameters())
        LOGGER.info(f'âœ… æ¨¡å‹åŠ è½½å®Œæˆ: Gold-YOLO-{model_version.upper()}')
        LOGGER.info(f'   å‚æ•°é‡: {total_params:,} ({total_params/1e6:.2f}M)')
        LOGGER.info(f'   ç±»åˆ«æ•°: {len(self.class_names)}')
        
        return self.model
        
    def preprocess_image(self, img_path):
        """å›¾åƒé¢„å¤„ç† - å¯¹é½PyTorchç‰ˆæœ¬"""
        # è¯»å–å›¾åƒ
        img0 = cv2.imread(str(img_path))
        if img0 is None:
            raise ValueError(f'Cannot read image: {img_path}')
            
        # è®°å½•åŸå§‹å°ºå¯¸
        h0, w0 = img0.shape[:2]
        
        # Letterbox resize
        img, ratio, (dw, dh) = letterbox(img0, self.img_size, stride=32, auto=False)
        
        # è½¬æ¢æ ¼å¼
        img = img.transpose((2, 0, 1))[::-1]  # HWC to CHW, BGR to RGB
        img = np.ascontiguousarray(img)
        img = jt.array(img).float() / 255.0
        
        if len(img.shape) == 3:
            img = img.unsqueeze(0)
            
        return img, img0, (h0, w0), ratio, (dw, dh)
        
    def postprocess(self, predictions, img_shape, orig_shape, ratio, pad):
        """åå¤„ç† - å¯¹é½PyTorchç‰ˆæœ¬"""
        # NMS
        predictions = non_max_suppression(
            predictions,
            conf_thres=self.args.conf_thres,
            iou_thres=self.args.iou_thres,
            classes=self.args.classes,
            agnostic=self.args.agnostic_nms,
            max_det=self.args.max_det
        )
        
        # å¤„ç†æ¯ä¸ªå›¾åƒçš„é¢„æµ‹ç»“æœ
        results = []
        for i, pred in enumerate(predictions):
            if len(pred):
                # åæ ‡ç¼©æ”¾å›åŸå›¾
                pred[:, :4] = scale_coords(img_shape[2:], pred[:, :4], orig_shape).round()
                
                # è½¬æ¢ä¸ºåˆ—è¡¨æ ¼å¼
                for *xyxy, conf, cls in pred:
                    results.append({
                        'bbox': [float(x) for x in xyxy],
                        'confidence': float(conf),
                        'class': int(cls),
                        'class_name': self.class_names[int(cls)]
                    })
                    
        return results
        
    def draw_results(self, img, results):
        """ç»˜åˆ¶ç»“æœ - å¯¹é½PyTorchç‰ˆæœ¬"""
        for result in results:
            x1, y1, x2, y2 = [int(x) for x in result['bbox']]
            conf = result['confidence']
            cls = result['class']
            class_name = result['class_name']
            
            # ç»˜åˆ¶è¾¹ç•Œæ¡†
            color = (0, 255, 0)  # ç»¿è‰²
            cv2.rectangle(img, (x1, y1), (x2, y2), color, 2)
            
            # ç»˜åˆ¶æ ‡ç­¾
            if not self.args.hide_labels:
                label = f'{class_name}'
                if not self.args.hide_conf:
                    label += f' {conf:.2f}'
                    
                # è®¡ç®—æ–‡æœ¬å°ºå¯¸
                (text_width, text_height), _ = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 1)
                
                # ç»˜åˆ¶èƒŒæ™¯
                cv2.rectangle(img, (x1, y1 - text_height - 10), (x1 + text_width, y1), color, -1)
                
                # ç»˜åˆ¶æ–‡æœ¬
                cv2.putText(img, label, (x1, y1 - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)
                
        return img
        
    def save_results(self, results, img_path, save_txt=False):
        """ä¿å­˜ç»“æœ - å¯¹é½PyTorchç‰ˆæœ¬"""
        if save_txt:
            # ä¿å­˜ä¸ºtxtæ ¼å¼
            txt_path = osp.join(self.save_dir, Path(img_path).stem + '.txt')
            with open(txt_path, 'w') as f:
                for result in results:
                    x1, y1, x2, y2 = result['bbox']
                    conf = result['confidence']
                    cls = result['class']
                    f.write(f'{cls} {x1} {y1} {x2} {y2} {conf}\n')
                    
    def run_inference(self):
        """è¿è¡Œæ¨ç† - å¯¹é½PyTorchç‰ˆæœ¬"""
        LOGGER.info(f'ğŸš€ å¼€å§‹æ¨ç†...')
        
        # åŠ è½½æ¨¡å‹
        model = self.load_model()
        
        # è·å–è¾“å…¥æ–‡ä»¶åˆ—è¡¨
        source_path = Path(self.args.source)
        if source_path.is_file():
            img_paths = [source_path]
        elif source_path.is_dir():
            img_paths = list(source_path.glob('*.jpg')) + list(source_path.glob('*.png')) + list(source_path.glob('*.jpeg'))
        else:
            raise ValueError(f'Invalid source: {self.args.source}')
            
        LOGGER.info(f'   æ‰¾åˆ° {len(img_paths)} å¼ å›¾ç‰‡')
        
        # æ¨ç†å¾ªç¯
        start_time = time.time()
        total_results = []
        
        with jt.no_grad():
            for img_path in img_paths:
                # é¢„å¤„ç†
                img, img0, orig_shape, ratio, pad = self.preprocess_image(img_path)
                
                # æ¨ç†
                predictions = model(img)
                
                # åå¤„ç†
                results = self.postprocess(predictions, img.shape, orig_shape, ratio, pad)
                
                # ä¿å­˜ç»“æœ
                if not self.args.not_save_img:
                    # ç»˜åˆ¶ç»“æœ
                    img_with_results = self.draw_results(img0.copy(), results)
                    
                    # ä¿å­˜å›¾ç‰‡
                    save_path = osp.join(self.save_dir, Path(img_path).name)
                    cv2.imwrite(save_path, img_with_results)
                    
                if self.args.save_txt:
                    self.save_results(results, img_path, save_txt=True)
                    
                # æ˜¾ç¤ºç»“æœ
                if self.args.view_img:
                    cv2.imshow('Results', img_with_results)
                    cv2.waitKey(1)
                    
                total_results.append({
                    'image': str(img_path),
                    'detections': results
                })
                
                LOGGER.info(f'   {Path(img_path).name}: {len(results)} ä¸ªæ£€æµ‹ç»“æœ')
                
        inference_time = time.time() - start_time
        
        LOGGER.info(f'ğŸ‰ æ¨ç†å®Œæˆ!')
        LOGGER.info(f'   ç”¨æ—¶: {inference_time:.2f}s')
        LOGGER.info(f'   å¹³å‡: {inference_time/len(img_paths):.3f}s/å¼ ')
        LOGGER.info(f'   ç»“æœä¿å­˜è‡³: {self.save_dir}')
        
        return total_results


@jt.no_grad()
def main(args):
    """ä¸»æ¨ç†å‡½æ•° - å®Œå…¨å¯¹é½PyTorchç‰ˆæœ¬"""
    LOGGER.info(f'ğŸ¯ å¼€å§‹Gold-YOLO Jittoræ¨ç†')
    LOGGER.info(f'æ¨ç†å‚æ•°: {args}\n')
    
    # åˆ›å»ºæ¨ç†å™¨
    inferer = AlignedInferer(args)
    
    # è¿è¡Œæ¨ç†
    results = inferer.run_inference()
    
    return results


if __name__ == '__main__':
    args = get_args_parser().parse_args()
    main(args)
