#!/usr/bin/env python3
# -*- coding:utf-8 -*-
"""
å®Œå…¨å¯¹é½PyTorchç‰ˆæœ¬çš„Gold-YOLOè®­ç»ƒè„šæœ¬
ä¸¥æ ¼æŒ‰ç…§PyTorchç‰ˆæœ¬çš„å‚æ•°ã€é€»è¾‘å’Œæµç¨‹å®ç°
"""

import argparse
import os
import sys
import time
import yaml
import os.path as osp
from pathlib import Path
from copy import deepcopy

import jittor as jt
import jittor.nn as nn
import numpy as np
from tqdm import tqdm

# è®¾ç½®Jittorä¼˜åŒ–
jt.flags.use_cuda = 1
jt.flags.lazy_execution = 1

# æ·»åŠ é¡¹ç›®è·¯å¾„
ROOT = os.getcwd()
if str(ROOT) not in sys.path:
    sys.path.append(str(ROOT))

# å¯¼å…¥Gold-YOLOç»„ä»¶
from gold_yolo.models.gold_yolo import create_gold_yolo
from gold_yolo.data.coco_dataset import COCODataset
from gold_yolo.training.loss import ComputeLoss
from gold_yolo.training.optimizer import build_optimizer
from gold_yolo.training.scheduler import build_lr_scheduler
from gold_yolo.utils.general import increment_name, set_random_seed
from gold_yolo.utils.events import LOGGER


def get_args_parser(add_help=True):
    """å®Œå…¨å¯¹é½PyTorchç‰ˆæœ¬çš„å‚æ•°è§£æå™¨"""
    parser = argparse.ArgumentParser(description='Gold-YOLO Jittor Training', add_help=add_help)
    
    # æ•°æ®ç›¸å…³å‚æ•° - ä¸¥æ ¼å¯¹é½PyTorchç‰ˆæœ¬
    parser.add_argument('--data-path', default='./data/coco.yaml', type=str, help='path of dataset')
    parser.add_argument('--conf-file', default='./configs/gold_yolo-s.py', type=str, help='experiments description file')
    parser.add_argument('--img-size', default=640, type=int, help='train, val image size (pixels)')
    parser.add_argument('--batch-size', default=32, type=int, help='total batch size for all GPUs')
    parser.add_argument('--epochs', default=400, type=int, help='number of total epochs to run')
    parser.add_argument('--workers', default=8, type=int, help='number of data loading workers (default: 8)')
    
    # è®¾å¤‡ç›¸å…³å‚æ•°
    parser.add_argument('--device', default='0', type=str, help='cuda device, i.e. 0 or 0,1,2,3 or cpu')
    
    # è¯„ä¼°ç›¸å…³å‚æ•°
    parser.add_argument('--eval-interval', default=20, type=int, help='evaluate at every interval epochs')
    parser.add_argument('--eval-final-only', action='store_true', help='only evaluate at the final epoch')
    parser.add_argument('--heavy-eval-range', default=50, type=int,
                        help='evaluating every epoch for last such epochs')
    
    # æ•°æ®æ£€æŸ¥å‚æ•°
    parser.add_argument('--check-images', action='store_true', help='check images when initializing datasets')
    parser.add_argument('--check-labels', action='store_true', help='check label files when initializing datasets')
    
    # è¾“å‡ºç›¸å…³å‚æ•°
    parser.add_argument('--output-dir', default='./runs/train', type=str, help='path to save outputs')
    parser.add_argument('--name', default='exp', type=str, help='experiment name, saved to output_dir/name')
    
    # è®­ç»ƒæ§åˆ¶å‚æ•°
    parser.add_argument('--resume', nargs='?', const=True, default=False, help='resume the most recent training')
    parser.add_argument('--stop_aug_last_n_epoch', default=15, type=int, help='stop strong aug at last n epoch')
    parser.add_argument('--save_ckpt_on_last_n_epoch', default=-1, type=int, help='save last n epoch checkpoints')
    
    # ä¼˜åŒ–å™¨ç›¸å…³å‚æ•°
    parser.add_argument('--lr', default=0.01, type=float, help='initial learning rate')
    parser.add_argument('--momentum', default=0.937, type=float, help='SGD momentum')
    parser.add_argument('--weight-decay', default=0.0005, type=float, help='weight decay')
    parser.add_argument('--warmup-epochs', default=3, type=int, help='warmup epochs')
    
    return parser


class AlignedTrainer:
    """å®Œå…¨å¯¹é½PyTorchç‰ˆæœ¬çš„è®­ç»ƒå™¨"""
    
    def __init__(self, args, cfg, device):
        self.args = args
        self.cfg = cfg
        self.device = device
        
        # è®­ç»ƒçŠ¶æ€
        self.epoch = 0
        self.best_fitness = 0.0
        self.start_epoch = 0
        
        # åˆ›å»ºä¿å­˜ç›®å½•
        self.save_dir = str(increment_name(osp.join(args.output_dir, args.name)))
        os.makedirs(self.save_dir, exist_ok=True)
        
        LOGGER.info(f'ğŸ¯ å¯¹é½è®­ç»ƒå™¨åˆå§‹åŒ–å®Œæˆ')
        LOGGER.info(f'   ä¿å­˜ç›®å½•: {self.save_dir}')
        LOGGER.info(f'   è®¾å¤‡: {self.device}')
        
    def setup_model(self):
        """è®¾ç½®æ¨¡å‹ - ä¸¥æ ¼å¯¹é½PyTorchç‰ˆæœ¬"""
        # ä»é…ç½®æ–‡ä»¶è§£ææ¨¡å‹ç‰ˆæœ¬
        model_version = 's'  # é»˜è®¤ä½¿ç”¨Sç‰ˆæœ¬
        if 'gold_yolo-n' in self.args.conf_file:
            model_version = 'n'
        elif 'gold_yolo-m' in self.args.conf_file:
            model_version = 'm'
        elif 'gold_yolo-l' in self.args.conf_file:
            model_version = 'l'
            
        # åˆ›å»ºæ¨¡å‹
        self.model = create_gold_yolo(
            model_version, 
            num_classes=80,  # COCOæ•°æ®é›†
            use_pytorch_components=False
        )
        
        # è®¡ç®—å‚æ•°é‡
        total_params = sum(p.numel() for p in self.model.parameters())
        LOGGER.info(f'âœ… æ¨¡å‹åˆ›å»ºå®Œæˆ: Gold-YOLO-{model_version.upper()}')
        LOGGER.info(f'   å‚æ•°é‡: {total_params:,} ({total_params/1e6:.2f}M)')
        
        return self.model
        
    def setup_data(self):
        """è®¾ç½®æ•°æ®åŠ è½½å™¨ - å¯¹é½PyTorchç‰ˆæœ¬"""
        # åŠ è½½æ•°æ®é…ç½®
        with open(self.args.data_path, 'r') as f:
            data_cfg = yaml.safe_load(f)
            
        # åˆ›å»ºè®­ç»ƒæ•°æ®é›†
        train_dataset = COCODataset(
            img_dir=data_cfg['train'],
            ann_file=data_cfg.get('train_ann', None),
            img_size=self.args.img_size,
            augment=True,
            cache=False
        )
        
        # åˆ›å»ºéªŒè¯æ•°æ®é›†
        val_dataset = COCODataset(
            img_dir=data_cfg['val'],
            ann_file=data_cfg.get('val_ann', None),
            img_size=self.args.img_size,
            augment=False,
            cache=False
        )
        
        # åˆ›å»ºæ•°æ®åŠ è½½å™¨
        self.train_loader = train_dataset.set_attrs(
            batch_size=self.args.batch_size,
            shuffle=True,
            num_workers=self.args.workers,
            drop_last=True
        )
        
        self.val_loader = val_dataset.set_attrs(
            batch_size=self.args.batch_size,
            shuffle=False,
            num_workers=self.args.workers,
            drop_last=False
        )
        
        LOGGER.info(f'âœ… æ•°æ®åŠ è½½å™¨åˆ›å»ºå®Œæˆ')
        LOGGER.info(f'   è®­ç»ƒé›†: {len(train_dataset)} å¼ å›¾ç‰‡')
        LOGGER.info(f'   éªŒè¯é›†: {len(val_dataset)} å¼ å›¾ç‰‡')
        LOGGER.info(f'   æ‰¹æ¬¡å¤§å°: {self.args.batch_size}')
        
        return self.train_loader, self.val_loader
        
    def setup_loss(self):
        """è®¾ç½®æŸå¤±å‡½æ•° - å¯¹é½PyTorchç‰ˆæœ¬"""
        self.compute_loss = ComputeLoss(
            num_classes=80,
            ori_img_size=self.args.img_size,
            warmup_epoch=self.args.warmup_epochs,
            use_dfl=True,
            reg_max=16,
            iou_type='giou',
            loss_weight={
                'class': 1.0,
                'iou': 2.5,
                'dfl': 0.5
            }
        )
        
        LOGGER.info(f'âœ… æŸå¤±å‡½æ•°åˆ›å»ºå®Œæˆ: ComputeLoss')
        return self.compute_loss
        
    def setup_optimizer(self):
        """è®¾ç½®ä¼˜åŒ–å™¨å’Œå­¦ä¹ ç‡è°ƒåº¦å™¨ - å¯¹é½PyTorchç‰ˆæœ¬"""
        # åˆ›å»ºä¼˜åŒ–å™¨
        self.optimizer = build_optimizer(
            model=self.model,
            name='SGD',
            lr=self.args.lr,
            momentum=self.args.momentum,
            weight_decay=self.args.weight_decay
        )
        
        # åˆ›å»ºå­¦ä¹ ç‡è°ƒåº¦å™¨
        self.scheduler = build_lr_scheduler(
            optimizer=self.optimizer,
            name='cosine',
            epochs=self.args.epochs,
            warmup_epochs=self.args.warmup_epochs,
            warmup_momentum=0.8,
            warmup_bias_lr=0.1
        )
        
        LOGGER.info(f'âœ… ä¼˜åŒ–å™¨åˆ›å»ºå®Œæˆ: SGD')
        LOGGER.info(f'   å­¦ä¹ ç‡: {self.args.lr}')
        LOGGER.info(f'   åŠ¨é‡: {self.args.momentum}')
        LOGGER.info(f'   æƒé‡è¡°å‡: {self.args.weight_decay}')
        
        return self.optimizer, self.scheduler


def check_and_init(args):
    """æ£€æŸ¥å’Œåˆå§‹åŒ– - å¯¹é½PyTorchç‰ˆæœ¬"""
    # è®¾ç½®éšæœºç§å­
    set_random_seed(1, deterministic=True)
    
    # åˆ›å»ºä¿å­˜ç›®å½•
    if not args.resume:
        args.save_dir = str(increment_name(osp.join(args.output_dir, args.name)))
        os.makedirs(args.save_dir, exist_ok=True)
    
    # åŠ è½½é…ç½®æ–‡ä»¶
    if args.conf_file.endswith('.py'):
        # Pythoné…ç½®æ–‡ä»¶
        import importlib.util
        spec = importlib.util.spec_from_file_location("config", args.conf_file)
        cfg = importlib.util.module_from_spec(spec)
        spec.loader.exec_module(cfg)
    else:
        # YAMLé…ç½®æ–‡ä»¶
        with open(args.conf_file, 'r') as f:
            cfg = yaml.safe_load(f)
    
    # è®¾ç½®è®¾å¤‡
    if args.device == 'cpu':
        jt.flags.use_cuda = 0
    else:
        jt.flags.use_cuda = 1
        
    LOGGER.info(f'âœ… åˆå§‹åŒ–å®Œæˆ')
    LOGGER.info(f'   é…ç½®æ–‡ä»¶: {args.conf_file}')
    LOGGER.info(f'   è®¾å¤‡: {"CUDA" if jt.flags.use_cuda else "CPU"}')
    
    return cfg, args


def main(args):
    """ä¸»è®­ç»ƒå‡½æ•° - å®Œå…¨å¯¹é½PyTorchç‰ˆæœ¬"""
    LOGGER.info(f'ğŸ¯ å¼€å§‹Gold-YOLO Jittorè®­ç»ƒ')
    LOGGER.info(f'è®­ç»ƒå‚æ•°: {args}\n')
    
    # åˆå§‹åŒ–
    cfg, args = check_and_init(args)
    
    # åˆ›å»ºè®­ç»ƒå™¨
    trainer = AlignedTrainer(args, cfg, 'cuda' if jt.flags.use_cuda else 'cpu')
    
    # è®¾ç½®ç»„ä»¶
    model = trainer.setup_model()
    train_loader, val_loader = trainer.setup_data()
    compute_loss = trainer.setup_loss()
    optimizer, scheduler = trainer.setup_optimizer()
    
    LOGGER.info(f'ğŸš€ å¼€å§‹è®­ç»ƒ...')
    LOGGER.info(f'   æ€»è½®æ•°: {args.epochs}')
    LOGGER.info(f'   è¯„ä¼°é—´éš”: {args.eval_interval}')
    
    # å¼€å§‹è®­ç»ƒå¾ªç¯
    start_time = time.time()

    for epoch in range(args.epochs):
        trainer.epoch = epoch

        # è®­ç»ƒä¸€ä¸ªepoch
        train_loss = train_one_epoch(
            model, train_loader, compute_loss, optimizer, scheduler, epoch, args
        )

        # è¯„ä¼°
        if epoch % args.eval_interval == 0 or epoch == args.epochs - 1:
            val_loss = validate(model, val_loader, compute_loss, epoch, args)

            # ä¿å­˜æ£€æŸ¥ç‚¹
            save_checkpoint(
                model, optimizer, scheduler, epoch, train_loss, val_loss,
                osp.join(trainer.save_dir, f'epoch_{epoch}.pt')
            )

        LOGGER.info(f'Epoch {epoch}/{args.epochs} - Train Loss: {train_loss:.4f}')

    total_time = time.time() - start_time
    LOGGER.info(f'ğŸ‰ è®­ç»ƒå®Œæˆ! æ€»ç”¨æ—¶: {total_time:.2f}s')


def train_one_epoch(model, dataloader, compute_loss, optimizer, scheduler, epoch, args):
    """è®­ç»ƒä¸€ä¸ªepoch - å¯¹é½PyTorchç‰ˆæœ¬"""
    model.train()
    total_loss = 0.0
    num_batches = len(dataloader)

    pbar = tqdm(enumerate(dataloader), total=num_batches, desc=f'Epoch {epoch}')

    for batch_idx, (images, targets) in pbar:
        # å‰å‘ä¼ æ’­
        predictions = model(images)

        # è®¡ç®—æŸå¤±
        loss, loss_items = compute_loss(predictions, targets)

        # åå‘ä¼ æ’­
        optimizer.zero_grad()
        optimizer.backward(loss)
        optimizer.step()

        # æ›´æ–°å­¦ä¹ ç‡
        scheduler.step()

        # ç»Ÿè®¡
        total_loss += loss.item()
        avg_loss = total_loss / (batch_idx + 1)

        # æ›´æ–°è¿›åº¦æ¡
        pbar.set_postfix({
            'loss': f'{avg_loss:.4f}',
            'lr': f'{scheduler.get_lr()[0]:.6f}'
        })

    return total_loss / num_batches


def validate(model, dataloader, compute_loss, epoch, args):
    """éªŒè¯ - å¯¹é½PyTorchç‰ˆæœ¬"""
    model.eval()
    total_loss = 0.0
    num_batches = len(dataloader)

    with jt.no_grad():
        for batch_idx, (images, targets) in enumerate(dataloader):
            # å‰å‘ä¼ æ’­
            predictions = model(images)

            # è®¡ç®—æŸå¤±
            loss, loss_items = compute_loss(predictions, targets)
            total_loss += loss.item()

    return total_loss / num_batches


def save_checkpoint(model, optimizer, scheduler, epoch, train_loss, val_loss, path):
    """ä¿å­˜æ£€æŸ¥ç‚¹ - å¯¹é½PyTorchç‰ˆæœ¬"""
    checkpoint = {
        'epoch': epoch,
        'model_state_dict': model.state_dict(),
        'optimizer_state_dict': optimizer.state_dict(),
        'scheduler_state_dict': scheduler.state_dict(),
        'train_loss': train_loss,
        'val_loss': val_loss,
    }
    jt.save(checkpoint, path)
    LOGGER.info(f'âœ… æ£€æŸ¥ç‚¹å·²ä¿å­˜: {path}')


if __name__ == '__main__':
    args = get_args_parser().parse_args()
    main(args)
