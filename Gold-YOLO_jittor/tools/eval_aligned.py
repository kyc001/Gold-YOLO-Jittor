#!/usr/bin/env python3
# -*- coding:utf-8 -*-
"""
å®Œå…¨å¯¹é½PyTorchç‰ˆæœ¬çš„Gold-YOLOè¯„ä¼°è„šæœ¬
ä¸¥æ ¼æŒ‰ç…§PyTorchç‰ˆæœ¬çš„è¯„ä¼°é€»è¾‘å’ŒæŒ‡æ ‡è®¡ç®—
"""

import argparse
import os
import sys
import time
import yaml
import json
import os.path as osp
from pathlib import Path

import jittor as jt
import jittor.nn as nn
import numpy as np
from tqdm import tqdm

# è®¾ç½®Jittor
jt.flags.use_cuda = 1

# æ·»åŠ é¡¹ç›®è·¯å¾„
ROOT = os.getcwd()
if str(ROOT) not in sys.path:
    sys.path.append(str(ROOT))

# å¯¼å…¥Gold-YOLOç»„ä»¶
from gold_yolo.models.gold_yolo import create_gold_yolo
from gold_yolo.data.coco_dataset import COCODataset
from gold_yolo.utils.general import increment_name
from gold_yolo.utils.events import LOGGER
from gold_yolo.utils.metrics import COCOEvaluator


def boolean_string(s):
    """å¸ƒå°”å­—ç¬¦ä¸²è½¬æ¢ - å¯¹é½PyTorchç‰ˆæœ¬"""
    if s not in {'False', 'True'}:
        raise ValueError('Not a valid boolean string')
    return s == 'True'


def get_args_parser(add_help=True):
    """å®Œå…¨å¯¹é½PyTorchç‰ˆæœ¬çš„è¯„ä¼°å‚æ•°è§£æå™¨"""
    parser = argparse.ArgumentParser(description='Gold-YOLO Jittor Evaluation', add_help=add_help)
    
    # æ•°æ®å’Œæ¨¡å‹å‚æ•°
    parser.add_argument('--data', type=str, default='./data/coco.yaml', help='dataset.yaml path')
    parser.add_argument('--weights', type=str, default='./weights/gold_yolo_s.pt', help='model.pt path(s)')
    parser.add_argument('--batch-size', type=int, default=32, help='batch size')
    parser.add_argument('--img-size', type=int, default=640, help='inference size (pixels)')
    
    # æ¨ç†å‚æ•°
    parser.add_argument('--conf-thres', type=float, default=0.03, help='confidence threshold')
    parser.add_argument('--iou-thres', type=float, default=0.65, help='NMS IoU threshold')
    parser.add_argument('--max-det', type=int, default=300, help='maximum detections per image')
    
    # ä»»åŠ¡å’Œè®¾å¤‡
    parser.add_argument('--task', default='val', help='val, test, or speed')
    parser.add_argument('--device', default='0', help='cuda device, i.e. 0 or 0,1,2,3 or cpu')
    parser.add_argument('--half', default=False, action='store_true', help='whether to use fp16 infer')
    
    # è¾“å‡ºå‚æ•°
    parser.add_argument('--save_dir', type=str, default='runs/val/', help='evaluation save dir')
    parser.add_argument('--name', type=str, default='exp', help='save evaluation results to save_dir/name')
    
    # è¯„ä¼°æ§åˆ¶å‚æ•°
    parser.add_argument('--test_load_size', type=int, default=640, help='load img resize when test')
    parser.add_argument('--letterbox_return_int', default=False, action='store_true', help='return int offset for letterbox')
    parser.add_argument('--scale_exact', default=False, action='store_true', help='use exact scale size to scale coords')
    parser.add_argument('--force_no_pad', default=False, action='store_true', help='for no extra pad in letterbox')
    parser.add_argument('--not_infer_on_rect', default=False, action='store_true', help='default to use rect image size to boost infer')
    
    # æŒ‡æ ‡è®¡ç®—å‚æ•°
    parser.add_argument('--do_coco_metric', default=True, type=boolean_string, help='whether to use pycocotool to metric')
    parser.add_argument('--do_pr_metric', default=False, type=boolean_string, help='whether to calculate precision, recall and F1')
    parser.add_argument('--plot_curve', default=True, type=boolean_string, help='whether to save plots in savedir')
    parser.add_argument('--plot_confusion_matrix', default=False, action='store_true', help='whether to save confusion matrix plots')
    parser.add_argument('--verbose', default=False, action='store_true', help='whether to print metric on each class')
    
    # é…ç½®æ–‡ä»¶
    parser.add_argument('--config-file', default='', type=str, help='experiments description file')
    
    return parser


class AlignedEvaler:
    """å®Œå…¨å¯¹é½PyTorchç‰ˆæœ¬çš„è¯„ä¼°å™¨"""
    
    def __init__(self, args):
        self.args = args
        
        # è®¾ç½®è®¾å¤‡
        if args.device == 'cpu':
            jt.flags.use_cuda = 0
        else:
            jt.flags.use_cuda = 1
            
        # åˆ›å»ºä¿å­˜ç›®å½•
        self.save_dir = str(increment_name(osp.join(args.save_dir, args.name)))
        os.makedirs(self.save_dir, exist_ok=True)
        
        LOGGER.info(f'ğŸ¯ å¯¹é½è¯„ä¼°å™¨åˆå§‹åŒ–å®Œæˆ')
        LOGGER.info(f'   ä¿å­˜ç›®å½•: {self.save_dir}')
        LOGGER.info(f'   è®¾å¤‡: {"CUDA" if jt.flags.use_cuda else "CPU"}')
        
    def load_model(self):
        """åŠ è½½æ¨¡å‹ - å¯¹é½PyTorchç‰ˆæœ¬"""
        # ä»æƒé‡æ–‡ä»¶åæ¨æ–­æ¨¡å‹ç‰ˆæœ¬
        model_version = 's'  # é»˜è®¤
        if 'gold_yolo_n' in self.args.weights or 'gold-yolo-n' in self.args.weights:
            model_version = 'n'
        elif 'gold_yolo_m' in self.args.weights or 'gold-yolo-m' in self.args.weights:
            model_version = 'm'
        elif 'gold_yolo_l' in self.args.weights or 'gold-yolo-l' in self.args.weights:
            model_version = 'l'
            
        # åˆ›å»ºæ¨¡å‹
        self.model = create_gold_yolo(
            model_version,
            num_classes=80,  # COCOæ•°æ®é›†
            use_pytorch_components=False
        )
        
        # åŠ è½½æƒé‡
        if os.path.exists(self.args.weights):
            checkpoint = jt.load(self.args.weights)
            if 'model_state_dict' in checkpoint:
                self.model.load_state_dict(checkpoint['model_state_dict'])
            else:
                self.model.load_state_dict(checkpoint)
            LOGGER.info(f'âœ… æƒé‡åŠ è½½å®Œæˆ: {self.args.weights}')
        else:
            LOGGER.warning(f'âš ï¸ æƒé‡æ–‡ä»¶ä¸å­˜åœ¨: {self.args.weights}ï¼Œä½¿ç”¨éšæœºåˆå§‹åŒ–æƒé‡')
            
        self.model.eval()
        
        # è®¡ç®—å‚æ•°é‡
        total_params = sum(p.numel() for p in self.model.parameters())
        LOGGER.info(f'âœ… æ¨¡å‹åŠ è½½å®Œæˆ: Gold-YOLO-{model_version.upper()}')
        LOGGER.info(f'   å‚æ•°é‡: {total_params:,} ({total_params/1e6:.2f}M)')
        
        return self.model
        
    def setup_data(self):
        """è®¾ç½®æ•°æ®åŠ è½½å™¨ - å¯¹é½PyTorchç‰ˆæœ¬"""
        # åŠ è½½æ•°æ®é…ç½®
        with open(self.args.data, 'r') as f:
            data_cfg = yaml.safe_load(f)
            
        # æ ¹æ®ä»»åŠ¡é€‰æ‹©æ•°æ®é›†
        if self.args.task == 'val':
            img_dir = data_cfg['val']
            ann_file = data_cfg.get('val_ann', None)
        elif self.args.task == 'test':
            img_dir = data_cfg.get('test', data_cfg['val'])
            ann_file = data_cfg.get('test_ann', data_cfg.get('val_ann', None))
        else:
            raise ValueError(f'Unsupported task: {self.args.task}')
            
        # åˆ›å»ºæ•°æ®é›†
        dataset = COCODataset(
            img_dir=img_dir,
            ann_file=ann_file,
            img_size=self.args.img_size,
            augment=False,
            cache=False,
            rect=not self.args.not_infer_on_rect,
            stride=32
        )
        
        # åˆ›å»ºæ•°æ®åŠ è½½å™¨
        self.dataloader = dataset.set_attrs(
            batch_size=self.args.batch_size,
            shuffle=False,
            num_workers=8,
            drop_last=False
        )
        
        LOGGER.info(f'âœ… æ•°æ®åŠ è½½å™¨åˆ›å»ºå®Œæˆ')
        LOGGER.info(f'   æ•°æ®é›†: {len(dataset)} å¼ å›¾ç‰‡')
        LOGGER.info(f'   æ‰¹æ¬¡å¤§å°: {self.args.batch_size}')
        
        return self.dataloader, dataset
        
    def setup_evaluator(self):
        """è®¾ç½®è¯„ä¼°å™¨ - å¯¹é½PyTorchç‰ˆæœ¬"""
        self.evaluator = COCOEvaluator(
            save_dir=self.save_dir,
            conf_thres=self.args.conf_thres,
            iou_thres=self.args.iou_thres,
            max_det=self.args.max_det,
            do_coco_metric=self.args.do_coco_metric,
            do_pr_metric=self.args.do_pr_metric,
            plot_curve=self.args.plot_curve,
            plot_confusion_matrix=self.args.plot_confusion_matrix,
            verbose=self.args.verbose
        )
        
        LOGGER.info(f'âœ… è¯„ä¼°å™¨åˆ›å»ºå®Œæˆ')
        LOGGER.info(f'   ç½®ä¿¡åº¦é˜ˆå€¼: {self.args.conf_thres}')
        LOGGER.info(f'   NMS IoUé˜ˆå€¼: {self.args.iou_thres}')
        LOGGER.info(f'   æœ€å¤§æ£€æµ‹æ•°: {self.args.max_det}')
        
        return self.evaluator
        
    def run_evaluation(self):
        """è¿è¡Œè¯„ä¼° - å¯¹é½PyTorchç‰ˆæœ¬"""
        LOGGER.info(f'ğŸš€ å¼€å§‹è¯„ä¼°...')
        
        # åŠ è½½ç»„ä»¶
        model = self.load_model()
        dataloader, dataset = self.setup_data()
        evaluator = self.setup_evaluator()
        
        # è¯„ä¼°å¾ªç¯
        start_time = time.time()
        all_predictions = []
        all_targets = []
        
        with jt.no_grad():
            pbar = tqdm(enumerate(dataloader), total=len(dataloader), desc='Evaluating')
            
            for batch_idx, (images, targets, paths, shapes) in pbar:
                # å‰å‘ä¼ æ’­
                predictions = model(images)
                
                # åå¤„ç†
                processed_preds = evaluator.postprocess(
                    predictions, images.shape, shapes
                )
                
                # æ”¶é›†ç»“æœ
                all_predictions.extend(processed_preds)
                all_targets.extend(targets)
                
                # æ›´æ–°è¿›åº¦æ¡
                pbar.set_postfix({
                    'batch': f'{batch_idx+1}/{len(dataloader)}'
                })
                
        eval_time = time.time() - start_time
        
        # è®¡ç®—æŒ‡æ ‡
        LOGGER.info(f'ğŸ“Š è®¡ç®—è¯„ä¼°æŒ‡æ ‡...')
        metrics = evaluator.compute_metrics(all_predictions, all_targets)
        
        # ä¿å­˜ç»“æœ
        results_file = osp.join(self.save_dir, 'results.json')
        with open(results_file, 'w') as f:
            json.dump(metrics, f, indent=2)
            
        LOGGER.info(f'ğŸ‰ è¯„ä¼°å®Œæˆ!')
        LOGGER.info(f'   ç”¨æ—¶: {eval_time:.2f}s')
        LOGGER.info(f'   ç»“æœä¿å­˜è‡³: {results_file}')
        
        # æ‰“å°ä¸»è¦æŒ‡æ ‡
        if 'mAP_0.5:0.95' in metrics:
            LOGGER.info(f'   mAP@0.5:0.95: {metrics["mAP_0.5:0.95"]:.4f}')
        if 'mAP_0.5' in metrics:
            LOGGER.info(f'   mAP@0.5: {metrics["mAP_0.5"]:.4f}')
            
        return metrics


def main(args):
    """ä¸»è¯„ä¼°å‡½æ•° - å®Œå…¨å¯¹é½PyTorchç‰ˆæœ¬"""
    LOGGER.info(f'ğŸ¯ å¼€å§‹Gold-YOLO Jittorè¯„ä¼°')
    LOGGER.info(f'è¯„ä¼°å‚æ•°: {args}\n')
    
    # åˆ›å»ºè¯„ä¼°å™¨
    evaler = AlignedEvaler(args)
    
    # è¿è¡Œè¯„ä¼°
    metrics = evaler.run_evaluation()
    
    return metrics


if __name__ == '__main__':
    args = get_args_parser().parse_args()
    main(args)
