#!/usr/bin/env python3
# -*- coding:utf-8 -*-
"""
æ·±å…¥è¯Šæ–­JittoræŸå¤±å‡½æ•°é—®é¢˜
æ–°èŠ½ç¬¬äºŒé˜¶æ®µï¼šå½»åº•è§£å†³æŸå¤±æ•°å€¼å¼‚å¸¸å°çš„é—®é¢˜
"""

import jittor as jt
import jittor.nn as nn
import numpy as np
import matplotlib.pyplot as plt
from pathlib import Path

# è®¾ç½®Jittor
jt.flags.use_cuda = 1

class ProblemLossFunction(nn.Module):
    """æœ‰é—®é¢˜çš„æŸå¤±å‡½æ•° - é‡ç°å¼‚å¸¸"""
    
    def __init__(self):
        super().__init__()
        self.mse_loss = nn.MSELoss()
        self.bce_loss = nn.BCEWithLogitsLoss()
    
    def execute(self, pred, targets=None):
        multi_feats, cls_pred, reg_pred = pred
        
        batch_size = cls_pred.shape[0]
        num_anchors = cls_pred.shape[1]
        num_classes = cls_pred.shape[2]
        reg_dim = reg_pred.shape[2]
        
        # é—®é¢˜1: äººå·¥ç›®æ ‡è¿‡äºç®€å•
        cls_targets = jt.zeros_like(cls_pred)
        reg_targets = jt.zeros_like(reg_pred)
        
        for b in range(batch_size):
            num_pos = min(10, num_anchors)
            for i in range(num_pos):
                cls_id = i % num_classes
                cls_targets[b, i, cls_id] = 1.0
                
                # é—®é¢˜2: å›å½’ç›®æ ‡è¿‡å°
                for j in range(min(4, reg_dim)):
                    reg_targets[b, i, j] = 0.1 + 0.05 * jt.randn(1)
        
        # é—®é¢˜3: BCEå¯¹ç¨€ç–ç›®æ ‡è®¡ç®—æœ‰é—®é¢˜
        reg_loss = self.mse_loss(reg_pred, reg_targets)
        cls_loss = self.bce_loss(cls_pred, cls_targets)
        
        # é—®é¢˜4: æ¢¯åº¦å¼ºåˆ¶é¡¹æƒé‡è¿‡å°
        reg_gradient_force = jt.mean(reg_pred ** 2) * 0.1
        
        total_loss = reg_loss + cls_loss + reg_gradient_force
        
        return total_loss, reg_loss, cls_loss, reg_gradient_force


class FixedLossFunction(nn.Module):
    """ä¿®å¤åçš„æŸå¤±å‡½æ•°"""
    
    def __init__(self):
        super().__init__()
        self.mse_loss = nn.MSELoss()
        self.bce_loss = nn.BCEWithLogitsLoss()
        
        # ä¿®å¤1: åˆç†çš„æŸå¤±æƒé‡
        self.lambda_coord = 5.0
        self.lambda_cls = 1.0
        self.lambda_obj = 1.0
        self.lambda_reg = 0.5
    
    def execute(self, pred, targets=None):
        multi_feats, cls_pred, reg_pred = pred
        
        batch_size = cls_pred.shape[0]
        num_anchors = cls_pred.shape[1]
        num_classes = cls_pred.shape[2]
        reg_dim = reg_pred.shape[2]
        
        # ä¿®å¤2: æ›´çœŸå®çš„ç›®æ ‡åˆ†å¸ƒ
        cls_targets = jt.zeros_like(cls_pred)
        reg_targets = jt.zeros_like(reg_pred)
        obj_mask = jt.zeros((batch_size, num_anchors))
        
        for b in range(batch_size):
            # ä¿®å¤3: æ›´åˆç†çš„æ­£æ ·æœ¬æ•°é‡
            num_pos = np.random.randint(3, min(15, num_anchors//5))
            pos_indices = np.random.choice(num_anchors, num_pos, replace=False)
            
            for idx in pos_indices:
                obj_mask[b, idx] = 1.0
                
                # éšæœºç±»åˆ«
                cls_id = np.random.randint(0, num_classes)
                cls_targets[b, idx, cls_id] = 1.0
                
                # ä¿®å¤4: æ›´çœŸå®çš„å›å½’ç›®æ ‡
                reg_targets[b, idx, 0] = np.random.uniform(0.2, 0.8)  # x
                reg_targets[b, idx, 1] = np.random.uniform(0.2, 0.8)  # y
                reg_targets[b, idx, 2] = np.random.uniform(0.1, 0.6)  # w
                reg_targets[b, idx, 3] = np.random.uniform(0.1, 0.6)  # h
                
                # DFLåˆ†å¸ƒ
                for j in range(4, min(20, reg_dim)):
                    reg_targets[b, idx, j] = np.random.uniform(0.0, 0.5)
        
        # ä¿®å¤5: åªå¯¹æœ‰ç›®æ ‡çš„ä½ç½®è®¡ç®—æŸå¤±
        pos_mask_cls = obj_mask.unsqueeze(-1).expand_as(cls_pred)
        pos_mask_reg = obj_mask.unsqueeze(-1).expand_as(reg_pred)
        
        # åˆ†ç±»æŸå¤± - åªå¯¹æ­£æ ·æœ¬
        cls_loss = self.bce_loss(cls_pred * pos_mask_cls, cls_targets * pos_mask_cls)
        
        # å›å½’æŸå¤± - åªå¯¹æ­£æ ·æœ¬
        reg_loss = self.mse_loss(reg_pred * pos_mask_reg, reg_targets * pos_mask_reg)
        
        # ç›®æ ‡æ€§æŸå¤±
        obj_pred = jt.max(cls_pred, dim=-1)  # [batch, anchors]
        if isinstance(obj_pred, tuple):
            obj_pred = obj_pred[0]
        obj_loss = self.bce_loss(obj_pred, obj_mask)
        
        # æ— ç›®æ ‡æŸå¤±
        noobj_mask = 1.0 - obj_mask
        noobj_loss = self.bce_loss(obj_pred * noobj_mask, jt.zeros_like(obj_pred) * noobj_mask)
        
        # ä¿®å¤6: åˆç†çš„æƒé‡ç»„åˆ
        total_loss = (self.lambda_coord * reg_loss + 
                     self.lambda_cls * cls_loss + 
                     self.lambda_obj * obj_loss + 
                     self.lambda_reg * noobj_loss)
        
        return total_loss, reg_loss, cls_loss, obj_loss, noobj_loss


def create_test_model():
    """åˆ›å»ºæµ‹è¯•æ¨¡å‹"""
    class SimpleTestModel(nn.Module):
        def __init__(self):
            super().__init__()
            self.backbone = nn.Sequential(
                nn.Conv2d(3, 32, 3, 2, 1),
                nn.BatchNorm2d(32),
                nn.SiLU(),
                nn.AdaptiveAvgPool2d(1)
            )
            self.cls_head = nn.Linear(32, 525 * 80)
            self.reg_head = nn.Linear(32, 525 * 68)
        
        def execute(self, x):
            feat = self.backbone(x)
            feat = feat.view(x.size(0), -1)
            
            cls_pred = self.cls_head(feat).view(x.size(0), 525, 80)
            reg_pred = self.reg_head(feat).view(x.size(0), 525, 68)
            
            return feat, cls_pred, reg_pred
    
    return SimpleTestModel()


def diagnose_loss_behavior():
    """è¯Šæ–­æŸå¤±å‡½æ•°è¡Œä¸º"""
    print("ğŸ” æ·±å…¥è¯Šæ–­æŸå¤±å‡½æ•°é—®é¢˜...")
    print("=" * 60)
    
    # åˆ›å»ºæµ‹è¯•æ¨¡å‹
    model = create_test_model()
    problem_loss = ProblemLossFunction()
    fixed_loss = FixedLossFunction()
    
    # æµ‹è¯•æ•°æ®
    batch_size = 4
    test_input = jt.randn(batch_size, 3, 640, 640)
    
    print(f"ğŸ“Š æµ‹è¯•é…ç½®:")
    print(f"   æ‰¹æ¬¡å¤§å°: {batch_size}")
    print(f"   è¾“å…¥å°ºå¯¸: {test_input.shape}")
    
    # è®°å½•æŸå¤±å˜åŒ–
    problem_losses = []
    fixed_losses = []
    
    print("\nğŸ§ª æŸå¤±å‡½æ•°å¯¹æ¯”æµ‹è¯•:")
    print("=" * 60)
    
    for step in range(20):
        # å‰å‘ä¼ æ’­
        with jt.no_grad():
            outputs = model(test_input)
        
        # é—®é¢˜æŸå¤±å‡½æ•°
        prob_total, prob_reg, prob_cls, prob_force = problem_loss(outputs)
        problem_losses.append(prob_total.item())
        
        # ä¿®å¤æŸå¤±å‡½æ•°
        fixed_total, fixed_reg, fixed_cls, fixed_obj, fixed_noobj = fixed_loss(outputs)
        fixed_losses.append(fixed_total.item())
        
        if step % 5 == 0:
            print(f"Step {step:2d}:")
            print(f"  é—®é¢˜ç‰ˆæœ¬: total={prob_total.item():.4f}, reg={prob_reg.item():.4f}, cls={prob_cls.item():.4f}")
            print(f"  ä¿®å¤ç‰ˆæœ¬: total={fixed_total.item():.4f}, reg={fixed_reg.item():.4f}, cls={fixed_cls.item():.4f}")
            print()
    
    # åˆ†æç»“æœ
    print("ğŸ“ˆ æŸå¤±åˆ†æç»“æœ:")
    print("=" * 60)
    print(f"é—®é¢˜ç‰ˆæœ¬:")
    print(f"  åˆå§‹æŸå¤±: {problem_losses[0]:.4f}")
    print(f"  æœ€ç»ˆæŸå¤±: {problem_losses[-1]:.4f}")
    print(f"  æŸå¤±èŒƒå›´: {min(problem_losses):.4f} - {max(problem_losses):.4f}")
    print(f"  å¹³å‡æŸå¤±: {np.mean(problem_losses):.4f}")
    
    print(f"\nä¿®å¤ç‰ˆæœ¬:")
    print(f"  åˆå§‹æŸå¤±: {fixed_losses[0]:.4f}")
    print(f"  æœ€ç»ˆæŸå¤±: {fixed_losses[-1]:.4f}")
    print(f"  æŸå¤±èŒƒå›´: {min(fixed_losses):.4f} - {max(fixed_losses):.4f}")
    print(f"  å¹³å‡æŸå¤±: {np.mean(fixed_losses):.4f}")
    
    # ç»˜åˆ¶å¯¹æ¯”å›¾
    plt.figure(figsize=(12, 6))
    
    plt.subplot(1, 2, 1)
    plt.plot(problem_losses, 'r-', label='é—®é¢˜ç‰ˆæœ¬', linewidth=2)
    plt.plot(fixed_losses, 'b-', label='ä¿®å¤ç‰ˆæœ¬', linewidth=2)
    plt.xlabel('Step')
    plt.ylabel('Loss')
    plt.title('æŸå¤±å‡½æ•°å¯¹æ¯”')
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    plt.subplot(1, 2, 2)
    plt.plot(problem_losses, 'r-', label='é—®é¢˜ç‰ˆæœ¬', linewidth=2)
    plt.xlabel('Step')
    plt.ylabel('Loss')
    plt.title('é—®é¢˜ç‰ˆæœ¬æŸå¤± (æ”¾å¤§)')
    plt.legend()
    plt.grid(True, alpha=0.3)
    plt.yscale('log')
    
    plt.tight_layout()
    
    # ä¿å­˜å›¾è¡¨
    output_dir = Path("runs/loss_diagnosis")
    output_dir.mkdir(parents=True, exist_ok=True)
    plt.savefig(output_dir / 'loss_comparison_diagnosis.png', dpi=300, bbox_inches='tight')
    plt.close()
    
    print(f"\nâœ… è¯Šæ–­å›¾è¡¨å·²ä¿å­˜: {output_dir}/loss_comparison_diagnosis.png")
    
    # é—®é¢˜æ€»ç»“
    print("\nğŸš¨ é—®é¢˜æ€»ç»“:")
    print("=" * 60)
    
    if np.mean(problem_losses) < 0.1:
        print("âŒ ç¡®è®¤é—®é¢˜: æŸå¤±å€¼å¼‚å¸¸å°")
        print("   åŸå› 1: äººå·¥ç›®æ ‡è¿‡äºç®€å•ï¼Œå®¹æ˜“æ‹Ÿåˆ")
        print("   åŸå› 2: BCEæŸå¤±å¯¹ç¨€ç–ç›®æ ‡è®¡ç®—ä¸å½“")
        print("   åŸå› 3: æ¢¯åº¦å¼ºåˆ¶é¡¹æƒé‡è¿‡å°")
        print("   åŸå› 4: ç¼ºä¹çœŸå®çš„ç›®æ ‡æ£€æµ‹æŸå¤±ç»“æ„")
    
    if np.mean(fixed_losses) > 1.0:
        print("âœ… ä¿®å¤æˆåŠŸ: æŸå¤±å€¼å›åˆ°æ­£å¸¸èŒƒå›´")
        print("   ä¿®å¤1: ä½¿ç”¨çœŸå®çš„ç›®æ ‡æ£€æµ‹æŸå¤±ç»“æ„")
        print("   ä¿®å¤2: åˆç†çš„æ­£è´Ÿæ ·æœ¬æ¯”ä¾‹")
        print("   ä¿®å¤3: é€‚å½“çš„æŸå¤±æƒé‡é…ç½®")
        print("   ä¿®å¤4: åªå¯¹æœ‰ç›®æ ‡ä½ç½®è®¡ç®—æŸå¤±")
    
    return problem_losses, fixed_losses


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ¯ JittoræŸå¤±å‡½æ•°æ·±åº¦è¯Šæ–­")
    print("æ–°èŠ½ç¬¬äºŒé˜¶æ®µï¼šå½»åº•è§£å†³æŸå¤±å¼‚å¸¸é—®é¢˜")
    print("=" * 80)
    
    # è¿è¡Œè¯Šæ–­
    problem_losses, fixed_losses = diagnose_loss_behavior()
    
    print("\nğŸ”§ ä¸‹ä¸€æ­¥ä¿®å¤å»ºè®®:")
    print("=" * 60)
    print("1. æ›¿æ¢å½“å‰çš„StableGradientLossä¸ºFixedLossFunction")
    print("2. ä½¿ç”¨çœŸå®çš„YOLOæŸå¤±ç»“æ„")
    print("3. åˆç†è®¾ç½®æ­£è´Ÿæ ·æœ¬æ¯”ä¾‹")
    print("4. è°ƒæ•´æŸå¤±æƒé‡é…ç½®")
    print("5. é‡æ–°è®­ç»ƒéªŒè¯ä¿®å¤æ•ˆæœ")
    
    print("\nâœ… è¯Šæ–­å®Œæˆï¼è¯·æŸ¥çœ‹runs/loss_diagnosis/ç›®å½•è·å–è¯¦ç»†ç»“æœ")


if __name__ == "__main__":
    main()
