#!/usr/bin/env python3
"""
ÂèØËßÜÂåñÂçïÂº†ÂõæÁâáËøáÊãüÂêàÁªìÊûú
ÁªòÂà∂Ê£ÄÊµãÊ°ÜÔºåÊòæÁ§∫ÁΩÆ‰ø°Â∫¶ÂíåÁ±ªÂà´
"""

import os
import sys
import cv2
import numpy as np
import jittor as jt
import time
from pathlib import Path

# Ê∑ªÂä†Ë∑ØÂæÑ
sys.path.append('.')
sys.path.append('./yolov6')

from models.perfect_gold_yolo import create_perfect_gold_yolo_model
from yolov6.data.data_augment import letterbox
from yolov6.models.losses import ComputeLoss
from yolov6.utils.nms import non_max_suppression

# VOCÊï∞ÊçÆÈõÜÁ±ªÂà´ÂêçÁß∞
VOC_CLASSES = [
    'aeroplane', 'bicycle', 'bird', 'boat', 'bottle', 'bus', 'car', 'cat', 'chair', 'cow',
    'diningtable', 'dog', 'horse', 'motorbike', 'person', 'pottedplant', 'sheep', 'sofa', 'train', 'tvmonitor'
]

# Á±ªÂà´È¢úËâ≤
COLORS = np.array([
    [255, 178, 50], [255, 0, 0], [0, 255, 0], [0, 0, 255], [255, 255, 0],
    [255, 0, 255], [0, 255, 255], [128, 0, 0], [0, 128, 0], [0, 0, 128],
    [128, 128, 0], [128, 0, 128], [0, 128, 128], [192, 192, 192], [128, 128, 128],
    [255, 165, 0], [255, 20, 147], [0, 191, 255], [255, 105, 180], [34, 139, 34]
], dtype=np.uint8)

def pytorch_exact_initialization(model):
    """ÂÆåÂÖ®ÁÖßÊäÑPyTorchÁâàÊú¨ÁöÑÂàùÂßãÂåñ"""
    for name, module in model.named_modules():
        if hasattr(module, 'initialize_biases'):
            module.initialize_biases()
            break
    return model

def draw_detection_box(img, box, label, confidence, color):
    """ÁªòÂà∂Ê£ÄÊµãÊ°Ü"""
    x1, y1, x2, y2 = map(int, box)
    
    # ÁªòÂà∂Ê£ÄÊµãÊ°Ü
    cv2.rectangle(img, (x1, y1), (x2, y2), color, 2)
    
    # ÂáÜÂ§áÊ†áÁ≠æÊñáÊú¨
    label_text = f'{label}: {confidence:.3f}'
    
    # ËÆ°ÁÆóÊñáÊú¨Â§ßÂ∞è
    font = cv2.FONT_HERSHEY_SIMPLEX
    font_scale = 0.6
    thickness = 2
    (text_width, text_height), _ = cv2.getTextSize(label_text, font, font_scale, thickness)
    
    # ÁªòÂà∂Ê†áÁ≠æËÉåÊôØ
    cv2.rectangle(img, (x1, y1 - text_height - 10), (x1 + text_width, y1), color, -1)
    
    # ÁªòÂà∂Ê†áÁ≠æÊñáÊú¨
    cv2.putText(img, label_text, (x1, y1 - 5), font, font_scale, (255, 255, 255), thickness)

def draw_ground_truth_box(img, box, label, color=(0, 255, 0)):
    """ÁªòÂà∂ÁúüÂÆûÊ†áÊ≥®Ê°Ü"""
    x1, y1, x2, y2 = map(int, box)
    
    # ÁªòÂà∂ÁúüÂÆûÊ°ÜÔºàËôöÁ∫øÊïàÊûúÔºâ
    cv2.rectangle(img, (x1, y1), (x2, y2), color, 2)
    
    # ÁªòÂà∂Ê†áÁ≠æ
    label_text = f'GT: {label}'
    font = cv2.FONT_HERSHEY_SIMPLEX
    font_scale = 0.5
    thickness = 1
    (text_width, text_height), _ = cv2.getTextSize(label_text, font, font_scale, thickness)
    
    cv2.rectangle(img, (x1, y2), (x1 + text_width, y2 + text_height + 5), color, -1)
    cv2.putText(img, label_text, (x1, y2 + text_height), font, font_scale, (255, 255, 255), thickness)

def visualize_overfit_results():
    """ÂèØËßÜÂåñËøáÊãüÂêàÁªìÊûú"""
    print(f"üé® ÂèØËßÜÂåñÂçïÂº†ÂõæÁâáËøáÊãüÂêàÁªìÊûú")
    print("=" * 50)
    
    # ÂáÜÂ§áÊï∞ÊçÆ
    label_file = "/home/kyc/project/GOLD-YOLO/2008_001420.txt"
    img_path = "/home/kyc/project/GOLD-YOLO/2008_001420.jpg"
    
    # ËØªÂèñÁúüÂÆûÊ†áÊ≥®
    annotations = []
    with open(label_file, 'r') as f:
        for line in f:
            line = line.strip()
            if line:
                parts = line.split()
                if len(parts) >= 5:
                    cls_id = int(parts[0])
                    x_center = float(parts[1])
                    y_center = float(parts[2])
                    width = float(parts[3])
                    height = float(parts[4])
                    annotations.append([cls_id, x_center, y_center, width, height])
    
    target_counts = {}
    for ann in annotations:
        cls_name = VOC_CLASSES[ann[0]]
        target_counts[cls_name] = target_counts.get(cls_name, 0) + 1
    
    print(f"üìã ÊúüÊúõÊ£ÄÊµãÁªìÊûú: {target_counts}")
    print(f"   ÊÄªÁõÆÊ†áÊï∞: {len(annotations)}")
    
    # ËØªÂèñÂéüÂßãÂõæÂÉè
    original_img = cv2.imread(img_path)
    img_height, img_width = original_img.shape[:2]
    
    # ÂáÜÂ§áËæìÂÖ•
    img = letterbox(original_img, new_shape=640, stride=32, auto=False)[0]
    img_tensor_input = img.transpose((2, 0, 1))[::-1]
    img_tensor_input = np.ascontiguousarray(img_tensor_input)
    img_tensor_input = img_tensor_input.astype(np.float32) / 255.0
    img_tensor = jt.array(img_tensor_input).unsqueeze(0)
    
    # ÂáÜÂ§áÊ†áÁ≠æ
    targets = []
    for ann in annotations:
        cls_id, x_center, y_center, width, height = ann
        targets.append([0, cls_id, x_center, y_center, width, height])
    targets_tensor = jt.array(targets, dtype=jt.float32).unsqueeze(0)
    
    # ÂàõÂª∫Ê®°Âûã
    model = create_perfect_gold_yolo_model()
    model = pytorch_exact_initialization(model)
    model.train()
    
    # ÂàõÂª∫ÊçüÂ§±ÂáΩÊï∞Âíå‰ºòÂåñÂô®
    loss_fn = ComputeLoss(
        num_classes=20,
        ori_img_size=640,
        warmup_epoch=4,
        use_dfl=False,
        reg_max=0,
        iou_type='giou',
        loss_weight={'class': 1.0, 'iou': 2.5, 'dfl': 0.5}
    )
    optimizer = jt.optim.AdamW(model.parameters(), lr=0.05)
    
    # ÂàõÂª∫‰øùÂ≠òÁõÆÂΩï
    save_dir = Path("runs/visualization_overfit")
    save_dir.mkdir(parents=True, exist_ok=True)
    
    print(f"\nüöÄ Âø´ÈÄüËÆ≠ÁªÉ100ËΩÆÂπ∂ÂèØËßÜÂåñ:")
    
    # ËÆ≠ÁªÉÂæ™ÁéØ
    for epoch in range(100):
        # ÂâçÂêë‰º†Êí≠
        outputs = model(img_tensor)
        
        # ËÆ°ÁÆóÊçüÂ§±
        loss, loss_items = loss_fn(outputs, targets_tensor, epoch_num=epoch+1, step_num=1)
        
        # ‰ºòÂåñ
        optimizer.step(loss)
        
        epoch_loss = float(loss.numpy())
        
        # ÊØè25ËΩÆÂèØËßÜÂåñ‰∏ÄÊ¨°
        if (epoch + 1) % 25 == 0:
            print(f"\n   Epoch {epoch+1}: Loss {epoch_loss:.6f}")
            
            # Êé®ÁêÜÊ®°Âºè
            model.eval()
            with jt.no_grad():
                test_outputs = model(img_tensor)
                
                # ÂàÜÊûêÊ®°ÂûãËæìÂá∫
                coords = test_outputs[..., :4]
                objectness = test_outputs[..., 4]
                classes = test_outputs[..., 5:]
                
                print(f"     Ê®°ÂûãËæìÂá∫ÂàÜÊûê:")
                print(f"       ÂùêÊ†áËåÉÂõ¥: [{coords.min():.3f}, {coords.max():.3f}]")
                print(f"       objectnessËåÉÂõ¥: [{objectness.min():.3f}, {objectness.max():.3f}]")
                print(f"       Á±ªÂà´ÂàÜÊï∞ËåÉÂõ¥: [{classes.min():.6f}, {classes.max():.6f}]")
                
                # Ê£ÄÊü•ÊúüÊúõÁ±ªÂà´ÁöÑÂàÜÊï∞
                expected_classes = [3, 11, 14]  # boat, dog, person
                print(f"     ÊúüÊúõÁ±ªÂà´ÂàÜÊï∞:")
                for cls_id in expected_classes:
                    cls_scores = classes[0, :, cls_id]
                    max_score = float(cls_scores.max())
                    argmax_result = cls_scores.argmax(dim=0)
                    if isinstance(argmax_result, tuple):
                        max_idx = int(argmax_result[0])
                    else:
                        max_idx = int(argmax_result)
                    print(f"       {VOC_CLASSES[cls_id]}(Á±ªÂà´{cls_id}): ÊúÄÂ§ß{max_score:.6f} (‰ΩçÁΩÆ{max_idx})")

                # Ê£ÄÊü•aeroplaneÁöÑÂàÜÊï∞
                aero_scores = classes[0, :, 0]
                aero_max_score = float(aero_scores.max())
                aero_argmax_result = aero_scores.argmax(dim=0)
                if isinstance(aero_argmax_result, tuple):
                    aero_max_idx = int(aero_argmax_result[0])
                else:
                    aero_max_idx = int(aero_argmax_result)
                print(f"       aeroplane(Á±ªÂà´0): ÊúÄÂ§ß{aero_max_score:.6f} (‰ΩçÁΩÆ{aero_max_idx})")
                
                # **ÂÖ≥ÈîÆË∞ÉËØïÔºöÊâãÂä®Ê£ÄÊü•ÊúÄÈ´òÂàÜÊï∞ÁöÑÁ±ªÂà´**
                print(f"\n     üîç ÊâãÂä®Ê£ÄÊü•ÊúÄÈ´òÂàÜÊï∞ÁöÑÁ±ªÂà´:")
                all_max_scores = classes[0].max(dim=1)  # ÊØè‰∏™anchorÁöÑÊúÄÂ§ßÂàÜÊï∞
                if isinstance(all_max_scores, tuple):
                    max_scores, max_indices = all_max_scores
                else:
                    max_scores = all_max_scores
                    max_indices = classes[0].argmax(dim=1)
                
                # ÊâæÂà∞ÂÖ®Â±ÄÊúÄÈ´òÂàÜÊï∞
                global_max_score = float(max_scores.max())
                global_max_anchor_result = max_scores.argmax(dim=0)
                if isinstance(global_max_anchor_result, tuple):
                    global_max_anchor = int(global_max_anchor_result[0])
                else:
                    global_max_anchor = int(global_max_anchor_result)

                if isinstance(max_indices, tuple):
                    max_indices_tensor = max_indices[0] if len(max_indices) > 0 else max_indices
                else:
                    max_indices_tensor = max_indices

                global_max_class_result = max_indices_tensor[global_max_anchor]
                if isinstance(global_max_class_result, tuple):
                    global_max_class = int(global_max_class_result[0])
                else:
                    global_max_class = int(global_max_class_result)
                global_max_class_name = VOC_CLASSES[global_max_class] if global_max_class < len(VOC_CLASSES) else f'Class{global_max_class}'
                
                print(f"       ÂÖ®Â±ÄÊúÄÈ´òÂàÜÊï∞: {global_max_score:.6f}")
                print(f"       ÂØπÂ∫îanchor: {global_max_anchor}")
                print(f"       ÂØπÂ∫îÁ±ªÂà´: {global_max_class_name}(Á±ªÂà´{global_max_class})")
                
                # NMSÂ§ÑÁêÜ
                pred = non_max_suppression(test_outputs, conf_thres=0.01, iou_thres=0.45, max_det=100)
                
                if len(pred) > 0 and len(pred[0]) > 0:
                    detections = pred[0]
                    det_count = len(detections)
                    print(f"     NMSÂêéÊ£ÄÊµãÊï∞Èáè: {det_count}")
                    
                    # ËΩ¨Êç¢‰∏∫numpy
                    if hasattr(detections, 'numpy'):
                        detections_np = detections.numpy()
                    else:
                        detections_np = detections
                    
                    # Á°Æ‰øùÊ£ÄÊµãÁªìÊûúÊòØ2Áª¥ÁöÑ
                    if detections_np.ndim == 3:
                        detections_np = detections_np.reshape(-1, detections_np.shape[-1])
                    
                    # ÂàõÂª∫ÂèØËßÜÂåñÂõæÂÉè
                    vis_img = original_img.copy()
                    
                    # ÁªòÂà∂ÁúüÂÆûÊ†áÊ≥®Ê°Ü
                    for ann in annotations:
                        cls_id, x_center, y_center, width, height = ann
                        
                        # ËΩ¨Êç¢‰∏∫ÂÉèÁ¥†ÂùêÊ†á
                        x1 = int((x_center - width/2) * img_width)
                        y1 = int((y_center - height/2) * img_height)
                        x2 = int((x_center + width/2) * img_width)
                        y2 = int((y_center + height/2) * img_height)
                        
                        cls_name = VOC_CLASSES[cls_id]
                        draw_ground_truth_box(vis_img, [x1, y1, x2, y2], cls_name)
                    
                    # ÁªüËÆ°Ê£ÄÊµãÁªìÊûú
                    detected_counts = {}
                    confidence_info = []
                    
                    # ÁªòÂà∂Ê£ÄÊµãÊ°Ü
                    for i, detection in enumerate(detections_np):
                        if len(detection) >= 6:
                            x1, y1, x2, y2, conf, cls_id = detection[:6]
                            cls_id = int(cls_id)
                            cls_name = VOC_CLASSES[cls_id] if cls_id < len(VOC_CLASSES) else f'Class{cls_id}'
                            
                            detected_counts[cls_name] = detected_counts.get(cls_name, 0) + 1
                            confidence_info.append((cls_name, float(conf)))
                            
                            # Âè™ÁªòÂà∂Ââç10‰∏™Ê£ÄÊµãÊ°Ü
                            if i < 10:
                                color = COLORS[cls_id % len(COLORS)].tolist()
                                
                                # Áº©ÊîæÂùêÊ†áÂà∞ÂéüÂõæÂ∞∫ÂØ∏
                                scale_x = img_width / 640
                                scale_y = img_height / 640
                                x1_scaled = int(x1 * scale_x)
                                y1_scaled = int(y1 * scale_y)
                                x2_scaled = int(x2 * scale_x)
                                y2_scaled = int(y2 * scale_y)
                                
                                draw_detection_box(vis_img, [x1_scaled, y1_scaled, x2_scaled, y2_scaled], 
                                                 cls_name, float(conf), color)
                    
                    print(f"     Ê£ÄÊµãÁ±ªÂà´ÁªüËÆ°: {detected_counts}")
                    
                    # ÊòæÁ§∫ÁΩÆ‰ø°Â∫¶ÊúÄÈ´òÁöÑÂâç5‰∏™Ê£ÄÊµã
                    confidence_info.sort(key=lambda x: x[1], reverse=True)
                    print(f"     ÁΩÆ‰ø°Â∫¶ÊúÄÈ´òÁöÑ5‰∏™Ê£ÄÊµã:")
                    for i, (cls_name, conf) in enumerate(confidence_info[:5]):
                        print(f"       {i+1}. {cls_name}: {conf:.6f}")
                    
                    # ‰øùÂ≠òÂèØËßÜÂåñÁªìÊûú
                    save_path = save_dir / f'epoch_{epoch+1:03d}_visualization.jpg'
                    cv2.imwrite(str(save_path), vis_img)
                    print(f"     üíæ ÂèØËßÜÂåñÁªìÊûúÂ∑≤‰øùÂ≠ò: {save_path}")
                    
                    # Ê£ÄÊü•ÊòØÂê¶Ê£ÄÊµãÂà∞ÊúüÊúõÁ±ªÂà´
                    expected_class_names = set(target_counts.keys())
                    detected_class_names = set(detected_counts.keys())
                    correct_classes = expected_class_names.intersection(detected_class_names)
                    
                    if len(correct_classes) > 0:
                        print(f"     ‚úÖ Ê£ÄÊµãÂà∞Ê≠£Á°ÆÁ±ªÂà´: {correct_classes}")
                        species_accuracy = len(correct_classes) / len(expected_class_names)
                        print(f"     ÁßçÁ±ªÂáÜÁ°ÆÁéá: {species_accuracy*100:.1f}%")
                        
                        # **ÂÖ≥ÈîÆÊ£ÄÊü•ÔºödogÊòØÂê¶ÊòØÁΩÆ‰ø°Â∫¶ÊúÄÈ´òÁöÑ**
                        if confidence_info and confidence_info[0][0] == 'dog':
                            print(f"     üéâ dogÊòØÁΩÆ‰ø°Â∫¶ÊúÄÈ´òÁöÑÁ±ªÂà´ÔºÅ")
                            if species_accuracy >= 0.8:
                                print(f"\nüéâ ÂÆåÁæéËøáÊãüÂêàÊàêÂäüÔºÅ")
                                return True
                        else:
                            print(f"     ‚ùå dog‰∏çÊòØÁΩÆ‰ø°Â∫¶ÊúÄÈ´òÁöÑÁ±ªÂà´ÔºåÈúÄË¶ÅÁªßÁª≠ËÆ≠ÁªÉ")
                    else:
                        expected_class_names_list = list(expected_class_names)
                        print(f"     ‚ùå Êú™Ê£ÄÊµãÂà∞Ê≠£Á°ÆÁ±ªÂà´ÔºåÊúüÊúõ: {expected_class_names_list}")
                else:
                    print(f"     ‚ùå Ê≤°ÊúâÊ£ÄÊµãÁªìÊûú")
            
            model.train()
    
    print(f"\n‚ö†Ô∏è 100ËΩÆËÆ≠ÁªÉÂÆåÊàêÔºåÈúÄË¶ÅËøõ‰∏ÄÊ≠•ÂàÜÊûê")
    return False

def main():
    print("üî• ÂèØËßÜÂåñÂçïÂº†ÂõæÁâáËøáÊãüÂêàÁªìÊûú")
    print("=" * 70)
    print("ÁõÆÊ†áÔºöÂèØËßÜÂåñÊ£ÄÊµãÁªìÊûúÔºåÁ°Æ‰øùdogÊòØÁΩÆ‰ø°Â∫¶ÊúÄÈ´òÁöÑÁ±ªÂà´")
    print("=" * 70)
    
    success = visualize_overfit_results()
    
    if success:
        print(f"\nüéâüéâüéâ ÂèØËßÜÂåñÊàêÂäüÔºÅüéâüéâüéâ")
        print(f"‚úÖ dogÊòØÁΩÆ‰ø°Â∫¶ÊúÄÈ´òÁöÑÁ±ªÂà´")
        print(f"‚úÖ ÂçïÂº†ÂõæÁâáËøáÊãüÂêàÊàêÂäü")
    else:
        print(f"\n‚ö†Ô∏è ÈúÄË¶ÅÊ∑±ÂÖ•‰øÆÂ§çÁΩÆ‰ø°Â∫¶ÈóÆÈ¢ò")

if __name__ == "__main__":
    main()
