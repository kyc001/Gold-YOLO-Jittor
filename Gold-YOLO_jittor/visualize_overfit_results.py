#!/usr/bin/env python3
"""
å¯è§†åŒ–å•å¼ å›¾ç‰‡è¿‡æ‹Ÿåˆç»“æœ
ç»˜åˆ¶æ£€æµ‹æ¡†ï¼Œæ˜¾ç¤ºç½®ä¿¡åº¦å’Œç±»åˆ«
"""

import os
import sys
import cv2
import numpy as np
import jittor as jt
import time
from pathlib import Path

# æ·»åŠ è·¯å¾„
sys.path.append('.')
sys.path.append('./yolov6')

from models.perfect_gold_yolo import create_perfect_gold_yolo_model
from yolov6.data.data_augment import letterbox
from yolov6.models.losses import ComputeLoss
from yolov6.utils.nms import non_max_suppression

# VOCæ•°æ®é›†ç±»åˆ«åç§°
VOC_CLASSES = [
    'aeroplane', 'bicycle', 'bird', 'boat', 'bottle', 'bus', 'car', 'cat', 'chair', 'cow',
    'diningtable', 'dog', 'horse', 'motorbike', 'person', 'pottedplant', 'sheep', 'sofa', 'train', 'tvmonitor'
]

# ç±»åˆ«é¢œè‰²
COLORS = np.array([
    [255, 178, 50], [255, 0, 0], [0, 255, 0], [0, 0, 255], [255, 255, 0],
    [255, 0, 255], [0, 255, 255], [128, 0, 0], [0, 128, 0], [0, 0, 128],
    [128, 128, 0], [128, 0, 128], [0, 128, 128], [192, 192, 192], [128, 128, 128],
    [255, 165, 0], [255, 20, 147], [0, 191, 255], [255, 105, 180], [34, 139, 34]
], dtype=np.uint8)

def pytorch_exact_initialization(model):
    """å®Œå…¨ç…§æŠ„PyTorchç‰ˆæœ¬çš„åˆå§‹åŒ–"""
    for name, module in model.named_modules():
        if hasattr(module, 'initialize_biases'):
            module.initialize_biases()
            break
    return model

def draw_detection_box(img, box, label, confidence, color):
    """ç»˜åˆ¶æ£€æµ‹æ¡†"""
    x1, y1, x2, y2 = map(int, box)
    
    # ç»˜åˆ¶æ£€æµ‹æ¡†
    cv2.rectangle(img, (x1, y1), (x2, y2), color, 2)
    
    # å‡†å¤‡æ ‡ç­¾æ–‡æœ¬
    label_text = f'{label}: {confidence:.3f}'
    
    # è®¡ç®—æ–‡æœ¬å¤§å°
    font = cv2.FONT_HERSHEY_SIMPLEX
    font_scale = 0.6
    thickness = 2
    (text_width, text_height), _ = cv2.getTextSize(label_text, font, font_scale, thickness)
    
    # ç»˜åˆ¶æ ‡ç­¾èƒŒæ™¯
    cv2.rectangle(img, (x1, y1 - text_height - 10), (x1 + text_width, y1), color, -1)
    
    # ç»˜åˆ¶æ ‡ç­¾æ–‡æœ¬
    cv2.putText(img, label_text, (x1, y1 - 5), font, font_scale, (255, 255, 255), thickness)

def draw_ground_truth_box(img, box, label, color=(0, 255, 0)):
    """ç»˜åˆ¶çœŸå®æ ‡æ³¨æ¡†"""
    x1, y1, x2, y2 = map(int, box)
    
    # ç»˜åˆ¶çœŸå®æ¡†ï¼ˆè™šçº¿æ•ˆæœï¼‰
    cv2.rectangle(img, (x1, y1), (x2, y2), color, 2)
    
    # ç»˜åˆ¶æ ‡ç­¾
    label_text = f'GT: {label}'
    font = cv2.FONT_HERSHEY_SIMPLEX
    font_scale = 0.5
    thickness = 1
    (text_width, text_height), _ = cv2.getTextSize(label_text, font, font_scale, thickness)
    
    cv2.rectangle(img, (x1, y2), (x1 + text_width, y2 + text_height + 5), color, -1)
    cv2.putText(img, label_text, (x1, y2 + text_height), font, font_scale, (255, 255, 255), thickness)

def visualize_overfit_results():
    """å¯è§†åŒ–è¿‡æ‹Ÿåˆç»“æœ"""
    print(f"ğŸ¨ å¯è§†åŒ–å•å¼ å›¾ç‰‡è¿‡æ‹Ÿåˆç»“æœ")
    print("=" * 50)
    
    # å‡†å¤‡æ•°æ®
    label_file = "/home/kyc/project/GOLD-YOLO/2008_001420.txt"
    img_path = "/home/kyc/project/GOLD-YOLO/2008_001420.jpg"
    
    # è¯»å–çœŸå®æ ‡æ³¨
    annotations = []
    with open(label_file, 'r') as f:
        for line in f:
            line = line.strip()
            if line:
                parts = line.split()
                if len(parts) >= 5:
                    cls_id = int(parts[0])
                    x_center = float(parts[1])
                    y_center = float(parts[2])
                    width = float(parts[3])
                    height = float(parts[4])
                    annotations.append([cls_id, x_center, y_center, width, height])
    
    target_counts = {}
    for ann in annotations:
        cls_name = VOC_CLASSES[ann[0]]
        target_counts[cls_name] = target_counts.get(cls_name, 0) + 1
    
    print(f"ğŸ“‹ æœŸæœ›æ£€æµ‹ç»“æœ: {target_counts}")
    print(f"   æ€»ç›®æ ‡æ•°: {len(annotations)}")
    
    # è¯»å–åŸå§‹å›¾åƒ
    original_img = cv2.imread(img_path)
    img_height, img_width = original_img.shape[:2]
    
    # å‡†å¤‡è¾“å…¥
    img = letterbox(original_img, new_shape=640, stride=32, auto=False)[0]
    img_tensor_input = img.transpose((2, 0, 1))[::-1]
    img_tensor_input = np.ascontiguousarray(img_tensor_input)
    img_tensor_input = img_tensor_input.astype(np.float32) / 255.0
    img_tensor = jt.array(img_tensor_input).unsqueeze(0)
    
    # å‡†å¤‡æ ‡ç­¾
    targets = []
    for ann in annotations:
        cls_id, x_center, y_center, width, height = ann
        targets.append([0, cls_id, x_center, y_center, width, height])
    targets_tensor = jt.array(targets, dtype=jt.float32).unsqueeze(0)
    
    # åˆ›å»ºæ¨¡å‹
    model = create_perfect_gold_yolo_model()
    model = pytorch_exact_initialization(model)
    model.train()
    
    # åˆ›å»ºæŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨
    loss_fn = ComputeLoss(
        num_classes=20,
        ori_img_size=640,
        warmup_epoch=4,
        use_dfl=False,
        reg_max=0,
        iou_type='giou',
        loss_weight={'class': 1.0, 'iou': 2.5, 'dfl': 0.5}
    )
    optimizer = jt.optim.AdamW(model.parameters(), lr=0.05)
    
    # åˆ›å»ºä¿å­˜ç›®å½•
    save_dir = Path("runs/visualization_overfit")
    save_dir.mkdir(parents=True, exist_ok=True)
    
    print(f"\nğŸš€ å¿«é€Ÿè®­ç»ƒ100è½®å¹¶å¯è§†åŒ–:")
    
    # è®­ç»ƒå¾ªç¯
    for epoch in range(100):
        # å‰å‘ä¼ æ’­
        outputs = model(img_tensor)
        
        # è®¡ç®—æŸå¤±
        loss, loss_items = loss_fn(outputs, targets_tensor, epoch_num=epoch+1, step_num=1)
        
        # ä¼˜åŒ–
        optimizer.step(loss)
        
        epoch_loss = float(loss.numpy())
        
        # æ¯25è½®å¯è§†åŒ–ä¸€æ¬¡
        if (epoch + 1) % 25 == 0:
            print(f"\n   Epoch {epoch+1}: Loss {epoch_loss:.6f}")
            
            # æ¨ç†æ¨¡å¼
            model.eval()
            with jt.no_grad():
                test_outputs = model(img_tensor)
                
                # åˆ†ææ¨¡å‹è¾“å‡º
                coords = test_outputs[..., :4]
                objectness = test_outputs[..., 4]
                classes = test_outputs[..., 5:]
                
                print(f"     æ¨¡å‹è¾“å‡ºåˆ†æ:")
                print(f"       åæ ‡èŒƒå›´: [{coords.min():.3f}, {coords.max():.3f}]")
                print(f"       objectnessèŒƒå›´: [{objectness.min():.3f}, {objectness.max():.3f}]")
                print(f"       ç±»åˆ«åˆ†æ•°èŒƒå›´: [{classes.min():.6f}, {classes.max():.6f}]")
                
                # æ£€æŸ¥æœŸæœ›ç±»åˆ«çš„åˆ†æ•°
                expected_classes = [3, 11, 14]  # boat, dog, person
                print(f"     æœŸæœ›ç±»åˆ«åˆ†æ•°:")
                for cls_id in expected_classes:
                    cls_scores = classes[0, :, cls_id]
                    max_score = float(cls_scores.max())
                    argmax_result = cls_scores.argmax(dim=0)
                    if isinstance(argmax_result, tuple):
                        max_idx = int(argmax_result[0])
                    else:
                        max_idx = int(argmax_result)
                    print(f"       {VOC_CLASSES[cls_id]}(ç±»åˆ«{cls_id}): æœ€å¤§{max_score:.6f} (ä½ç½®{max_idx})")

                # æ£€æŸ¥aeroplaneçš„åˆ†æ•°
                aero_scores = classes[0, :, 0]
                aero_max_score = float(aero_scores.max())
                aero_argmax_result = aero_scores.argmax(dim=0)
                if isinstance(aero_argmax_result, tuple):
                    aero_max_idx = int(aero_argmax_result[0])
                else:
                    aero_max_idx = int(aero_argmax_result)
                print(f"       aeroplane(ç±»åˆ«0): æœ€å¤§{aero_max_score:.6f} (ä½ç½®{aero_max_idx})")
                
                # **å…³é”®è°ƒè¯•ï¼šæ‰‹åŠ¨æ£€æŸ¥æœ€é«˜åˆ†æ•°çš„ç±»åˆ«**
                print(f"\n     ğŸ” æ‰‹åŠ¨æ£€æŸ¥æœ€é«˜åˆ†æ•°çš„ç±»åˆ«:")
                all_max_scores = classes[0].max(dim=1)  # æ¯ä¸ªanchorçš„æœ€å¤§åˆ†æ•°
                if isinstance(all_max_scores, tuple):
                    max_scores, max_indices = all_max_scores
                else:
                    max_scores = all_max_scores
                    max_indices = classes[0].argmax(dim=1)
                
                # æ‰¾åˆ°å…¨å±€æœ€é«˜åˆ†æ•°
                global_max_score = float(max_scores.max())
                global_max_anchor_result = max_scores.argmax(dim=0)
                if isinstance(global_max_anchor_result, tuple):
                    global_max_anchor = int(global_max_anchor_result[0])
                else:
                    global_max_anchor = int(global_max_anchor_result)

                if isinstance(max_indices, tuple):
                    max_indices_tensor = max_indices[0] if len(max_indices) > 0 else max_indices
                else:
                    max_indices_tensor = max_indices

                global_max_class_result = max_indices_tensor[global_max_anchor]
                if isinstance(global_max_class_result, tuple):
                    global_max_class = int(global_max_class_result[0])
                else:
                    global_max_class = int(global_max_class_result)
                global_max_class_name = VOC_CLASSES[global_max_class] if global_max_class < len(VOC_CLASSES) else f'Class{global_max_class}'
                
                print(f"       å…¨å±€æœ€é«˜åˆ†æ•°: {global_max_score:.6f}")
                print(f"       å¯¹åº”anchor: {global_max_anchor}")
                print(f"       å¯¹åº”ç±»åˆ«: {global_max_class_name}(ç±»åˆ«{global_max_class})")
                
                # NMSå¤„ç†
                pred = non_max_suppression(test_outputs, conf_thres=0.01, iou_thres=0.45, max_det=100)
                
                if len(pred) > 0 and len(pred[0]) > 0:
                    detections = pred[0]
                    det_count = len(detections)
                    print(f"     NMSåæ£€æµ‹æ•°é‡: {det_count}")
                    
                    # è½¬æ¢ä¸ºnumpy
                    if hasattr(detections, 'numpy'):
                        detections_np = detections.numpy()
                    else:
                        detections_np = detections
                    
                    # ç¡®ä¿æ£€æµ‹ç»“æœæ˜¯2ç»´çš„
                    if detections_np.ndim == 3:
                        detections_np = detections_np.reshape(-1, detections_np.shape[-1])
                    
                    # åˆ›å»ºå¯è§†åŒ–å›¾åƒ
                    vis_img = original_img.copy()
                    
                    # ç»˜åˆ¶çœŸå®æ ‡æ³¨æ¡†
                    for ann in annotations:
                        cls_id, x_center, y_center, width, height = ann
                        
                        # è½¬æ¢ä¸ºåƒç´ åæ ‡
                        x1 = int((x_center - width/2) * img_width)
                        y1 = int((y_center - height/2) * img_height)
                        x2 = int((x_center + width/2) * img_width)
                        y2 = int((y_center + height/2) * img_height)
                        
                        cls_name = VOC_CLASSES[cls_id]
                        draw_ground_truth_box(vis_img, [x1, y1, x2, y2], cls_name)
                    
                    # ç»Ÿè®¡æ£€æµ‹ç»“æœ
                    detected_counts = {}
                    confidence_info = []
                    
                    # ç»˜åˆ¶æ£€æµ‹æ¡†
                    for i, detection in enumerate(detections_np):
                        if len(detection) >= 6:
                            x1, y1, x2, y2, conf, cls_id = detection[:6]
                            cls_id = int(cls_id)
                            cls_name = VOC_CLASSES[cls_id] if cls_id < len(VOC_CLASSES) else f'Class{cls_id}'
                            
                            detected_counts[cls_name] = detected_counts.get(cls_name, 0) + 1
                            confidence_info.append((cls_name, float(conf)))
                            
                            # åªç»˜åˆ¶å‰10ä¸ªæ£€æµ‹æ¡†
                            if i < 10:
                                color = COLORS[cls_id % len(COLORS)].tolist()
                                
                                # ç¼©æ”¾åæ ‡åˆ°åŸå›¾å°ºå¯¸
                                scale_x = img_width / 640
                                scale_y = img_height / 640
                                x1_scaled = int(x1 * scale_x)
                                y1_scaled = int(y1 * scale_y)
                                x2_scaled = int(x2 * scale_x)
                                y2_scaled = int(y2 * scale_y)
                                
                                draw_detection_box(vis_img, [x1_scaled, y1_scaled, x2_scaled, y2_scaled], 
                                                 cls_name, float(conf), color)
                    
                    print(f"     æ£€æµ‹ç±»åˆ«ç»Ÿè®¡: {detected_counts}")
                    
                    # æ˜¾ç¤ºç½®ä¿¡åº¦æœ€é«˜çš„å‰5ä¸ªæ£€æµ‹
                    confidence_info.sort(key=lambda x: x[1], reverse=True)
                    print(f"     ç½®ä¿¡åº¦æœ€é«˜çš„5ä¸ªæ£€æµ‹:")
                    for i, (cls_name, conf) in enumerate(confidence_info[:5]):
                        print(f"       {i+1}. {cls_name}: {conf:.6f}")
                    
                    # ä¿å­˜å¯è§†åŒ–ç»“æœ
                    save_path = save_dir / f'epoch_{epoch+1:03d}_visualization.jpg'
                    cv2.imwrite(str(save_path), vis_img)
                    print(f"     ğŸ’¾ å¯è§†åŒ–ç»“æœå·²ä¿å­˜: {save_path}")
                    
                    # æ£€æŸ¥æ˜¯å¦æ£€æµ‹åˆ°æœŸæœ›ç±»åˆ«
                    expected_class_names = set(target_counts.keys())
                    detected_class_names = set(detected_counts.keys())
                    correct_classes = expected_class_names.intersection(detected_class_names)
                    
                    if len(correct_classes) > 0:
                        print(f"     âœ… æ£€æµ‹åˆ°æ­£ç¡®ç±»åˆ«: {correct_classes}")
                        species_accuracy = len(correct_classes) / len(expected_class_names)
                        print(f"     ç§ç±»å‡†ç¡®ç‡: {species_accuracy*100:.1f}%")
                        
                        # **å…³é”®æ£€æŸ¥ï¼šdogæ˜¯å¦æ˜¯ç½®ä¿¡åº¦æœ€é«˜çš„**
                        if confidence_info and confidence_info[0][0] == 'dog':
                            print(f"     ğŸ‰ dogæ˜¯ç½®ä¿¡åº¦æœ€é«˜çš„ç±»åˆ«ï¼")
                            if species_accuracy >= 0.8:
                                print(f"\nğŸ‰ å®Œç¾è¿‡æ‹ŸåˆæˆåŠŸï¼")
                                return True
                        else:
                            print(f"     âŒ dogä¸æ˜¯ç½®ä¿¡åº¦æœ€é«˜çš„ç±»åˆ«ï¼Œéœ€è¦ç»§ç»­è®­ç»ƒ")
                    else:
                        expected_class_names_list = list(expected_class_names)
                        print(f"     âŒ æœªæ£€æµ‹åˆ°æ­£ç¡®ç±»åˆ«ï¼ŒæœŸæœ›: {expected_class_names_list}")
                else:
                    print(f"     âŒ æ²¡æœ‰æ£€æµ‹ç»“æœ")
            
            model.train()
    
    print(f"\nâš ï¸ 100è½®è®­ç»ƒå®Œæˆï¼Œéœ€è¦è¿›ä¸€æ­¥åˆ†æ")
    return False

def main():
    print("ğŸ”¥ å¯è§†åŒ–å•å¼ å›¾ç‰‡è¿‡æ‹Ÿåˆç»“æœ")
    print("=" * 70)
    print("ç›®æ ‡ï¼šå¯è§†åŒ–æ£€æµ‹ç»“æœï¼Œç¡®ä¿dogæ˜¯ç½®ä¿¡åº¦æœ€é«˜çš„ç±»åˆ«")
    print("=" * 70)
    
    success = visualize_overfit_results()
    
    if success:
        print(f"\nğŸ‰ğŸ‰ğŸ‰ å¯è§†åŒ–æˆåŠŸï¼ğŸ‰ğŸ‰ğŸ‰")
        print(f"âœ… dogæ˜¯ç½®ä¿¡åº¦æœ€é«˜çš„ç±»åˆ«")
        print(f"âœ… å•å¼ å›¾ç‰‡è¿‡æ‹ŸåˆæˆåŠŸ")
    else:
        print(f"\nâš ï¸ éœ€è¦æ·±å…¥ä¿®å¤ç½®ä¿¡åº¦é—®é¢˜")

if __name__ == "__main__":
    main()
