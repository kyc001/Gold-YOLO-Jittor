#!/usr/bin/env python3
# -*- coding:utf-8 -*-
"""
GOLD-YOLO Jittorç‰ˆæœ¬ - CPUè®­ç»ƒè„šæœ¬ï¼ˆä¿®å¤æ•°æ®æ ¼å¼é—®é¢˜ï¼‰
ä¸“é—¨ç”¨äºCPUè®­ç»ƒï¼Œé¿å…CUDAé—®é¢˜
"""

import os
import sys
import argparse
import yaml
import numpy as np
from pathlib import Path
from tqdm import tqdm

# å¼ºåˆ¶CPUæ¨¡å¼
os.environ['JT_SYNC'] = '1'

import jittor as jt
jt.flags.use_cuda = 0  # å¼ºåˆ¶ä½¿ç”¨CPU
print("âš ï¸ å¼ºåˆ¶ä½¿ç”¨CPUè®­ç»ƒï¼ˆé¿å…CUDAé—®é¢˜ï¼‰")


def parse_args():
    """è§£æå‘½ä»¤è¡Œå‚æ•°"""
    parser = argparse.ArgumentParser(description='GOLD-YOLO CPU Training')
    parser.add_argument('--data', type=str, default='data/voc_subset_improved.yaml', 
                        help='æ•°æ®é…ç½®æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--epochs', type=int, default=50, help='è®­ç»ƒè½®æ•°')
    parser.add_argument('--batch-size', type=int, default=2, help='æ‰¹æ¬¡å¤§å°(CPUä¼˜åŒ–)')
    parser.add_argument('--img-size', type=int, default=640, help='å›¾åƒå°ºå¯¸')
    parser.add_argument('--lr', type=float, default=0.005, help='å­¦ä¹ ç‡(CPUä¼˜åŒ–)')
    parser.add_argument('--project', type=str, default='runs/train', help='é¡¹ç›®ä¿å­˜ç›®å½•')
    parser.add_argument('--name', type=str, default='cpu_training', help='å®éªŒåç§°')
    
    return parser.parse_args()


def load_data_config(config_path):
    """åŠ è½½æ•°æ®é…ç½®æ–‡ä»¶"""
    try:
        # å¦‚æœæ˜¯ç›¸å¯¹è·¯å¾„ï¼Œè½¬æ¢ä¸ºç»å¯¹è·¯å¾„
        if not os.path.isabs(config_path):
            script_dir = os.path.dirname(os.path.abspath(__file__))
            config_path = os.path.join(script_dir, config_path)
        
        with open(config_path, 'r', encoding='utf-8') as f:
            config = yaml.safe_load(f)
        print(f"âœ… æ•°æ®é…ç½®åŠ è½½æˆåŠŸ: {config_path}")
        return config
    except Exception as e:
        print(f"âŒ æ•°æ®é…ç½®åŠ è½½å¤±è´¥: {e}")
        print(f"   å°è¯•çš„è·¯å¾„: {config_path}")
        sys.exit(1)


def create_simple_dataset(data_config, img_size, is_train=True):
    """åˆ›å»ºç®€åŒ–çš„æ•°æ®é›† - CPUä¼˜åŒ–ç‰ˆ"""
    try:
        from yolov6.data.datasets import TrainValDataset
        
        data_path = data_config['train'] if is_train else data_config['val']
        
        # CPUä¼˜åŒ–çš„ç®€åŒ–é…ç½®
        hyp = {
            'mosaic': 0.0,      # ç¦ç”¨å¤æ‚å¢å¼º
            'mixup': 0.0,
            'degrees': 0.0,
            'translate': 0.0,
            'scale': 0.0,
            'shear': 0.0,
            'flipud': 0.0,
            'fliplr': 0.5,      # åªä¿ç•™æ°´å¹³ç¿»è½¬
            'hsv_h': 0.0,
            'hsv_s': 0.0,
            'hsv_v': 0.0
        }
        
        dataset = TrainValDataset(
            img_dir=data_path,
            img_size=img_size,
            augment=is_train,
            hyp=hyp,
            rect=False,
            check_images=True,
            check_labels=True,
            stride=32,
            pad=0.0,
            rank=-1,
            data_dict=data_config
        )
        
        print(f"âœ… {'è®­ç»ƒ' if is_train else 'éªŒè¯'}æ•°æ®é›†åˆ›å»ºæˆåŠŸ: {len(dataset)} æ ·æœ¬")
        return dataset
        
    except Exception as e:
        print(f"âŒ æ•°æ®é›†åˆ›å»ºå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)


def create_model_and_loss(num_classes=20):
    """åˆ›å»ºæ¨¡å‹å’ŒæŸå¤±å‡½æ•°"""
    try:
        # åˆ›å»ºæ¨¡å‹
        from models.perfect_gold_yolo import create_perfect_gold_yolo_model
        model = create_perfect_gold_yolo_model('gold_yolo-n', num_classes)
        
        # åˆ›å»ºæŸå¤±å‡½æ•° - ç›´æ¥å¯¼å…¥ä¿®å¤ç‰ˆæœ¬
        import importlib.util
        losses_file = os.path.join(os.path.dirname(__file__), 'yolov6', 'models', 'losses.py')
        spec = importlib.util.spec_from_file_location("fixed_losses", losses_file)
        fixed_losses = importlib.util.module_from_spec(spec)
        spec.loader.exec_module(fixed_losses)
        ComputeLoss = fixed_losses.ComputeLoss
        
        loss_fn = ComputeLoss(
            fpn_strides=[8, 16, 32],
            grid_cell_size=5.0,
            grid_cell_offset=0.5,
            num_classes=num_classes,
            ori_img_size=640,
            warmup_epoch=4,
            use_dfl=False,
            reg_max=0,
            iou_type='giou',
            loss_weight={'class': 1.0, 'iou': 2.5, 'dfl': 0.5}
        )
        
        print("âœ… æ¨¡å‹å’ŒæŸå¤±å‡½æ•°åˆ›å»ºæˆåŠŸ")
        return model, loss_fn
        
    except Exception as e:
        print(f"âŒ æ¨¡å‹åˆ›å»ºå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)


def safe_loss_computation(loss_fn, outputs, targets, epoch, step):
    """å®‰å…¨çš„æŸå¤±è®¡ç®—"""
    try:
        loss_result = loss_fn(outputs, targets, epoch, step)
        
        if isinstance(loss_result, tuple):
            total_loss = loss_result[0]
        else:
            total_loss = loss_result
        
        # ç¡®ä¿æŸå¤±æ˜¯æ ‡é‡
        if hasattr(total_loss, 'shape') and len(total_loss.shape) > 0:
            total_loss = jt.mean(total_loss)
        
        # æ£€æŸ¥æŸå¤±æ˜¯å¦æœ‰æ•ˆ
        if jt.isnan(total_loss) or jt.isinf(total_loss):
            return None
        
        # CPUæ¨¡å¼ä¸‹çš„æŸå¤±ç¼©æ”¾
        loss_value = float(total_loss)
        if loss_value > 100.0:
            total_loss = total_loss / 10.0
        elif loss_value > 50.0:
            total_loss = total_loss / 5.0
        
        return total_loss
        
    except Exception as e:
        print(f"âš ï¸ æŸå¤±è®¡ç®—å¤±è´¥: {e}")
        return None


def safe_target_processing(batch_targets):
    """å®‰å…¨çš„ç›®æ ‡å¤„ç† - ä¿®å¤æ•°æ®æ ¼å¼é—®é¢˜"""
    if not batch_targets:
        return jt.zeros((0, 6))
    
    try:
        # å¤„ç†æ¯ä¸ªç›®æ ‡å¼ é‡ï¼Œç¡®ä¿æ ¼å¼æ­£ç¡®
        processed_targets = []
        
        for target_tensor in batch_targets:
            # ç¡®ä¿æ˜¯å¼ é‡
            if not isinstance(target_tensor, jt.Var):
                target_tensor = jt.array(target_tensor)
            
            # ç¡®ä¿æ˜¯2ç»´çš„
            if len(target_tensor.shape) == 1:
                target_tensor = target_tensor.unsqueeze(0)
            elif len(target_tensor.shape) == 0:
                continue  # è·³è¿‡ç©ºç›®æ ‡
            
            # ç¡®ä¿æœ‰æ­£ç¡®çš„åˆ—æ•°
            if target_tensor.shape[1] >= 5:  # è‡³å°‘æœ‰batch_idx, class, x, y, w, h
                processed_targets.append(target_tensor)
        
        if processed_targets:
            return jt.concat(processed_targets, dim=0)
        else:
            return jt.zeros((0, 6))
            
    except Exception as e:
        print(f"âš ï¸ ç›®æ ‡å¤„ç†å¤±è´¥: {e}")
        return jt.zeros((0, 6))


def train_one_epoch_cpu(model, dataset, loss_fn, optimizer, epoch, batch_size=2):
    """CPUä¼˜åŒ–çš„è®­ç»ƒå‡½æ•°"""
    model.train()
    total_loss = 0
    successful_batches = 0
    failed_batches = 0
    
    dataset_size = len(dataset)
    # CPUæ¨¡å¼ä¸‹å¤„ç†è¾ƒå°‘æ‰¹æ¬¡
    max_batches = 30
    num_batches = min(max_batches, (dataset_size + batch_size - 1) // batch_size)
    
    print(f"ğŸ“Š Epoch {epoch+1}: æ•°æ®é›†å¤§å°={dataset_size}, è®­ç»ƒæ‰¹æ¬¡æ•°={num_batches}")
    
    pbar = tqdm(range(num_batches), desc=f'Epoch {epoch+1}')
    
    for batch_idx in pbar:
        try:
            # æ”¶é›†æ‰¹æ¬¡æ•°æ®
            batch_images = []
            batch_targets = []
            
            valid_samples = 0
            for i in range(batch_size):
                sample_idx = batch_idx * batch_size + i
                if sample_idx >= dataset_size:
                    break
                
                try:
                    dataset_output = dataset[sample_idx]
                    
                    if len(dataset_output) >= 2:
                        image = dataset_output[0]
                        target = dataset_output[1]
                        
                        if image is not None:
                            batch_images.append(image)
                            
                            # å®‰å…¨å¤„ç†ç›®æ ‡
                            if target is not None and len(target) > 0:
                                # ç¡®ä¿targetæ˜¯2ç»´çš„
                                if len(target.shape) == 1:
                                    target = target.unsqueeze(0)
                                
                                # æ·»åŠ æ‰¹æ¬¡ç´¢å¼•
                                batch_indices = jt.full((target.shape[0], 1), valid_samples)
                                target_with_batch = jt.concat([batch_indices, target], dim=1)
                                batch_targets.append(target_with_batch)
                            
                            valid_samples += 1
                            
                except Exception as e:
                    continue
            
            # æ£€æŸ¥æ˜¯å¦æœ‰æœ‰æ•ˆæ•°æ®
            if len(batch_images) == 0:
                failed_batches += 1
                continue
            
            # å †å å›¾åƒ - ä¿®å¤ï¼šç¡®ä¿æ•°æ®ç±»å‹æ­£ç¡®
            images = jt.stack(batch_images)
            if images.dtype != 'float32':
                images = images.float32()  # å¼ºåˆ¶è½¬æ¢ä¸ºfloat32
            
            # å®‰å…¨å¤„ç†ç›®æ ‡
            targets = safe_target_processing(batch_targets)
            
            # å‰å‘ä¼ æ’­
            outputs = model(images)
            
            # è®¡ç®—æŸå¤±
            loss = safe_loss_computation(loss_fn, outputs, targets, epoch, batch_idx)
            
            if loss is None:
                failed_batches += 1
                continue
            
            # åå‘ä¼ æ’­
            optimizer.zero_grad()
            optimizer.backward(loss)
            
            # CPUæ¨¡å¼ä¸‹ä½¿ç”¨æ¸©å’Œçš„æ¢¯åº¦è£å‰ª - ä¿®å¤Jittor API
            try:
                # æ·±å…¥ä¿®å¤æ¢¯åº¦è£å‰ª - æ­£ç¡®å¤„ç†å¼ é‡norm
                max_norm = 10.0
                total_norm = 0.0
                for param in model.parameters():
                    if param.opt_grad(optimizer) is not None:
                        grad = param.opt_grad(optimizer)
                        # æ·±å…¥ä¿®å¤ï¼šè®¡ç®—æ¢¯åº¦çš„L2èŒƒæ•°ï¼Œç¡®ä¿ç»“æœæ˜¯æ ‡é‡
                        param_norm_squared = jt.sum(grad * grad)  # è®¡ç®—å¹³æ–¹å’Œ
                        # è½¬æ¢ä¸ºPythonæ ‡é‡
                        if hasattr(param_norm_squared, 'data'):
                            norm_val = float(param_norm_squared.data)
                        else:
                            # ä½¿ç”¨numpyè½¬æ¢
                            norm_val = float(param_norm_squared.numpy())
                        total_norm += norm_val

                total_norm = (total_norm ** 0.5)  # å¼€å¹³æ–¹å¾—åˆ°L2èŒƒæ•°

                clip_coef = max_norm / (total_norm + 1e-6)
                if clip_coef < 1:
                    for param in model.parameters():
                        if param.opt_grad(optimizer) is not None:
                            param.opt_grad(optimizer).data.mul_(clip_coef)
            except:
                pass  # å¦‚æœæ¢¯åº¦è£å‰ªå¤±è´¥ï¼Œç»§ç»­è®­ç»ƒ
            
            optimizer.step()
            
            # æ›´æ–°ç»Ÿè®¡
            loss_value = float(loss)
            total_loss += loss_value
            successful_batches += 1
            
            # æ›´æ–°è¿›åº¦æ¡
            pbar.set_postfix({
                'Loss': f'{loss_value:.4f}',
                'Avg': f'{total_loss/successful_batches:.4f}',
                'Valid': f'{valid_samples}/{batch_size}'
            })
            
        except Exception as e:
            print(f"âš ï¸ Batch {batch_idx} è®­ç»ƒå¤±è´¥: {e}")
            failed_batches += 1
            continue
    
    avg_loss = total_loss / max(successful_batches, 1)
    success_rate = successful_batches / max(successful_batches + failed_batches, 1) * 100
    
    print(f"ğŸ“ˆ Epoch {epoch+1} å®Œæˆ: å¹³å‡æŸå¤±={avg_loss:.6f}, æˆåŠŸç‡={success_rate:.1f}%")
    
    return avg_loss


def main():
    """ä¸»è®­ç»ƒå‡½æ•°"""
    args = parse_args()
    
    print("ğŸš€ GOLD-YOLO Jittorç‰ˆæœ¬ - CPUè®­ç»ƒï¼ˆä¿®å¤æ•°æ®æ ¼å¼ï¼‰")
    print("=" * 60)
    
    try:
        # åŠ è½½æ•°æ®é…ç½®
        data_config = load_data_config(args.data)
        num_classes = data_config.get('nc', 20)
        
        # åˆ›å»ºæ•°æ®é›†
        train_dataset = create_simple_dataset(data_config, args.img_size, is_train=True)
        
        # åˆ›å»ºæ¨¡å‹å’ŒæŸå¤±å‡½æ•°
        model, loss_fn = create_model_and_loss(num_classes)
        
        # åˆ›å»ºä¼˜åŒ–å™¨ - CPUä¼˜åŒ–
        optimizer = jt.optim.SGD(
            model.parameters(), 
            lr=args.lr,
            momentum=0.9, 
            weight_decay=0.0001
        )
        
        # åˆ›å»ºä¿å­˜ç›®å½•
        save_dir = Path(args.project) / args.name
        save_dir.mkdir(parents=True, exist_ok=True)
        
        print(f"ğŸ’¾ æ¨¡å‹ä¿å­˜ç›®å½•: {save_dir}")
        print(f"ğŸ¯ å¼€å§‹CPUè®­ç»ƒ {args.epochs} è½®...")
        
        best_loss = float('inf')
        
        for epoch in range(args.epochs):
            avg_loss = train_one_epoch_cpu(
                model, train_dataset, loss_fn, optimizer, epoch, args.batch_size
            )
            
            # ä¿å­˜æœ€ä½³æ¨¡å‹
            if avg_loss < best_loss:
                best_loss = avg_loss
                best_model_path = str(save_dir / "best.pkl")
                try:
                    save_dict = {
                        'epoch': epoch + 1,
                        'loss': float(avg_loss),
                        'model_state_dict': model.state_dict(),
                        'optimizer_state_dict': optimizer.state_dict(),
                    }
                    jt.save(save_dict, best_model_path)
                    print(f"âœ… æœ€ä½³æ¨¡å‹ä¿å­˜: {best_model_path}")
                except Exception as e:
                    print(f"âš ï¸ æ¨¡å‹ä¿å­˜å¤±è´¥: {e}")
        
        print(f"ğŸ‰ CPUè®­ç»ƒå®Œæˆï¼æœ€ä½³æŸå¤±: {best_loss:.6f}")
        
    except Exception as e:
        print(f"âŒ è®­ç»ƒå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    main()
