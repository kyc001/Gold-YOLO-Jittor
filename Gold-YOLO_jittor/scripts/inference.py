#!/usr/bin/env python3
# -*- coding:utf-8 -*-
"""
ä½¿ç”¨å®Œæ•´YOLOè§£ç å™¨çš„æ¨ç†æµ‹è¯•
æ–°èŠ½ç¬¬äºŒé˜¶æ®µï¼šæ»¡è¡€æ¨ç†å®ç°ï¼Œä¸ç®€åŒ–ä¸å¦¥å
"""

import os
import sys
import json
import time
import numpy as np
from pathlib import Path

import jittor as jt
import jittor.nn as nn
from PIL import Image, ImageDraw, ImageFont
import matplotlib.pyplot as plt
import matplotlib.patches as patches

# è®¾ç½®Jittor
jt.flags.use_cuda = 1

# å¯¼å…¥å®Œæ•´æ¨¡å‹å’Œè§£ç å™¨
from full_pytorch_small_model import FullPyTorchGoldYOLOSmall
from full_yolo_decoder import FullYOLODecoder

class FullGoldYOLOInference:
    """ä½¿ç”¨å®Œæ•´è§£ç å™¨çš„Gold-YOLOæ¨ç†å™¨"""
    
    def __init__(self, model_path, conf_threshold=0.3, nms_threshold=0.5):
        self.model_path = Path(model_path)
        self.conf_threshold = conf_threshold
        self.nms_threshold = nms_threshold
        
        # åŠ è½½æ¨¡å‹
        self.model = self._load_model()
        self.model.eval()
        
        # åˆ›å»ºå®Œæ•´è§£ç å™¨
        self.decoder = FullYOLODecoder(
            input_size=640,
            num_classes=80,
            strides=[8, 16, 32]
        )
        
        print(f"ğŸ¯ å®Œæ•´Gold-YOLOæ¨ç†å™¨åˆå§‹åŒ–å®Œæˆ")
        print(f"   æ¨¡å‹: {self.model_path}")
        print(f"   ç½®ä¿¡åº¦é˜ˆå€¼: {self.conf_threshold}")
        print(f"   NMSé˜ˆå€¼: {self.nms_threshold}")
        print(f"   è§£ç å™¨anchoræ•°: {sum(len(grid) for grid in self.decoder.anchor_grids)}")
    
    def _load_model(self):
        """åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹"""
        model = FullPyTorchGoldYOLOSmall(num_classes=80)
        
        if self.model_path.exists():
            try:
                checkpoint = jt.load(str(self.model_path))
                if isinstance(checkpoint, dict) and 'model' in checkpoint:
                    model.load_state_dict(checkpoint['model'])
                    print(f"âœ… æˆåŠŸåŠ è½½æ¨¡å‹æƒé‡: {self.model_path}")
                    if 'training_info' in checkpoint:
                        print(f"   è®­ç»ƒè½®æ¬¡: {checkpoint['training_info'].get('epoch', 'unknown')}")
                        print(f"   æœ€ä½³æŸå¤±: {checkpoint['training_info'].get('best_loss', 'unknown')}")
                else:
                    model.load_state_dict(checkpoint)
                    print(f"âœ… æˆåŠŸåŠ è½½æ¨¡å‹æƒé‡: {self.model_path}")
            except Exception as e:
                print(f"âš ï¸ åŠ è½½æƒé‡å¤±è´¥: {e}")
                print(f"ä½¿ç”¨éšæœºåˆå§‹åŒ–æƒé‡è¿›è¡Œæ¨ç†æµ‹è¯•")
        else:
            print(f"âš ï¸ æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {self.model_path}")
            print(f"ä½¿ç”¨éšæœºåˆå§‹åŒ–æƒé‡è¿›è¡Œæ¨ç†æµ‹è¯•")
        
        return model
    
    def preprocess_image(self, image_path, target_size=640):
        """é¢„å¤„ç†å›¾ç‰‡"""
        # è¯»å–å›¾ç‰‡
        image = Image.open(image_path).convert('RGB')
        original_size = image.size
        
        # ç­‰æ¯”ä¾‹ç¼©æ”¾åˆ°ç›®æ ‡å°ºå¯¸
        ratio = min(target_size / original_size[0], target_size / original_size[1])
        new_size = (int(original_size[0] * ratio), int(original_size[1] * ratio))
        image_resized = image.resize(new_size, Image.LANCZOS)
        
        # åˆ›å»ºç›®æ ‡å°ºå¯¸çš„ç”»å¸ƒå¹¶å±…ä¸­æ”¾ç½®
        canvas = Image.new('RGB', (target_size, target_size), (114, 114, 114))
        paste_x = (target_size - new_size[0]) // 2
        paste_y = (target_size - new_size[1]) // 2
        canvas.paste(image_resized, (paste_x, paste_y))
        
        # è½¬æ¢ä¸ºtensor
        image_array = np.array(canvas).astype(np.float32) / 255.0
        image_tensor = jt.array(image_array.transpose(2, 0, 1)).unsqueeze(0)
        
        return image_tensor, image, ratio, (paste_x, paste_y)
    
    def postprocess_detections(self, detections, ratio, offset, original_size):
        """åå¤„ç†æ£€æµ‹ç»“æœï¼Œè½¬æ¢å›åŸå›¾åæ ‡"""
        paste_x, paste_y = offset
        
        processed_detections = []
        for detection in detections:
            # è·å–640x640å›¾åƒä¸­çš„åæ ‡
            x1, y1, x2, y2 = detection['bbox']
            
            # è½¬æ¢å›åŸå›¾åæ ‡
            # 1. å‡å»paddingåç§»
            x1 -= paste_x
            y1 -= paste_y
            x2 -= paste_x
            y2 -= paste_y
            
            # 2. ç¼©æ”¾å›åŸå›¾å°ºå¯¸
            x1 /= ratio
            y1 /= ratio
            x2 /= ratio
            y2 /= ratio
            
            # 3. é™åˆ¶åœ¨åŸå›¾èŒƒå›´å†…
            x1 = max(0, min(x1, original_size[0]))
            y1 = max(0, min(y1, original_size[1]))
            x2 = max(0, min(x2, original_size[0]))
            y2 = max(0, min(y2, original_size[1]))
            
            # è¿‡æ»¤æ— æ•ˆæ¡†
            if x2 > x1 and y2 > y1:
                processed_detections.append({
                    'bbox': [x1, y1, x2, y2],
                    'confidence': detection['confidence'],
                    'class_id': detection['class_id'],
                    'class_name': detection['class_name']
                })
        
        return processed_detections
    
    def visualize_detections(self, image, detections, output_path):
        """å¯è§†åŒ–æ£€æµ‹ç»“æœ"""
        # åˆ›å»ºmatplotlibå›¾åƒ
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 10))
        
        # å·¦ä¾§ï¼šåŸå›¾
        ax1.imshow(image)
        ax1.set_title('Original Image', fontsize=16)
        ax1.axis('off')
        
        # å³ä¾§ï¼šæ£€æµ‹ç»“æœ
        ax2.imshow(image)
        ax2.set_title(f'Full YOLO Decoder Results ({len(detections)} objects)', fontsize=16)
        ax2.axis('off')
        
        # é¢œè‰²åˆ—è¡¨
        colors = plt.cm.Set3(np.linspace(0, 1, max(len(detections), 1)))
        
        # ç»˜åˆ¶æ£€æµ‹æ¡†
        for i, detection in enumerate(detections):
            bbox = detection['bbox']
            confidence = detection['confidence']
            class_name = detection['class_name']
            
            # è®¡ç®—æ¡†çš„åæ ‡å’Œå°ºå¯¸
            x1, y1, x2, y2 = bbox
            width = x2 - x1
            height = y2 - y1
            
            # ç»˜åˆ¶è¾¹ç•Œæ¡†
            rect = patches.Rectangle((x1, y1), width, height, 
                                   linewidth=3, edgecolor=colors[i], 
                                   facecolor='none', alpha=0.8)
            ax2.add_patch(rect)
            
            # ç»˜åˆ¶æ ‡ç­¾
            label = f"{class_name}: {confidence:.3f}"
            ax2.text(x1, y1-5, label, fontsize=12, 
                    bbox=dict(boxstyle="round,pad=0.3", facecolor=colors[i], alpha=0.8),
                    color='black', weight='bold')
        
        # ä¿å­˜å›¾åƒ
        plt.tight_layout()
        plt.savefig(output_path, dpi=150, bbox_inches='tight')
        plt.close()
        
        print(f"âœ… å®Œæ•´è§£ç å¯è§†åŒ–ç»“æœå·²ä¿å­˜: {output_path}")
    
    def inference_single_image(self, image_path, output_dir=None):
        """å¯¹å•å¼ å›¾ç‰‡è¿›è¡Œå®Œæ•´æ¨ç†"""
        image_path = Path(image_path)
        
        if output_dir is None:
            output_dir = Path("runs/full_inference_results")
        else:
            output_dir = Path(output_dir)
        output_dir.mkdir(parents=True, exist_ok=True)
        
        print(f"\nğŸ” å®Œæ•´æ¨ç†: {image_path}")
        
        # é¢„å¤„ç†
        start_time = time.time()
        image_tensor, original_image, ratio, offset = self.preprocess_image(image_path)
        preprocess_time = time.time() - start_time
        
        # æ¨¡å‹æ¨ç†
        start_time = time.time()
        with jt.no_grad():
            features, cls_pred, reg_pred = self.model(image_tensor)
        model_inference_time = time.time() - start_time
        
        # å®Œæ•´è§£ç 
        start_time = time.time()
        batch_detections = self.decoder.decode_predictions(
            cls_pred, reg_pred,
            conf_threshold=self.conf_threshold,
            nms_threshold=self.nms_threshold,
            max_detections=100
        )
        decode_time = time.time() - start_time
        
        # åå¤„ç†ï¼ˆåæ ‡è½¬æ¢ï¼‰
        start_time = time.time()
        detections = self.postprocess_detections(
            batch_detections[0], ratio, offset, original_image.size
        )
        postprocess_time = time.time() - start_time
        
        # å¯è§†åŒ–
        start_time = time.time()
        vis_output_path = output_dir / f"full_detection_{image_path.stem}.png"
        self.visualize_detections(original_image, detections, vis_output_path)
        visualization_time = time.time() - start_time
        
        # æ‰“å°ç»“æœ
        total_time = preprocess_time + model_inference_time + decode_time + postprocess_time + visualization_time
        print(f"â±ï¸ å®Œæ•´æ¨ç†æ—¶é—´ç»Ÿè®¡:")
        print(f"   é¢„å¤„ç†: {preprocess_time*1000:.2f} ms")
        print(f"   æ¨¡å‹æ¨ç†: {model_inference_time*1000:.2f} ms")
        print(f"   å®Œæ•´è§£ç : {decode_time*1000:.2f} ms")
        print(f"   åå¤„ç†: {postprocess_time*1000:.2f} ms")
        print(f"   å¯è§†åŒ–: {visualization_time*1000:.2f} ms")
        print(f"   æ€»æ—¶é—´: {total_time*1000:.2f} ms")
        print(f"   FPS: {1/total_time:.1f}")
        
        print(f"ğŸ¯ å®Œæ•´è§£ç æ£€æµ‹ç»“æœ: {len(detections)}ä¸ªç›®æ ‡")
        for i, det in enumerate(detections):
            print(f"   {i+1}. {det['class_name']}: {det['confidence']:.3f}")
        
        # ä¿å­˜è¯¦ç»†ç»“æœ
        results = {
            'image_path': str(image_path),
            'detections': detections,
            'timing': {
                'preprocess_ms': preprocess_time * 1000,
                'model_inference_ms': model_inference_time * 1000,
                'decode_ms': decode_time * 1000,
                'postprocess_ms': postprocess_time * 1000,
                'visualization_ms': visualization_time * 1000,
                'total_ms': total_time * 1000,
                'fps': 1 / total_time
            },
            'model_info': {
                'input_shape': list(image_tensor.shape),
                'cls_output_shape': list(cls_pred.shape),
                'reg_output_shape': list(reg_pred.shape),
                'num_features': len(features),
                'decoder_anchors': sum(len(grid) for grid in self.decoder.anchor_grids)
            }
        }
        
        results_file = output_dir / f"full_results_{image_path.stem}.json"
        with open(results_file, 'w') as f:
            json.dump(results, f, indent=2)
        
        return detections, results
    
    def batch_full_inference(self, image_dir, max_images=3):
        """æ‰¹é‡å®Œæ•´æ¨ç†"""
        image_dir = Path(image_dir)
        output_dir = Path("runs/batch_full_inference")
        output_dir.mkdir(parents=True, exist_ok=True)
        
        # æŸ¥æ‰¾å›¾ç‰‡æ–‡ä»¶
        image_extensions = ['.jpg', '.jpeg', '.png', '.bmp']
        image_files = []
        for ext in image_extensions:
            image_files.extend(list(image_dir.glob(f"*{ext}")))
            image_files.extend(list(image_dir.glob(f"*{ext.upper()}")))
        
        image_files = image_files[:max_images]
        
        print(f"\nğŸ¯ æ‰¹é‡å®Œæ•´æ¨ç†")
        print(f"   å›¾ç‰‡ç›®å½•: {image_dir}")
        print(f"   æ‰¾åˆ°å›¾ç‰‡: {len(image_files)}å¼ ")
        print(f"   è¾“å‡ºç›®å½•: {output_dir}")
        
        if not image_files:
            print("âŒ æœªæ‰¾åˆ°å›¾ç‰‡æ–‡ä»¶")
            return
        
        all_results = []
        total_detections = 0
        
        for i, image_file in enumerate(image_files):
            print(f"\n--- å®Œæ•´æ¨ç† {i+1}/{len(image_files)} ---")
            detections, results = self.inference_single_image(image_file, output_dir)
            all_results.append(results)
            total_detections += len(detections)
        
        # åˆ›å»ºæ±‡æ€»æŠ¥å‘Š
        summary = {
            'total_images': len(image_files),
            'total_detections': total_detections,
            'avg_detections_per_image': total_detections / len(image_files),
            'avg_model_inference_time_ms': np.mean([r['timing']['model_inference_ms'] for r in all_results]),
            'avg_decode_time_ms': np.mean([r['timing']['decode_ms'] for r in all_results]),
            'avg_total_time_ms': np.mean([r['timing']['total_ms'] for r in all_results]),
            'avg_fps': np.mean([r['timing']['fps'] for r in all_results]),
            'results': all_results
        }
        
        summary_file = output_dir / "full_batch_summary.json"
        with open(summary_file, 'w') as f:
            json.dump(summary, f, indent=2)
        
        print(f"\nğŸ“Š æ‰¹é‡å®Œæ•´æ¨ç†æ±‡æ€»:")
        print(f"   å¤„ç†å›¾ç‰‡: {len(image_files)}å¼ ")
        print(f"   æ€»æ£€æµ‹æ•°: {total_detections}ä¸ª")
        print(f"   å¹³å‡æ¯å¼ : {total_detections/len(image_files):.1f}ä¸ª")
        print(f"   å¹³å‡æ¨¡å‹æ¨ç†: {summary['avg_model_inference_time_ms']:.2f} ms")
        print(f"   å¹³å‡å®Œæ•´è§£ç : {summary['avg_decode_time_ms']:.2f} ms")
        print(f"   å¹³å‡æ€»æ—¶é—´: {summary['avg_total_time_ms']:.2f} ms")
        print(f"   å¹³å‡FPS: {summary['avg_fps']:.1f}")
        print(f"âœ… å®Œæ•´æ¨ç†æ±‡æ€»æŠ¥å‘Šå·²ä¿å­˜: {summary_file}")


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ¯ Gold-YOLO å®Œæ•´è§£ç æ¨ç†æµ‹è¯•")
    print("æ–°èŠ½ç¬¬äºŒé˜¶æ®µï¼šæ»¡è¡€YOLOè§£ç ï¼Œä¸ç®€åŒ–ä¸å¦¥å")
    print("=" * 60)
    
    # æ¨¡å‹è·¯å¾„ (ä½¿ç”¨ä¿®å¤åçš„æ¨¡å‹)
    model_path = "runs/validated_full_pytorch_small/fixed_best_model.pkl"
    
    # åˆ›å»ºå®Œæ•´æ¨ç†å™¨
    inferencer = FullGoldYOLOInference(
        model_path=model_path, 
        conf_threshold=0.3, 
        nms_threshold=0.5
    )
    
    # ä½¿ç”¨æµ‹è¯•é›†ä¸­çš„å›¾ç‰‡è¿›è¡Œå®Œæ•´æ¨ç†
    test_image_dir = "/home/kyc/project/GOLD-YOLO/data/coco2017_val/images"
    if Path(test_image_dir).exists():
        print(f"ä½¿ç”¨æµ‹è¯•é›†å›¾ç‰‡è¿›è¡Œå®Œæ•´æ¨ç†: {test_image_dir}")
        inferencer.batch_full_inference(test_image_dir, max_images=3)
        
        print(f"\nğŸ‰ å®Œæ•´è§£ç æ¨ç†æµ‹è¯•å®Œæˆï¼")
        print(f"ğŸ“ ç»“æœä¿å­˜åœ¨: runs/batch_full_inference/")
        print(f"ğŸ“Š åŒ…å«:")
        print(f"   - ä½¿ç”¨å®Œæ•´YOLOè§£ç å™¨çš„æ£€æµ‹ç»“æœ")
        print(f"   - çœŸå®çš„anchorç”Ÿæˆå’ŒNMSå¤„ç†")
        print(f"   - å®Œæ•´çš„è¾¹ç•Œæ¡†è§£ç ç®—æ³•")
        print(f"   - è¯¦ç»†çš„æ€§èƒ½åˆ†ææŠ¥å‘Š")
        
    else:
        print("âŒ æµ‹è¯•é›†å›¾ç‰‡ç›®å½•ä¸å­˜åœ¨")
        print("è¯·ç¡®ä¿æ•°æ®é›†å·²æ­£ç¡®ä¸‹è½½å’Œé…ç½®")


if __name__ == "__main__":
    main()
