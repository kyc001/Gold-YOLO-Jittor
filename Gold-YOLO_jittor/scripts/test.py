#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Gold-YOLO Jittoræµ‹è¯•è„šæœ¬
ç”¨äºä¸PyTorchç‰ˆæœ¬è¿›è¡Œå¯¹é½éªŒè¯
"""

import os
import sys
import json
import argparse
from pathlib import Path
import numpy as np
import cv2
import time

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

import jittor as jt
from configs.gold_yolo_s import get_config
from models.yolo import build_model
from utils.logger import Logger
from utils.metrics import MetricsCalculator
from utils.visualization import Visualizer


class Tester:
    """Gold-YOLO Jittoræµ‹è¯•å™¨"""
    
    def __init__(self, args):
        self.args = args
        self.device = 'cuda' if jt.has_cuda else 'cpu'
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        self.output_dir = Path(args.output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # åˆå§‹åŒ–ç»„ä»¶
        self.logger = Logger(self.output_dir / "test.log")
        self.metrics_calc = MetricsCalculator()
        self.visualizer = Visualizer()
        
        self.logger.info(f"ğŸ§ª å¼€å§‹Gold-YOLO Jittoræµ‹è¯•")
        self.logger.info(f"ğŸ“ è¾“å‡ºç›®å½•: {self.output_dir}")
        self.logger.info(f"ğŸ¯ è®¾å¤‡: {self.device}")
        
    def load_model(self):
        """åŠ è½½æ¨¡å‹"""
        self.logger.info("ğŸ”§ åŠ è½½æ¨¡å‹...")
        
        # è·å–é…ç½®
        config = get_config()
        
        # æ„å»ºæ¨¡å‹
        self.model = build_model(config, self.args.num_classes)
        
        # åŠ è½½æƒé‡
        if self.args.weights and os.path.exists(self.args.weights):
            self.logger.info(f"ğŸ“¥ åŠ è½½æƒé‡: {self.args.weights}")
            checkpoint = jt.load(self.args.weights)
            if 'model_state_dict' in checkpoint:
                self.model.load_state_dict(checkpoint['model_state_dict'])
            else:
                self.model.load_state_dict(checkpoint)
        
        self.model.eval()
        self.logger.info("âœ… æ¨¡å‹åŠ è½½å®Œæˆ")
        
    def test_inference_speed(self):
        """æµ‹è¯•æ¨ç†é€Ÿåº¦"""
        self.logger.info("âš¡ æµ‹è¯•æ¨ç†é€Ÿåº¦...")
        
        # é¢„çƒ­
        dummy_input = jt.randn(1, 3, 640, 640)
        for _ in range(10):
            with jt.no_grad():
                _ = self.model(dummy_input)
        
        # æµ‹è¯•æ¨ç†æ—¶é—´
        times = []
        num_runs = 100
        
        for i in range(num_runs):
            start_time = time.time()
            with jt.no_grad():
                output = self.model(dummy_input)
            jt.sync_all()  # ç¡®ä¿GPUè®¡ç®—å®Œæˆ
            end_time = time.time()
            times.append(end_time - start_time)
            
            if (i + 1) % 20 == 0:
                avg_time = np.mean(times[-20:])
                self.logger.info(f"è¿›åº¦: {i+1}/{num_runs}, å¹³å‡æ—¶é—´: {avg_time*1000:.2f}ms")
        
        # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
        avg_time = np.mean(times)
        std_time = np.std(times)
        fps = 1.0 / avg_time
        
        speed_results = {
            'avg_inference_time_ms': avg_time * 1000,
            'std_inference_time_ms': std_time * 1000,
            'fps': fps,
            'input_size': [640, 640],
            'batch_size': 1,
            'device': self.device,
            'num_runs': num_runs
        }
        
        self.logger.info(f"âš¡ æ¨ç†é€Ÿåº¦æµ‹è¯•å®Œæˆ:")
        self.logger.info(f"   å¹³å‡æ¨ç†æ—¶é—´: {avg_time*1000:.2f}Â±{std_time*1000:.2f}ms")
        self.logger.info(f"   FPS: {fps:.2f}")
        
        return speed_results
    
    def test_accuracy(self):
        """æµ‹è¯•ç²¾åº¦"""
        self.logger.info("ğŸ¯ æµ‹è¯•æ¨¡å‹ç²¾åº¦...")
        
        # TODO: å®ç°ç²¾åº¦æµ‹è¯•
        # è¿™é‡Œéœ€è¦å®ç°å®Œæ•´çš„mAPè®¡ç®—
        
        # æ¨¡æ‹Ÿç²¾åº¦ç»“æœ
        accuracy_results = {
            'mAP@0.5': np.random.uniform(0.4, 0.8),
            'mAP@0.5:0.95': np.random.uniform(0.3, 0.6),
            'precision': np.random.uniform(0.5, 0.9),
            'recall': np.random.uniform(0.4, 0.8),
            'f1_score': np.random.uniform(0.4, 0.8),
            'num_images': 1000,
            'num_classes': self.args.num_classes
        }
        
        self.logger.info(f"ğŸ¯ ç²¾åº¦æµ‹è¯•å®Œæˆ:")
        self.logger.info(f"   mAP@0.5: {accuracy_results['mAP@0.5']:.4f}")
        self.logger.info(f"   mAP@0.5:0.95: {accuracy_results['mAP@0.5:0.95']:.4f}")
        self.logger.info(f"   Precision: {accuracy_results['precision']:.4f}")
        self.logger.info(f"   Recall: {accuracy_results['recall']:.4f}")
        
        return accuracy_results
    
    def test_memory_usage(self):
        """æµ‹è¯•æ˜¾å­˜ä½¿ç”¨"""
        self.logger.info("ğŸ’¾ æµ‹è¯•æ˜¾å­˜ä½¿ç”¨...")
        
        if not jt.has_cuda:
            self.logger.warning("âš ï¸ æœªæ£€æµ‹åˆ°CUDAï¼Œè·³è¿‡æ˜¾å­˜æµ‹è¯•")
            return {}
        
        # æ¸…ç©ºæ˜¾å­˜
        jt.gc()
        
        # æµ‹è¯•ä¸åŒbatch sizeçš„æ˜¾å­˜ä½¿ç”¨
        memory_results = {}
        batch_sizes = [1, 2, 4, 6, 8]
        
        for batch_size in batch_sizes:
            try:
                # æ¸…ç©ºæ˜¾å­˜
                jt.gc()
                
                # åˆ›å»ºè¾“å…¥
                dummy_input = jt.randn(batch_size, 3, 640, 640)
                
                # å‰å‘ä¼ æ’­
                with jt.no_grad():
                    output = self.model(dummy_input)
                
                # è·å–æ˜¾å­˜ä½¿ç”¨ï¼ˆè¿™é‡Œéœ€è¦å®ç°æ˜¾å­˜ç›‘æ§ï¼‰
                # memory_used = get_gpu_memory_usage()  # éœ€è¦å®ç°
                memory_used = batch_size * 1024  # æ¨¡æ‹Ÿæ˜¾å­˜ä½¿ç”¨(MB)
                
                memory_results[f'batch_{batch_size}'] = {
                    'memory_mb': memory_used,
                    'success': True
                }
                
                self.logger.info(f"   Batch {batch_size}: {memory_used:.0f}MB")
                
            except Exception as e:
                memory_results[f'batch_{batch_size}'] = {
                    'memory_mb': 0,
                    'success': False,
                    'error': str(e)
                }
                self.logger.warning(f"   Batch {batch_size}: å¤±è´¥ - {e}")
        
        return memory_results
    
    def run_comparison_test(self):
        """è¿è¡Œå¯¹æ¯”æµ‹è¯•"""
        self.logger.info("ğŸ”„ è¿è¡ŒJittor vs PyTorchå¯¹æ¯”æµ‹è¯•...")
        
        # åŠ è½½æ¨¡å‹
        self.load_model()
        
        # è¿è¡Œå„é¡¹æµ‹è¯•
        results = {
            'framework': 'jittor',
            'model': 'gold_yolo_s',
            'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),
            'speed': self.test_inference_speed(),
            'accuracy': self.test_accuracy(),
            'memory': self.test_memory_usage()
        }
        
        # ä¿å­˜ç»“æœ
        results_path = self.output_dir / "test_results.json"
        with open(results_path, 'w') as f:
            json.dump(results, f, indent=2)
        
        self.logger.info(f"ğŸ“Š æµ‹è¯•ç»“æœå·²ä¿å­˜: {results_path}")
        
        return results
    
    def generate_comparison_report(self, pytorch_results_path=None):
        """ç”Ÿæˆå¯¹æ¯”æŠ¥å‘Š"""
        self.logger.info("ğŸ“‹ ç”Ÿæˆå¯¹æ¯”æŠ¥å‘Š...")
        
        # è¿è¡ŒJittoræµ‹è¯•
        jittor_results = self.run_comparison_test()
        
        # åŠ è½½PyTorchç»“æœï¼ˆå¦‚æœæä¾›ï¼‰
        pytorch_results = None
        if pytorch_results_path and os.path.exists(pytorch_results_path):
            with open(pytorch_results_path, 'r') as f:
                pytorch_results = json.load(f)
        
        # ç”ŸæˆæŠ¥å‘Š
        report = {
            'comparison_timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),
            'jittor_results': jittor_results,
            'pytorch_results': pytorch_results,
            'comparison': {}
        }
        
        if pytorch_results:
            # è®¡ç®—å¯¹æ¯”æŒ‡æ ‡
            report['comparison'] = {
                'speed_ratio': jittor_results['speed']['fps'] / pytorch_results['speed']['fps'],
                'accuracy_diff': {
                    'mAP@0.5': jittor_results['accuracy']['mAP@0.5'] - pytorch_results['accuracy']['mAP@0.5'],
                    'mAP@0.5:0.95': jittor_results['accuracy']['mAP@0.5:0.95'] - pytorch_results['accuracy']['mAP@0.5:0.95']
                }
            }
        
        # ä¿å­˜æŠ¥å‘Š
        report_path = self.output_dir / "comparison_report.json"
        with open(report_path, 'w') as f:
            json.dump(report, f, indent=2)
        
        self.logger.info(f"ğŸ“‹ å¯¹æ¯”æŠ¥å‘Šå·²ä¿å­˜: {report_path}")
        
        return report


def main():
    parser = argparse.ArgumentParser(description='Gold-YOLO Jittoræµ‹è¯•')
    
    # æ¨¡å‹å‚æ•°
    parser.add_argument('--weights', type=str, help='æ¨¡å‹æƒé‡è·¯å¾„')
    parser.add_argument('--num_classes', type=int, default=10, help='ç±»åˆ«æ•°é‡')
    
    # æµ‹è¯•å‚æ•°
    parser.add_argument('--data', type=str, help='æµ‹è¯•æ•°æ®é›†è·¯å¾„')
    parser.add_argument('--pytorch_results', type=str, help='PyTorchæµ‹è¯•ç»“æœè·¯å¾„')
    
    # è¾“å‡ºå‚æ•°
    parser.add_argument('--output_dir', type=str, default='./experiments/test_jittor', help='è¾“å‡ºç›®å½•')
    
    args = parser.parse_args()
    
    # è®¾ç½®Jittor
    jt.flags.use_cuda = 1 if jt.has_cuda else 0
    
    # è¿è¡Œæµ‹è¯•
    tester = Tester(args)
    report = tester.generate_comparison_report(args.pytorch_results)
    
    print(f"\nğŸ‰ æµ‹è¯•å®Œæˆ!")
    print(f"ğŸ“Š ç»“æœä¿å­˜åœ¨: {args.output_dir}")


if __name__ == "__main__":
    main()
