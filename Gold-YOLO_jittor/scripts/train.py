#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Gold-YOLO Jittorè®­ç»ƒè„šæœ¬
ç”¨äºä¸PyTorchç‰ˆæœ¬è¿›è¡Œå¯¹é½å®éªŒ
"""

import os
import sys
import time
import json
import argparse
from pathlib import Path
import numpy as np
from datetime import datetime

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

import jittor as jt
from jittor import nn

from configs.gold_yolo_s import get_config
from models.yolo import build_model
from models.loss import GoldYOLOLoss
from utils.logger import Logger
from utils.metrics import MetricsCalculator


class Trainer:
    """Gold-YOLO Jittorè®­ç»ƒå™¨"""
    
    def __init__(self, config, args):
        self.config = config
        self.args = args
        self.device = 'cuda' if jt.has_cuda else 'cpu'
        
        # è®¾ç½®éšæœºç§å­
        np.random.seed(args.seed)
        jt.set_global_seed(args.seed)
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        self.output_dir = Path(args.output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # åˆå§‹åŒ–æ—¥å¿—
        self.logger = Logger(self.output_dir / "train.log")
        self.metrics_calc = MetricsCalculator()
        
        # è®­ç»ƒçŠ¶æ€
        self.epoch = 0
        self.best_map = 0.0
        self.train_losses = []
        self.val_metrics = []
        
        self.logger.info(f"ğŸš€ å¼€å§‹Gold-YOLO Jittorè®­ç»ƒ")
        self.logger.info(f"ğŸ“ è¾“å‡ºç›®å½•: {self.output_dir}")
        self.logger.info(f"ğŸ¯ è®¾å¤‡: {self.device}")
        
    def build_model(self):
        """æ„å»ºæ¨¡å‹"""
        self.logger.info("ğŸ”§ æ„å»ºæ¨¡å‹...")
        
        # æ„å»ºæ¨¡å‹
        self.model = build_model(self.config, self.args.num_classes)

        # æ„å»ºæŸå¤±å‡½æ•°
        self.criterion = GoldYOLOLoss(num_classes=self.args.num_classes, reg_max=16, use_dfl=True)

        # åŠ è½½é¢„è®­ç»ƒæƒé‡
        if self.args.pretrained and os.path.exists(self.args.pretrained):
            self.logger.info(f"ğŸ“¥ åŠ è½½é¢„è®­ç»ƒæƒé‡: {self.args.pretrained}")
            # TODO: å®ç°æƒé‡åŠ è½½

        self.logger.info(f"âœ… æ¨¡å‹æ„å»ºå®Œæˆ")

        # æ‰“å°æ¨¡å‹ä¿¡æ¯
        total_params = sum(p.numel() for p in self.model.parameters())
        self.logger.info(f"ğŸ“Š æ¨¡å‹å‚æ•°é‡: {total_params:,}")
        
    def build_dataloader(self):
        """æ„å»ºæ•°æ®åŠ è½½å™¨"""
        self.logger.info("ğŸ“š æ„å»ºæ•°æ®åŠ è½½å™¨...")
        
        # TODO: å®ç°æ•°æ®åŠ è½½å™¨
        # è¿™é‡Œéœ€è¦å®ç°YOLOæ ¼å¼çš„æ•°æ®åŠ è½½å™¨
        self.logger.info("âœ… æ•°æ®åŠ è½½å™¨æ„å»ºå®Œæˆ")
        
    def build_optimizer(self):
        """æ„å»ºä¼˜åŒ–å™¨"""
        self.logger.info("âš™ï¸ æ„å»ºä¼˜åŒ–å™¨...")
        
        # å‚æ•°åˆ†ç»„
        pg0, pg1, pg2 = [], [], []  # optimizer parameter groups
        for k, v in self.model.named_modules():
            if hasattr(v, 'bias') and isinstance(v.bias, nn.Parameter):
                pg2.append(v.bias)  # biases
            if isinstance(v, nn.BatchNorm2d):
                pg0.append(v.weight)  # no decay
            elif hasattr(v, 'weight') and isinstance(v.weight, nn.Parameter):
                pg1.append(v.weight)  # apply decay
        
        # æ„å»ºä¼˜åŒ–å™¨
        if self.args.optimizer == 'SGD':
            self.optimizer = jt.optim.SGD(
                pg0, lr=self.args.lr, momentum=self.args.momentum, nesterov=True
            )
            self.optimizer.add_param_group({'params': pg1, 'weight_decay': self.args.weight_decay})
            self.optimizer.add_param_group({'params': pg2})
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„ä¼˜åŒ–å™¨: {self.args.optimizer}")
        
        self.logger.info(f"âœ… ä¼˜åŒ–å™¨æ„å»ºå®Œæˆ: {self.args.optimizer}")
        
    def train_epoch(self):
        """è®­ç»ƒä¸€ä¸ªepoch"""
        self.model.train()
        epoch_loss = 0.0
        num_batches = 0

        start_time = time.time()

        # æ¨¡æ‹Ÿè®­ç»ƒå¾ªç¯ï¼ˆä½¿ç”¨çœŸå®çš„æŸå¤±å‡½æ•°ï¼‰
        for batch_idx in range(10):  # æ¨¡æ‹Ÿ10ä¸ªbatch
            # åˆ›å»ºæ¨¡æ‹Ÿæ•°æ®
            images = jt.randn(self.args.batch_size, 3, 512, 512)

            # åˆ›å»ºæ¨¡æ‹Ÿæ ‡ç­¾ï¼ˆYOLOæ ¼å¼ï¼‰
            batch_size = images.shape[0]
            max_objects = 5

            # æ¨¡æ‹ŸçœŸå®çš„batchæ ¼å¼
            batch = {
                'cls': jt.randint(0, self.args.num_classes, (batch_size, max_objects)),
                'bboxes': jt.rand(batch_size, max_objects, 4),  # normalized xywh
                'mask_gt': jt.ones(batch_size, max_objects).bool()
            }

            # å‰å‘ä¼ æ’­
            outputs = self.model(images)

            # è®¡ç®—æŸå¤±
            loss, loss_items = self.criterion(outputs, batch)

            # åå‘ä¼ æ’­
            self.optimizer.step(loss)

            epoch_loss += loss.item()
            num_batches += 1

            if batch_idx % 5 == 0:
                self.logger.info(f"  Batch {batch_idx}: Loss = {loss.item():.4f}")

        avg_loss = epoch_loss / max(num_batches, 1)
        epoch_time = time.time() - start_time

        self.train_losses.append(avg_loss)

        self.logger.info(
            f"Epoch [{self.epoch}/{self.args.epochs}] "
            f"Loss: {avg_loss:.4f} "
            f"Time: {epoch_time:.2f}s"
        )

        return avg_loss
    
    def validate(self):
        """éªŒè¯æ¨¡å‹"""
        self.model.eval()
        
        # TODO: å®ç°éªŒè¯é€»è¾‘
        # æ¨¡æ‹ŸéªŒè¯ç»“æœ
        val_metrics = {
            'mAP@0.5': np.random.uniform(0.3, 0.8),
            'mAP@0.5:0.95': np.random.uniform(0.2, 0.6),
            'precision': np.random.uniform(0.4, 0.9),
            'recall': np.random.uniform(0.3, 0.8)
        }
        
        self.val_metrics.append(val_metrics)
        
        self.logger.info(
            f"Validation - "
            f"mAP@0.5: {val_metrics['mAP@0.5']:.4f} "
            f"mAP@0.5:0.95: {val_metrics['mAP@0.5:0.95']:.4f}"
        )
        
        return val_metrics
    
    def save_checkpoint(self, is_best=False):
        """ä¿å­˜æ£€æŸ¥ç‚¹"""
        checkpoint = {
            'epoch': self.epoch,
            'model_state_dict': self.model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'best_map': self.best_map,
            'train_losses': self.train_losses,
            'val_metrics': self.val_metrics,
            'config': self.config.__dict__ if hasattr(self.config, '__dict__') else str(self.config)
        }
        
        # ä¿å­˜æœ€æ–°æ£€æŸ¥ç‚¹
        checkpoint_path = self.output_dir / "last.pkl"
        jt.save(checkpoint, checkpoint_path)
        
        # ä¿å­˜æœ€ä½³æ£€æŸ¥ç‚¹
        if is_best:
            best_path = self.output_dir / "best.pkl"
            jt.save(checkpoint, best_path)
            self.logger.info(f"ğŸ’¾ ä¿å­˜æœ€ä½³æ¨¡å‹: {best_path}")
    
    def save_training_log(self):
        """ä¿å­˜è®­ç»ƒæ—¥å¿—"""
        log_data = {
            'args': vars(self.args),
            'config': self.config.__dict__ if hasattr(self.config, '__dict__') else str(self.config),
            'train_losses': self.train_losses,
            'val_metrics': self.val_metrics,
            'best_map': self.best_map,
            'total_epochs': self.epoch
        }
        
        log_path = self.output_dir / "training_log.json"
        with open(log_path, 'w') as f:
            json.dump(log_data, f, indent=2)
        
        self.logger.info(f"ğŸ“Š è®­ç»ƒæ—¥å¿—å·²ä¿å­˜: {log_path}")
    
    def train(self):
        """ä¸»è®­ç»ƒå¾ªç¯"""
        self.logger.info("ğŸ¯ å¼€å§‹è®­ç»ƒ...")
        
        # æ„å»ºç»„ä»¶
        self.build_model()
        self.build_dataloader()
        self.build_optimizer()
        
        # è®­ç»ƒå¾ªç¯
        for epoch in range(1, self.args.epochs + 1):
            self.epoch = epoch
            
            # è®­ç»ƒ
            train_loss = self.train_epoch()
            
            # éªŒè¯
            if epoch % self.args.val_interval == 0:
                val_metrics = self.validate()
                
                # æ£€æŸ¥æ˜¯å¦æ˜¯æœ€ä½³æ¨¡å‹
                current_map = val_metrics['mAP@0.5']
                is_best = current_map > self.best_map
                if is_best:
                    self.best_map = current_map
                
                # ä¿å­˜æ£€æŸ¥ç‚¹
                self.save_checkpoint(is_best)
        
        # ä¿å­˜æœ€ç»ˆæ—¥å¿—
        self.save_training_log()
        self.logger.info(f"ğŸ‰ è®­ç»ƒå®Œæˆ! æœ€ä½³mAP: {self.best_map:.4f}")


def main():
    parser = argparse.ArgumentParser(description='Gold-YOLO Jittorè®­ç»ƒ')
    
    # æ•°æ®å‚æ•°
    parser.add_argument('--data', type=str, required=True, help='æ•°æ®é›†é…ç½®æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--num_classes', type=int, default=10, help='ç±»åˆ«æ•°é‡')
    
    # æ¨¡å‹å‚æ•°
    parser.add_argument('--model', type=str, default='gold_yolo_s', help='æ¨¡å‹åç§°')
    parser.add_argument('--pretrained', type=str, help='é¢„è®­ç»ƒæƒé‡è·¯å¾„')
    
    # è®­ç»ƒå‚æ•°
    parser.add_argument('--epochs', type=int, default=100, help='è®­ç»ƒè½®æ•°')
    parser.add_argument('--batch_size', type=int, default=6, help='æ‰¹æ¬¡å¤§å°')
    parser.add_argument('--lr', type=float, default=0.01, help='å­¦ä¹ ç‡')
    parser.add_argument('--momentum', type=float, default=0.937, help='åŠ¨é‡')
    parser.add_argument('--weight_decay', type=float, default=0.0005, help='æƒé‡è¡°å‡')
    parser.add_argument('--optimizer', type=str, default='SGD', help='ä¼˜åŒ–å™¨')
    
    # éªŒè¯å‚æ•°
    parser.add_argument('--val_interval', type=int, default=10, help='éªŒè¯é—´éš”')
    
    # å…¶ä»–å‚æ•°
    parser.add_argument('--seed', type=int, default=42, help='éšæœºç§å­')
    parser.add_argument('--output_dir', type=str, default='./experiments/train_jittor', help='è¾“å‡ºç›®å½•')
    
    args = parser.parse_args()
    
    # è®¾ç½®Jittor
    jt.flags.use_cuda = 1 if jt.has_cuda else 0
    
    # è·å–é…ç½®
    config = get_config()
    
    # å¼€å§‹è®­ç»ƒ
    trainer = Trainer(config, args)
    trainer.train()


if __name__ == "__main__":
    main()
