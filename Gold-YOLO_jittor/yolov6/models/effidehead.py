"""
GOLD-YOLO Jittorç‰ˆæœ¬ - Efficient Decoupled Head
ä»PyTorchç‰ˆæœ¬è¿ç§»åˆ°Jittoræ¡†æ¶
"""

import jittor as jt
import jittor.nn as nn
import math
from yolov6.layers.common import *
from yolov6.assigners.anchor_generator import generate_anchors
from yolov6.utils.general import dist2bbox


class Detect(nn.Module):
    '''Efficient Decoupled Head
    With hardware-aware design, the decoupled head is optimized with
    hybridchannels methods.
    '''
    
    def __init__(self, num_classes=80, num_layers=3, inplace=True, head_layers=None, use_dfl=True,
                 reg_max=16):  # detection layer
        super().__init__()
        assert head_layers is not None
        self.nc = num_classes  # number of classes
        self.no = num_classes + 5  # number of outputs per anchor
        self.nl = num_layers  # number of detection layers
        self.grid = [jt.zeros(1)] * num_layers  # ä½¿ç”¨jt.zerosæ›¿ä»£torch.zeros
        self.prior_prob = 1e-2
        self.inplace = inplace
        stride = [8, 16, 32] if num_layers == 3 else [8, 16, 32, 64]  # strides computed during build
        self.stride = jt.array(stride)  # ä½¿ç”¨jt.arrayæ›¿ä»£torch.tensor
        self.use_dfl = use_dfl
        self.reg_max = reg_max
        # ä¿®å¤å…³é”®é”™è¯¯ï¼šä¸PyTorchç‰ˆæœ¬å¯¹é½ï¼Œæ€»æ˜¯åˆ›å»ºproj_convå±‚
        self.proj_conv = nn.Conv2d(self.reg_max + 1, 1, 1, bias=False)
        self.grid_cell_offset = 0.5
        self.grid_cell_size = 5.0
        
        # æ„å»ºæ£€æµ‹å¤´å±‚
        self.stems = nn.ModuleList()
        self.cls_convs = nn.ModuleList()
        self.reg_convs = nn.ModuleList()
        self.cls_preds = nn.ModuleList()
        self.reg_preds = nn.ModuleList()
        
        # ä»head_layersä¸­æå–å„å±‚
        for i in range(num_layers):
            idx = i * 5
            self.stems.append(head_layers[idx])
            self.cls_convs.append(head_layers[idx + 1])
            self.reg_convs.append(head_layers[idx + 2])
            self.cls_preds.append(head_layers[idx + 3])
            self.reg_preds.append(head_layers[idx + 4])
    
    def initialize_biases(self):
        """åˆå§‹åŒ–åç½® - ä¸PyTorchç‰ˆæœ¬å®Œå…¨å¯¹é½"""
        for conv in self.cls_preds:
            # ä¸PyTorchç‰ˆæœ¬å¯¹é½çš„åˆå§‹åŒ–
            bias_value = -math.log((1 - self.prior_prob) / self.prior_prob)
            conv.bias.data = jt.full_like(conv.bias.data, bias_value)
            conv.weight.data = jt.zeros_like(conv.weight.data)

        for conv in self.reg_preds:
            # ä¸PyTorchç‰ˆæœ¬å¯¹é½çš„åˆå§‹åŒ–
            conv.bias.data = jt.ones_like(conv.bias.data)
            conv.weight.data = jt.zeros_like(conv.weight.data)

        # ä¿®å¤å…³é”®é”™è¯¯ï¼šä¸PyTorchç‰ˆæœ¬å¯¹é½ï¼Œæ€»æ˜¯åˆå§‹åŒ–proj_conv
        # ä¸¥æ ¼å¯¹é½PyTorchç‰ˆæœ¬ï¼šprojå’Œproj_conv.weightéƒ½ä¸éœ€è¦æ¢¯åº¦
        self.proj = jt.linspace(0, self.reg_max, self.reg_max + 1)
        self.proj.requires_grad = False  # å…³é”®ä¿®å¤ï¼šä¸éœ€è¦æ¢¯åº¦

        # Jittorçš„æƒé‡èµ‹å€¼æ–¹å¼ - ä¿®å¤æ•°æ®ç±»å‹ä¸åŒ¹é…é—®é¢˜
        proj_weight = self.proj.view([1, self.reg_max + 1, 1, 1]).clone().detach()
        # ç¡®ä¿æ•°æ®ç±»å‹åŒ¹é…
        proj_weight = proj_weight.astype(self.proj_conv.weight.dtype)
        self.proj_conv.weight.assign(proj_weight)  # ä½¿ç”¨assignæ–¹æ³•
        self.proj_conv.weight.requires_grad = False  # å…³é”®ä¿®å¤ï¼šä¸éœ€è¦æ¢¯åº¦

        print(f"ğŸ”§ EffiDeHeadåˆå§‹åŒ–å®Œæˆ:")
        print(f"   use_dfl: {self.use_dfl}")
        print(f"   reg_max: {self.reg_max}")
        print(f"   projå½¢çŠ¶: {self.proj.shape}")
        print(f"   proj_convæƒé‡å½¢çŠ¶: {self.proj_conv.weight.shape}")
        print(f"   projéœ€è¦æ¢¯åº¦: {self.proj.requires_grad}")
        print(f"   proj_convæƒé‡éœ€è¦æ¢¯åº¦: {self.proj_conv.weight.requires_grad}")
    
    def execute(self, x):
        """Jittorç‰ˆæœ¬çš„å‰å‘ä¼ æ’­"""
        if self.training:
            cls_score_list = []
            reg_distri_list = []

            for i in range(self.nl):
                x[i] = self.stems[i](x[i])
                cls_x = x[i]
                reg_x = x[i]
                cls_feat = self.cls_convs[i](cls_x)
                cls_output = self.cls_preds[i](cls_feat)
                reg_feat = self.reg_convs[i](reg_x)
                reg_output = self.reg_preds[i](reg_feat)

                # å®Œå…¨ç…§æŠ„PyTorchç‰ˆæœ¬ï¼šè®­ç»ƒæ—¶ä¹Ÿåº”ç”¨sigmoid
                cls_output = jt.sigmoid(cls_output)
                cls_score_list.append(cls_output.flatten(2).permute((0, 2, 1)))

                # ä¿®å¤å…³é”®é”™è¯¯ï¼šæ­£ç¡®å¤„ç†DFLè¾“å‡ºæ ¼å¼
                # è®­ç»ƒæ—¶éœ€è¦ä¿æŒåŸå§‹çš„åˆ†å¸ƒå‚æ•°ï¼Œä¸è¿›è¡Œproj_convå˜æ¢
                reg_distri_list.append(reg_output.flatten(2).permute((0, 2, 1)))

            # ä¸¥æ ¼å¯¹é½PyTorchç‰ˆæœ¬ï¼šè¿”å›ç‹¬ç«‹çš„è¾“å‡º
            cls_score_list = jt.concat(cls_score_list, dim=1)  # [batch, anchors, num_classes]
            reg_distri_list = jt.concat(reg_distri_list, dim=1)  # [batch, anchors, 4*(reg_max+1)] æˆ– [batch, anchors, 4]

            return x, cls_score_list, reg_distri_list
        
        else:
            cls_score_list = []
            reg_dist_list = []
            anchor_points, stride_tensor = generate_anchors(
                x, self.stride, self.grid_cell_size, self.grid_cell_offset, device=None, is_eval=True, mode='af')
            
            for i in range(self.nl):
                b, _, h, w = x[i].shape
                l = h * w
                x[i] = self.stems[i](x[i])
                cls_x = x[i]
                reg_x = x[i]
                cls_feat = self.cls_convs[i](cls_x)
                cls_output = self.cls_preds[i](cls_feat)
                reg_feat = self.reg_convs[i](reg_x)
                reg_output = self.reg_preds[i](reg_feat)
                
                # ä¿®å¤å…³é”®é”™è¯¯ï¼šä¸PyTorchç‰ˆæœ¬å¯¹é½ï¼Œæ€»æ˜¯ä½¿ç”¨proj_convï¼ˆå½“use_dfl=Trueæ—¶ï¼‰
                if self.use_dfl:
                    reg_output = reg_output.reshape([-1, 4, self.reg_max + 1, l]).permute(0, 2, 1, 3)
                    reg_output = self.proj_conv(nn.softmax(reg_output, dim=1))
                
                # å®Œå…¨ç…§æŠ„PyTorchç‰ˆæœ¬ï¼šæ¨ç†æ—¶ä¹Ÿåº”ç”¨sigmoid
                cls_output = jt.sigmoid(cls_output)
                cls_score_list.append(cls_output.reshape([b, self.nc, l]))
                reg_dist_list.append(reg_output.reshape([b, 4, l]))
            
            cls_score_list = jt.concat(cls_score_list, dim=-1).permute(0, 2, 1)
            reg_dist_list = jt.concat(reg_dist_list, dim=-1).permute(0, 2, 1)

            pred_bboxes = dist2bbox(reg_dist_list, anchor_points, box_format='xywh')
            pred_bboxes *= stride_tensor

            # å®Œå…¨ç…§æŠ„PyTorchç‰ˆæœ¬çš„è¾“å‡ºæ ¼å¼
            # PyTorchç‰ˆæœ¬ç¬¬125-132è¡Œçš„å®Œå…¨ç…§æŠ„
            return jt.concat([
                pred_bboxes,      # [b, anchors, 4] åæ ‡
                jt.ones((b, pred_bboxes.shape[1], 1), dtype=pred_bboxes.dtype),  # å®Œå…¨ç…§æŠ„ï¼šobjectnesså…¨ä¸º1
                cls_score_list    # [b, anchors, 20] ç±»åˆ«åˆ†æ•°
            ], dim=-1)


def build_effidehead_layer(channels_list, num_anchors, num_classes, reg_max=16, num_layers=3):
    """æ„å»ºEffiDeHeadå±‚ - channels_listæ˜¯headçš„è¾“å…¥é€šé“åˆ—è¡¨ï¼Œå¦‚[32, 64, 128]"""

    head_layers = []

    # ä¸ºæ¯ä¸ªæ£€æµ‹å±‚åˆ›å»º5ä¸ªæ¨¡å—ï¼šstem, cls_conv, reg_conv, cls_pred, reg_pred
    for i in range(num_layers):
        ch = channels_list[i]  # å½“å‰å±‚çš„è¾“å…¥é€šé“æ•°

        # stem
        head_layers.append(Conv(
            in_channels=ch,
            out_channels=ch,
            kernel_size=1,
            stride=1
        ))

        # cls_conv
        head_layers.append(Conv(
            in_channels=ch,
            out_channels=ch,
            kernel_size=3,
            stride=1
        ))

        # reg_conv
        head_layers.append(Conv(
            in_channels=ch,
            out_channels=ch,
            kernel_size=3,
            stride=1
        ))

        # cls_pred
        head_layers.append(nn.Conv2d(
            in_channels=ch,
            out_channels=num_classes * num_anchors,
            kernel_size=1
        ))

        # reg_pred - ä¿®å¤å…³é”®é”™è¯¯ï¼šä¸PyTorchç‰ˆæœ¬å®é™…éœ€æ±‚å¯¹é½
        # PyTorchç‰ˆæœ¬è™½ç„¶å†™çš„æ˜¯4*(reg_max+num_anchors)ï¼Œä½†å®é™…forwardä¸­æœŸæœ›çš„æ˜¯4*(reg_max+1)
        if reg_max > 0:  # DFLå¯ç”¨æ—¶
            reg_out_channels = 4 * (reg_max + 1)  # DFLæ¨¡å¼ï¼šæ¯ä¸ªåæ ‡æœ‰(reg_max+1)ä¸ªåˆ†å¸ƒå‚æ•°
        else:  # DFLç¦ç”¨æ—¶
            reg_out_channels = 4 * num_anchors    # ä¼ ç»Ÿæ¨¡å¼ï¼šæ¯ä¸ªanchoræœ‰4ä¸ªåæ ‡

        head_layers.append(nn.Conv2d(
            in_channels=ch,
            out_channels=reg_out_channels,
            kernel_size=1
        ))

    return head_layers
