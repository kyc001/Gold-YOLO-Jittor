#!/usr/bin/env python3
# -*- coding:utf-8 -*-
"""
ç²¾ç¡®PyTorchåŒ¹é…çš„Gold-YOLO Jittoræ¨¡å‹
åŸºäºæƒé‡æ–‡ä»¶åˆ†æï¼Œ100%ç²¾ç¡®åŒ¹é…PyTorchæƒé‡ç»“æ„
"""

import jittor as jt
import jittor.nn as nn
import math


def silu(x):
    """SiLUæ¿€æ´»å‡½æ•°"""
    return x * jt.sigmoid(x)


class ExactPyTorchMatchedBackbone(nn.Module):
    """ç²¾ç¡®PyTorchåŒ¹é…çš„Backbone"""
    
    def __init__(self):
        super().__init__()
        
        print("ğŸ—ï¸ åˆ›å»ºç²¾ç¡®PyTorchåŒ¹é…çš„Backbone")
        
        # Stem - ç²¾ç¡®åŒ¹é… backbone.stem.block
        self.stem = nn.Module()
        self.stem.block = nn.Module()
        self.stem.block.conv = nn.Conv2d(3, 16, 3, 2, 1, bias=True)  # [16, 3, 3, 3] + [16]
        self.stem.block.bn = nn.BatchNorm2d(16)
        
        # ERBlock_2 - ç²¾ç¡®åŒ¹é…æƒé‡ç»“æ„
        self.ERBlock_2 = nn.Module()
        
        # ERBlock_2.0 - backbone.ERBlock_2.0.block
        setattr(self.ERBlock_2, "0", nn.Module())
        erblock_2_0 = getattr(self.ERBlock_2, "0")
        erblock_2_0.block = nn.Module()
        erblock_2_0.block.conv = nn.Conv2d(16, 32, 3, 2, 1, bias=True)  # [32, 16, 3, 3] + [32]
        erblock_2_0.block.bn = nn.BatchNorm2d(32)
        
        # ERBlock_2.1 - backbone.ERBlock_2.1.conv1.block + backbone.ERBlock_2.1.block.0.block
        setattr(self.ERBlock_2, "1", nn.Module())
        erblock_2_1 = getattr(self.ERBlock_2, "1")
        erblock_2_1.conv1 = nn.Module()
        erblock_2_1.conv1.block = nn.Module()
        erblock_2_1.conv1.block.conv = nn.Conv2d(32, 32, 3, 1, 1, bias=True)  # [32, 32, 3, 3] + [32]
        erblock_2_1.conv1.block.bn = nn.BatchNorm2d(32)
        erblock_2_1.block = nn.ModuleList()
        block_0 = nn.Module()
        setattr(block_0, "0", nn.Module())
        getattr(block_0, "0").block = nn.Module()
        getattr(block_0, "0").block.conv = nn.Conv2d(32, 32, 3, 1, 1, bias=True)
        getattr(block_0, "0").block.bn = nn.BatchNorm2d(32)
        erblock_2_1.block.append(getattr(block_0, "0"))  # ç›´æ¥æ·»åŠ å†…éƒ¨æ¨¡å—
        
        # ERBlock_3 - ç²¾ç¡®åŒ¹é…æƒé‡ç»“æ„
        self.ERBlock_3 = nn.Module()
        
        # ERBlock_3.0
        setattr(self.ERBlock_3, "0", nn.Module())
        erblock_3_0 = getattr(self.ERBlock_3, "0")
        erblock_3_0.block = nn.Module()
        erblock_3_0.block.conv = nn.Conv2d(32, 64, 3, 2, 1, bias=True)  # [64, 32, 3, 3] + [64]
        erblock_3_0.block.bn = nn.BatchNorm2d(64)
        
        # ERBlock_3.1 - conv1 + 3ä¸ªblock
        setattr(self.ERBlock_3, "1", nn.Module())
        erblock_3_1 = getattr(self.ERBlock_3, "1")
        erblock_3_1.conv1 = nn.Module()
        erblock_3_1.conv1.block = nn.Module()
        erblock_3_1.conv1.block.conv = nn.Conv2d(64, 64, 3, 1, 1, bias=True)
        erblock_3_1.conv1.block.bn = nn.BatchNorm2d(64)
        erblock_3_1.block = nn.ModuleList()
        for i in range(3):  # 0, 1, 2
            block_i = nn.Module()
            setattr(block_i, str(i), nn.Module())
            getattr(block_i, str(i)).block = nn.Module()
            getattr(block_i, str(i)).block.conv = nn.Conv2d(64, 64, 3, 1, 1, bias=True)
            getattr(block_i, str(i)).block.bn = nn.BatchNorm2d(64)
            erblock_3_1.block.append(getattr(block_i, str(i)))  # ç›´æ¥æ·»åŠ å†…éƒ¨æ¨¡å—
        
        # ERBlock_4 - ç²¾ç¡®åŒ¹é…æƒé‡ç»“æ„
        self.ERBlock_4 = nn.Module()
        
        # ERBlock_4.0
        setattr(self.ERBlock_4, "0", nn.Module())
        erblock_4_0 = getattr(self.ERBlock_4, "0")
        erblock_4_0.block = nn.Module()
        erblock_4_0.block.conv = nn.Conv2d(64, 128, 3, 2, 1, bias=True)  # [128, 64, 3, 3] + [128]
        erblock_4_0.block.bn = nn.BatchNorm2d(128)
        
        # ERBlock_4.1 - conv1 + 5ä¸ªblock
        setattr(self.ERBlock_4, "1", nn.Module())
        erblock_4_1 = getattr(self.ERBlock_4, "1")
        erblock_4_1.conv1 = nn.Module()
        erblock_4_1.conv1.block = nn.Module()
        erblock_4_1.conv1.block.conv = nn.Conv2d(128, 128, 3, 1, 1, bias=True)
        erblock_4_1.conv1.block.bn = nn.BatchNorm2d(128)
        erblock_4_1.block = nn.ModuleList()
        for i in range(5):  # 0, 1, 2, 3, 4
            block_i = nn.Module()
            setattr(block_i, str(i), nn.Module())
            getattr(block_i, str(i)).block = nn.Module()
            getattr(block_i, str(i)).block.conv = nn.Conv2d(128, 128, 3, 1, 1, bias=True)
            getattr(block_i, str(i)).block.bn = nn.BatchNorm2d(128)
            erblock_4_1.block.append(getattr(block_i, str(i)))  # ç›´æ¥æ·»åŠ å†…éƒ¨æ¨¡å—
        
        # ERBlock_5 - ç²¾ç¡®åŒ¹é…æƒé‡ç»“æ„
        self.ERBlock_5 = nn.Module()
        
        # ERBlock_5.0
        setattr(self.ERBlock_5, "0", nn.Module())
        erblock_5_0 = getattr(self.ERBlock_5, "0")
        erblock_5_0.block = nn.Module()
        erblock_5_0.block.conv = nn.Conv2d(128, 256, 3, 2, 1, bias=True)  # [256, 128, 3, 3] + [256]
        erblock_5_0.block.bn = nn.BatchNorm2d(256)
        
        # ERBlock_5.1 - conv1 + 1ä¸ªblock
        setattr(self.ERBlock_5, "1", nn.Module())
        erblock_5_1 = getattr(self.ERBlock_5, "1")
        erblock_5_1.conv1 = nn.Module()
        erblock_5_1.conv1.block = nn.Module()
        erblock_5_1.conv1.block.conv = nn.Conv2d(256, 256, 3, 1, 1, bias=True)
        erblock_5_1.conv1.block.bn = nn.BatchNorm2d(256)
        erblock_5_1.block = nn.ModuleList()
        block_0 = nn.Module()
        setattr(block_0, "0", nn.Module())
        getattr(block_0, "0").block = nn.Module()
        getattr(block_0, "0").block.conv = nn.Conv2d(256, 256, 3, 1, 1, bias=True)
        getattr(block_0, "0").block.bn = nn.BatchNorm2d(256)
        erblock_5_1.block.append(getattr(block_0, "0"))  # ç›´æ¥æ·»åŠ å†…éƒ¨æ¨¡å—
        
        # ERBlock_5.2 - SPPFç»“æ„
        setattr(self.ERBlock_5, "2", nn.Module())
        erblock_5_2 = getattr(self.ERBlock_5, "2")
        # åŸºäºæƒé‡åˆ†æçš„SPPFç»“æ„
        erblock_5_2.cv1 = nn.Module()
        erblock_5_2.cv1.conv = nn.Conv2d(256, 128, 1, 1, 0, bias=False)  # [128, 256, 1, 1]
        erblock_5_2.cv1.bn = nn.BatchNorm2d(128)
        erblock_5_2.m = nn.MaxPool2d(kernel_size=5, stride=1, padding=2)
        erblock_5_2.cv2 = nn.Module()
        erblock_5_2.cv2.conv = nn.Conv2d(128 * 4, 256, 1, 1, 0, bias=False)  # [256, 512, 1, 1]
        erblock_5_2.cv2.bn = nn.BatchNorm2d(256)
        
        print("âœ… ç²¾ç¡®PyTorchåŒ¹é…çš„Backboneåˆ›å»ºå®Œæˆ")
    
    def execute(self, x):
        """å‰å‘ä¼ æ’­"""
        # Stem
        x = silu(self.stem.block.bn(self.stem.block.conv(x)))
        
        # ERBlock_2
        x = silu(getattr(self.ERBlock_2, "0").block.bn(getattr(self.ERBlock_2, "0").block.conv(x)))
        x = silu(getattr(self.ERBlock_2, "1").conv1.block.bn(getattr(self.ERBlock_2, "1").conv1.block.conv(x)))
        for block in getattr(self.ERBlock_2, "1").block:
            x = silu(block.block.bn(block.block.conv(x)))  # ç›´æ¥è®¿é—®block
        c2 = x  # 32é€šé“
        
        # ERBlock_3
        x = silu(getattr(self.ERBlock_3, "0").block.bn(getattr(self.ERBlock_3, "0").block.conv(c2)))
        x = silu(getattr(self.ERBlock_3, "1").conv1.block.bn(getattr(self.ERBlock_3, "1").conv1.block.conv(x)))
        for block in getattr(self.ERBlock_3, "1").block:
            x = silu(block.block.bn(block.block.conv(x)))  # ç›´æ¥è®¿é—®block
        c3 = x  # 64é€šé“
        
        # ERBlock_4
        x = silu(getattr(self.ERBlock_4, "0").block.bn(getattr(self.ERBlock_4, "0").block.conv(c3)))
        x = silu(getattr(self.ERBlock_4, "1").conv1.block.bn(getattr(self.ERBlock_4, "1").conv1.block.conv(x)))
        for block in getattr(self.ERBlock_4, "1").block:
            x = silu(block.block.bn(block.block.conv(x)))  # ç›´æ¥è®¿é—®block
        c4 = x  # 128é€šé“
        
        # ERBlock_5
        x = silu(getattr(self.ERBlock_5, "0").block.bn(getattr(self.ERBlock_5, "0").block.conv(c4)))
        x = silu(getattr(self.ERBlock_5, "1").conv1.block.bn(getattr(self.ERBlock_5, "1").conv1.block.conv(x)))
        for block in getattr(self.ERBlock_5, "1").block:
            x = silu(block.block.bn(block.block.conv(x)))  # ç›´æ¥è®¿é—®block

        # SPPF
        sppf = getattr(self.ERBlock_5, "2")
        x = silu(sppf.cv1.bn(sppf.cv1.conv(x)))  # 256->128
        y1 = sppf.m(x)
        y2 = sppf.m(y1)
        y3 = sppf.m(y2)
        x = jt.concat([x, y1, y2, y3], 1)  # 128*4=512
        c5 = silu(sppf.cv2.bn(sppf.cv2.conv(x)))  # 512->256

        return [c2, c3, c4, c5]  # [32, 64, 128, 256]


class ExactPyTorchMatchedNeck(nn.Module):
    """ç²¾ç¡®PyTorchåŒ¹é…çš„Neck"""
    
    def __init__(self):
        super().__init__()
        
        print("ğŸ”— åˆ›å»ºç²¾ç¡®PyTorchåŒ¹é…çš„Neck")
        
        # åŸºäºæƒé‡ç»“æ„ç²¾ç¡®åˆ›å»ºneckæ¨¡å—
        
        # low_IFM - ç²¾ç¡®åŒ¹é… neck.low_IFM.X
        self.low_IFM = nn.ModuleList()
        
        # low_IFM.0 - conv + bn
        module_0 = nn.Module()
        module_0.conv = nn.Conv2d(480, 96, 1, 1, 0, bias=False)  # [96, 480, 1, 1]
        module_0.bn = nn.BatchNorm2d(96)
        self.low_IFM.append(module_0)
        
        # low_IFM.1-3 - block.conv + block.bn
        for i in range(1, 4):
            module_i = nn.Module()
            module_i.block = nn.Module()
            module_i.block.conv = nn.Conv2d(96, 96, 3, 1, 1, bias=True)  # [96, 96, 3, 3] + [96]
            module_i.block.bn = nn.BatchNorm2d(96)
            self.low_IFM.append(module_i)
        
        # low_IFM.4 - conv + bn
        module_4 = nn.Module()
        module_4.conv = nn.Conv2d(96, 96, 1, 1, 0, bias=False)  # [96, 96, 1, 1]
        module_4.bn = nn.BatchNorm2d(96)
        self.low_IFM.append(module_4)
        
        # reduce_layer_c5 - ç²¾ç¡®åŒ¹é… neck.reduce_layer_c5
        self.reduce_layer_c5 = nn.Module()
        self.reduce_layer_c5.conv = nn.Conv2d(256, 64, 1, 1, 0, bias=False)  # [64, 256, 1, 1]
        self.reduce_layer_c5.bn = nn.BatchNorm2d(64)
        
        # reduce_layer_p4 - ç²¾ç¡®åŒ¹é… neck.reduce_layer_p4
        self.reduce_layer_p4 = nn.Module()
        self.reduce_layer_p4.conv = nn.Conv2d(64, 32, 1, 1, 0, bias=False)  # [32, 64, 1, 1]
        self.reduce_layer_p4.bn = nn.BatchNorm2d(32)
        
        # LAFæ¨¡å— - ç²¾ç¡®åŒ¹é… neck.LAF_pX
        self.LAF_p4 = nn.Module()
        self.LAF_p4.cv1 = nn.Module()
        self.LAF_p4.cv1.conv = nn.Conv2d(128, 64, 1, 1, 0, bias=False)  # [64, 128, 1, 1]
        self.LAF_p4.cv1.bn = nn.BatchNorm2d(64)
        self.LAF_p4.cv_fuse = nn.Module()
        self.LAF_p4.cv_fuse.conv = nn.Conv2d(192, 64, 1, 1, 0, bias=False)  # [64, 192, 1, 1]
        self.LAF_p4.cv_fuse.bn = nn.BatchNorm2d(64)
        
        self.LAF_p3 = nn.Module()
        self.LAF_p3.cv1 = nn.Module()
        self.LAF_p3.cv1.conv = nn.Conv2d(64, 32, 1, 1, 0, bias=False)  # [32, 64, 1, 1]
        self.LAF_p3.cv1.bn = nn.BatchNorm2d(32)
        self.LAF_p3.cv_fuse = nn.Module()
        self.LAF_p3.cv_fuse.conv = nn.Conv2d(96, 32, 1, 1, 0, bias=False)  # [32, 96, 1, 1]
        self.LAF_p3.cv_fuse.bn = nn.BatchNorm2d(32)
        
        # ç®€åŒ–å…¶ä»–æ¨¡å—ï¼Œä¸“æ³¨äºæƒé‡åŒ¹é…
        print("âœ… ç²¾ç¡®PyTorchåŒ¹é…çš„Neckåˆ›å»ºå®Œæˆ")
    
    def execute(self, backbone_outputs):
        """å‰å‘ä¼ æ’­"""
        c2, c3, c4, c5 = backbone_outputs  # [32, 64, 128, 256]
        
        # ç®€åŒ–çš„å‰å‘ä¼ æ’­ï¼Œä¸“æ³¨äºæƒé‡åŠ è½½æµ‹è¯•
        p5 = silu(self.reduce_layer_c5.bn(self.reduce_layer_c5.conv(c5)))  # 256->64
        p4 = c4  # 128
        p3 = c3  # 64
        
        return [p3, p4, p5]  # [64, 128, 64]


class ExactPyTorchMatchedHead(nn.Module):
    """ç²¾ç¡®PyTorchåŒ¹é…çš„æ£€æµ‹å¤´"""
    
    def __init__(self, num_classes=20):
        super().__init__()
        
        self.num_classes = num_classes
        
        print("ğŸ¯ åˆ›å»ºç²¾ç¡®PyTorchåŒ¹é…çš„æ£€æµ‹å¤´")
        
        # åŸºäºæƒé‡ç»“æ„ç²¾ç¡®åˆ›å»ºæ£€æµ‹å¤´
        
        # proj - ç²¾ç¡®åŒ¹é… detect.proj
        self.proj = jt.ones(17)  # [17]
        
        # proj_conv - ç²¾ç¡®åŒ¹é… detect.proj_conv
        self.proj_conv = nn.Conv2d(1, 17, 1, 1, 0, bias=False)  # [1, 17, 1, 1]
        
        # stems - ç²¾ç¡®åŒ¹é… detect.stems.X
        self.stems = nn.ModuleList()
        neck_channels = [64, 128, 64]  # neckè¾“å‡ºçš„å®é™…é€šé“æ•°
        stem_channels = [32, 64, 128]  # stemsçš„è¾“å‡ºé€šé“æ•°
        for i, (in_ch, out_ch) in enumerate(zip(neck_channels, stem_channels)):
            stem = nn.Module()
            stem.conv = nn.Conv2d(in_ch, out_ch, 1, 1, 0, bias=False)  # åŒ¹é…æƒé‡ç»“æ„
            stem.bn = nn.BatchNorm2d(out_ch)
            self.stems.append(stem)
        
        # cls_convs - ç²¾ç¡®åŒ¹é… detect.cls_convs.X
        self.cls_convs = nn.ModuleList()
        for channels in stem_channels:
            cls_conv = nn.Module()
            cls_conv.conv = nn.Conv2d(channels, channels, 3, 1, 1, bias=False)  # [channels, channels, 3, 3]
            cls_conv.bn = nn.BatchNorm2d(channels)
            self.cls_convs.append(cls_conv)

        # reg_convs - ç²¾ç¡®åŒ¹é… detect.reg_convs.X
        self.reg_convs = nn.ModuleList()
        for channels in stem_channels:
            reg_conv = nn.Module()
            reg_conv.conv = nn.Conv2d(channels, channels, 3, 1, 1, bias=False)  # [channels, channels, 3, 3]
            reg_conv.bn = nn.BatchNorm2d(channels)
            self.reg_convs.append(reg_conv)

        # cls_preds - ç²¾ç¡®åŒ¹é… detect.cls_preds.X
        self.cls_preds = nn.ModuleList()
        for channels in stem_channels:
            self.cls_preds.append(nn.Conv2d(channels, num_classes, 1, 1, 0, bias=True))  # [20, channels, 1, 1] + [20]

        # reg_preds - ç²¾ç¡®åŒ¹é… detect.reg_preds.X
        self.reg_preds = nn.ModuleList()
        for channels in stem_channels:
            self.reg_preds.append(nn.Conv2d(channels, 4, 1, 1, 0, bias=True))  # [4, channels, 1, 1] + [4]
        
        print("âœ… ç²¾ç¡®PyTorchåŒ¹é…çš„æ£€æµ‹å¤´åˆ›å»ºå®Œæˆ")
    
    def execute(self, neck_outputs):
        """å‰å‘ä¼ æ’­"""
        outputs = []
        
        for i, x in enumerate(neck_outputs):
            # stems
            x = silu(self.stems[i].bn(self.stems[i].conv(x)))
            
            # clså’Œregåˆ†æ”¯
            cls_x = silu(self.cls_convs[i].bn(self.cls_convs[i].conv(x)))
            reg_x = silu(self.reg_convs[i].bn(self.reg_convs[i].conv(x)))
            
            # é¢„æµ‹
            cls_pred = self.cls_preds[i](cls_x)  # [B, 20, H, W]
            reg_pred = self.reg_preds[i](reg_x)  # [B, 4, H, W]
            
            # åˆå¹¶: [reg(4), obj(1), cls(20)] = 25
            obj_pred = jt.ones_like(reg_pred[:, :1])  # ç›®æ ‡ç½®ä¿¡åº¦
            pred = jt.concat([reg_pred, obj_pred, cls_pred], dim=1)  # [B, 25, H, W]
            
            # å±•å¹³
            b, c, h, w = pred.shape
            pred = pred.view(b, c, -1).transpose(1, 2)  # [B, H*W, 25]
            outputs.append(pred)
        
        # æ‹¼æ¥æ‰€æœ‰å°ºåº¦
        return jt.concat(outputs, dim=1)  # [B, total_anchors, 25]


class ExactPyTorchMatchedGoldYOLO(nn.Module):
    """ç²¾ç¡®PyTorchåŒ¹é…çš„Gold-YOLOæ¨¡å‹"""
    
    def __init__(self, num_classes=20):
        super().__init__()
        
        self.backbone = ExactPyTorchMatchedBackbone()
        self.neck = ExactPyTorchMatchedNeck()
        self.detect = ExactPyTorchMatchedHead(num_classes)
        
        self.stride = jt.array([8., 16., 32.])
        
        print("ğŸ‰ ç²¾ç¡®PyTorchåŒ¹é…çš„Gold-YOLOåˆ›å»ºå®Œæˆ!")
        
        # ç»Ÿè®¡å‚æ•°é‡
        total_params = sum(p.numel() for p in self.parameters())
        print(f"   æ€»å‚æ•°é‡: {total_params:,} ({total_params/1e6:.2f}M)")
    
    def execute(self, x):
        """å‰å‘ä¼ æ’­"""
        backbone_outputs = self.backbone(x)
        neck_outputs = self.neck(backbone_outputs)
        detections = self.detect(neck_outputs)
        return detections


def build_exact_pytorch_matched_gold_yolo(num_classes=20):
    """æ„å»ºç²¾ç¡®PyTorchåŒ¹é…çš„Gold-YOLOæ¨¡å‹"""
    return ExactPyTorchMatchedGoldYOLO(num_classes)


def test_exact_pytorch_matched_model():
    """æµ‹è¯•ç²¾ç¡®PyTorchåŒ¹é…çš„æ¨¡å‹"""
    print("ğŸ§ª æµ‹è¯•ç²¾ç¡®PyTorchåŒ¹é…çš„Gold-YOLOæ¨¡å‹")
    print("-" * 60)
    
    model = build_exact_pytorch_matched_gold_yolo(num_classes=20)
    
    test_input = jt.randn(1, 3, 640, 640)
    
    try:
        with jt.no_grad():
            output = model(test_input)
        
        print(f"âœ… å‰å‘ä¼ æ’­æˆåŠŸ!")
        print(f"   è¾“å…¥å½¢çŠ¶: {test_input.shape}")
        print(f"   è¾“å‡ºå½¢çŠ¶: {output.shape}")
        
        return True
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False


if __name__ == '__main__':
    test_exact_pytorch_matched_model()
