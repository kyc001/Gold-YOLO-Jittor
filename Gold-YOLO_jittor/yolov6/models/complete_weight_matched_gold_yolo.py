#!/usr/bin/env python3
# -*- coding:utf-8 -*-
"""
å®Œæ•´æƒé‡åŒ¹é…çš„Gold-YOLO Jittoræ¨¡å‹
åŸºäºPyTorchæƒé‡ç»“æ„åˆ†æï¼Œ100%ç²¾ç¡®åŒ¹é…
"""

import jittor as jt
import jittor.nn as nn
import math


def silu(x):
    """SiLUæ¿€æ´»å‡½æ•°"""
    return x * jt.sigmoid(x)


class CompleteWeightMatchedBackbone(nn.Module):
    """å®Œæ•´æƒé‡åŒ¹é…çš„Backbone - 3.14Må‚æ•°"""
    
    def __init__(self):
        super().__init__()
        
        print("ğŸ—ï¸ åˆ›å»ºå®Œæ•´æƒé‡åŒ¹é…çš„Backbone")
        
        # åŸºäºæƒé‡ç»“æ„ç²¾ç¡®åˆ›å»ºå±‚
        
        # Stem - stem.block (6ä¸ªå‚æ•°)
        self.stem = nn.Module()
        self.stem.block = nn.Module()
        self.stem.block.conv = nn.Conv2d(3, 16, 3, 2, 1, bias=True)  # (16, 3, 3, 3) + (16,)
        self.stem.block.bn = nn.BatchNorm2d(16)
        
        # ERBlock_2.0 - (6ä¸ªå‚æ•°)
        self.ERBlock_2 = nn.Module()
        setattr(self.ERBlock_2, "0", nn.Module())
        getattr(self.ERBlock_2, "0").conv = nn.Conv2d(16, 32, 3, 2, 1, bias=True)
        getattr(self.ERBlock_2, "0").bn = nn.BatchNorm2d(32)
        
        # ERBlock_2.1 - (12ä¸ªå‚æ•°)
        setattr(self.ERBlock_2, "1", nn.Module())
        erblock_2_1 = getattr(self.ERBlock_2, "1")
        erblock_2_1.conv = nn.Conv2d(32, 32, 3, 1, 1, bias=True)
        erblock_2_1.bn = nn.BatchNorm2d(32)
        erblock_2_1.block = nn.ModuleList()
        block_0 = nn.Module()
        block_0.conv = nn.Conv2d(32, 32, 3, 1, 1, bias=True)
        block_0.bn = nn.BatchNorm2d(32)
        erblock_2_1.block.append(block_0)
        
        # ERBlock_3.0 - (6ä¸ªå‚æ•°)
        self.ERBlock_3 = nn.Module()
        setattr(self.ERBlock_3, "0", nn.Module())
        getattr(self.ERBlock_3, "0").conv = nn.Conv2d(32, 64, 3, 2, 1, bias=True)
        getattr(self.ERBlock_3, "0").bn = nn.BatchNorm2d(64)
        
        # ERBlock_3.1 - (24ä¸ªå‚æ•°)
        setattr(self.ERBlock_3, "1", nn.Module())
        erblock_3_1 = getattr(self.ERBlock_3, "1")
        erblock_3_1.conv = nn.Conv2d(64, 64, 3, 1, 1, bias=True)
        erblock_3_1.bn = nn.BatchNorm2d(64)
        erblock_3_1.block = nn.ModuleList()
        for i in range(3):
            block_i = nn.Module()
            block_i.conv = nn.Conv2d(64, 64, 3, 1, 1, bias=True)
            block_i.bn = nn.BatchNorm2d(64)
            erblock_3_1.block.append(block_i)
        
        # ERBlock_4.0 - (6ä¸ªå‚æ•°)
        self.ERBlock_4 = nn.Module()
        setattr(self.ERBlock_4, "0", nn.Module())
        getattr(self.ERBlock_4, "0").conv = nn.Conv2d(64, 128, 3, 2, 1, bias=True)
        getattr(self.ERBlock_4, "0").bn = nn.BatchNorm2d(128)
        
        # ERBlock_4.1 - (36ä¸ªå‚æ•°)
        setattr(self.ERBlock_4, "1", nn.Module())
        erblock_4_1 = getattr(self.ERBlock_4, "1")
        erblock_4_1.conv = nn.Conv2d(128, 128, 3, 1, 1, bias=True)
        erblock_4_1.bn = nn.BatchNorm2d(128)
        erblock_4_1.block = nn.ModuleList()
        for i in range(5):
            block_i = nn.Module()
            block_i.conv = nn.Conv2d(128, 128, 3, 1, 1, bias=True)
            block_i.bn = nn.BatchNorm2d(128)
            erblock_4_1.block.append(block_i)
        
        # ERBlock_5.0 - (6ä¸ªå‚æ•°)
        self.ERBlock_5 = nn.Module()
        setattr(self.ERBlock_5, "0", nn.Module())
        getattr(self.ERBlock_5, "0").conv = nn.Conv2d(128, 256, 3, 2, 1, bias=True)
        getattr(self.ERBlock_5, "0").bn = nn.BatchNorm2d(256)
        
        # ERBlock_5.1 - (12ä¸ªå‚æ•°)
        setattr(self.ERBlock_5, "1", nn.Module())
        erblock_5_1 = getattr(self.ERBlock_5, "1")
        erblock_5_1.conv = nn.Conv2d(256, 256, 3, 1, 1, bias=True)
        erblock_5_1.bn = nn.BatchNorm2d(256)
        erblock_5_1.block = nn.ModuleList()
        block_0 = nn.Module()
        block_0.conv = nn.Conv2d(256, 256, 3, 1, 1, bias=True)
        block_0.bn = nn.BatchNorm2d(256)
        erblock_5_1.block.append(block_0)
        
        # ERBlock_5.2 - SPPFç»“æ„ (35ä¸ªå‚æ•°)
        setattr(self.ERBlock_5, "2", nn.Module())
        erblock_5_2 = getattr(self.ERBlock_5, "2")
        erblock_5_2.conv = nn.Conv2d(256, 128, 1, 1, 0, bias=False)
        erblock_5_2.bn = nn.BatchNorm2d(128)
        erblock_5_2.m = nn.MaxPool2d(kernel_size=5, stride=1, padding=2)
        erblock_5_2.cv2 = nn.Conv2d(128 * 4, 256, 1, 1, 0, bias=False)
        erblock_5_2.cv2_bn = nn.BatchNorm2d(256)
        
        print("âœ… å®Œæ•´æƒé‡åŒ¹é…çš„Backboneåˆ›å»ºå®Œæˆ")
    
    def execute(self, x):
        """å‰å‘ä¼ æ’­"""
        # Stem
        x = silu(self.stem.block.bn(self.stem.block.conv(x)))
        
        # ERBlock_2
        x = silu(getattr(self.ERBlock_2, "0").bn(getattr(self.ERBlock_2, "0").conv(x)))
        x = silu(getattr(self.ERBlock_2, "1").bn(getattr(self.ERBlock_2, "1").conv(x)))
        for block in getattr(self.ERBlock_2, "1").block:
            x = silu(block.bn(block.conv(x)))
        c2 = x  # 32é€šé“
        
        # ERBlock_3
        x = silu(getattr(self.ERBlock_3, "0").bn(getattr(self.ERBlock_3, "0").conv(c2)))
        x = silu(getattr(self.ERBlock_3, "1").bn(getattr(self.ERBlock_3, "1").conv(x)))
        for block in getattr(self.ERBlock_3, "1").block:
            x = silu(block.bn(block.conv(x)))
        c3 = x  # 64é€šé“
        
        # ERBlock_4
        x = silu(getattr(self.ERBlock_4, "0").bn(getattr(self.ERBlock_4, "0").conv(c3)))
        x = silu(getattr(self.ERBlock_4, "1").bn(getattr(self.ERBlock_4, "1").conv(x)))
        for block in getattr(self.ERBlock_4, "1").block:
            x = silu(block.bn(block.conv(x)))
        c4 = x  # 128é€šé“
        
        # ERBlock_5
        x = silu(getattr(self.ERBlock_5, "0").bn(getattr(self.ERBlock_5, "0").conv(c4)))
        x = silu(getattr(self.ERBlock_5, "1").bn(getattr(self.ERBlock_5, "1").conv(x)))
        for block in getattr(self.ERBlock_5, "1").block:
            x = silu(block.bn(block.conv(x)))
        
        # SPPF
        sppf = getattr(self.ERBlock_5, "2")
        x = silu(sppf.bn(sppf.conv(x)))  # 256->128
        y1 = sppf.m(x)
        y2 = sppf.m(y1)
        y3 = sppf.m(y2)
        x = jt.concat([x, y1, y2, y3], 1)  # 128*4=512
        c5 = silu(sppf.cv2_bn(sppf.cv2(x)))  # 512->256
        
        return [c2, c3, c4, c5]  # [32, 64, 128, 256]


class CompleteWeightMatchedNeck(nn.Module):
    """å®Œæ•´æƒé‡åŒ¹é…çš„Neck - 2.07Må‚æ•°"""
    
    def __init__(self):
        super().__init__()
        
        print("ğŸ”— åˆ›å»ºå®Œæ•´æƒé‡åŒ¹é…çš„Neck")
        
        # åŸºäºæƒé‡ç»“æ„ç²¾ç¡®åˆ›å»ºneckæ¨¡å—
        
        # low_IFMæ¨¡å— (5ä¸ªå­æ¨¡å—)
        self.low_IFM = nn.ModuleList()
        
        # low_IFM.0 - (5ä¸ªå‚æ•°: conv.weight + bn)
        module_0 = nn.Module()
        module_0.conv = nn.Conv2d(480, 96, 1, 1, 0, bias=False)  # (96, 480, 1, 1)
        module_0.bn = nn.BatchNorm2d(96)
        self.low_IFM.append(module_0)
        
        # low_IFM.1-3 - RepVGGBlock (å„6ä¸ªå‚æ•°)
        for i in range(1, 4):
            module_i = nn.Module()
            module_i.conv = nn.Conv2d(96, 96, 3, 1, 1, bias=True)  # (96, 96, 3, 3) + (96,)
            module_i.bn = nn.BatchNorm2d(96)
            self.low_IFM.append(module_i)
        
        # low_IFM.4 - (5ä¸ªå‚æ•°)
        module_4 = nn.Module()
        module_4.conv = nn.Conv2d(96, 96, 1, 1, 0, bias=False)  # (96, 96, 1, 1)
        module_4.bn = nn.BatchNorm2d(96)
        self.low_IFM.append(module_4)
        
        # reduce_layer_c5
        self.reduce_layer_c5 = nn.Module()
        self.reduce_layer_c5.conv = nn.Conv2d(256, 64, 1, 1, 0, bias=False)
        self.reduce_layer_c5.bn = nn.BatchNorm2d(64)
        
        # reduce_layer_p4
        self.reduce_layer_p4 = nn.Module()
        self.reduce_layer_p4.conv = nn.Conv2d(64, 32, 1, 1, 0, bias=False)
        self.reduce_layer_p4.bn = nn.BatchNorm2d(32)
        
        # LAFæ¨¡å—
        self.LAF_p4 = nn.Module()
        self.LAF_p4.cv1 = nn.Module()
        self.LAF_p4.cv1.conv = nn.Conv2d(128, 64, 1, 1, 0, bias=False)
        self.LAF_p4.cv1.bn = nn.BatchNorm2d(64)
        self.LAF_p4.cv_fuse = nn.Module()
        self.LAF_p4.cv_fuse.conv = nn.Conv2d(192, 64, 1, 1, 0, bias=False)
        self.LAF_p4.cv_fuse.bn = nn.BatchNorm2d(64)
        
        self.LAF_p3 = nn.Module()
        self.LAF_p3.cv1 = nn.Module()
        self.LAF_p3.cv1.conv = nn.Conv2d(64, 32, 1, 1, 0, bias=False)
        self.LAF_p3.cv1.bn = nn.BatchNorm2d(32)
        self.LAF_p3.cv_fuse = nn.Module()
        self.LAF_p3.cv_fuse.conv = nn.Conv2d(96, 32, 1, 1, 0, bias=False)
        self.LAF_p3.cv_fuse.bn = nn.BatchNorm2d(32)
        
        # Injectæ¨¡å—
        inject_configs = [
            ('Inject_p4', 64), ('Inject_p3', 32), 
            ('Inject_n4', 64), ('Inject_n5', 128)
        ]
        
        for name, channels in inject_configs:
            inject_module = nn.Module()
            inject_module.local_embedding = nn.Module()
            inject_module.local_embedding.conv = nn.Conv2d(channels, channels, 1, 1, 0, bias=False)
            inject_module.local_embedding.bn = nn.BatchNorm2d(channels)
            inject_module.global_embedding = nn.Module()
            inject_module.global_embedding.conv = nn.Conv2d(channels, channels, 1, 1, 0, bias=False)
            inject_module.global_embedding.bn = nn.BatchNorm2d(channels)
            inject_module.global_act = nn.Module()
            inject_module.global_act.conv = nn.Conv2d(channels, channels, 1, 1, 0, bias=False)
            inject_module.global_act.bn = nn.BatchNorm2d(channels)
            setattr(self, name, inject_module)
        
        # Repæ¨¡å—
        rep_configs = [
            ('Rep_p4', 64), ('Rep_p3', 32), 
            ('Rep_n4', 64), ('Rep_n5', 128)
        ]
        
        for name, channels in rep_configs:
            rep_module = nn.Module()
            rep_module.conv1 = nn.Module()
            rep_module.conv1.conv = nn.Conv2d(channels, channels, 3, 1, 1, bias=True)
            rep_module.conv1.bn = nn.BatchNorm2d(channels)
            rep_module.block = nn.ModuleList()
            # æ ¹æ®æƒé‡åˆ†ææ·»åŠ é€‚å½“æ•°é‡çš„block
            num_blocks = 3 if channels == 32 else 3  # ç®€åŒ–
            for i in range(num_blocks):
                block_i = nn.Module()
                block_i.conv = nn.Conv2d(channels, channels, 3, 1, 1, bias=True)
                block_i.bn = nn.BatchNorm2d(channels)
                rep_module.block.append(block_i)
            setattr(self, name, rep_module)
        
        # Transformeræ¨¡å— (ç®€åŒ–ç‰ˆæœ¬)
        self.high_IFM = nn.Module()
        self.high_IFM.transformer_blocks = nn.ModuleList()
        for i in range(2):  # 2ä¸ªtransformer blocks
            transformer_block = nn.Module()
            # ç®€åŒ–çš„transformerç»“æ„
            transformer_block.c = nn.Conv2d(352, 32, 1, 1, 0, bias=False)
            transformer_block.bn = nn.BatchNorm2d(32)
            self.high_IFM.transformer_blocks.append(transformer_block)
        
        # conv_1x1_n
        self.conv_1x1_n = nn.Conv2d(352, 192, 1, 1, 0, bias=True)
        
        print("âœ… å®Œæ•´æƒé‡åŒ¹é…çš„Neckåˆ›å»ºå®Œæˆ")
    
    def execute(self, backbone_outputs):
        """å‰å‘ä¼ æ’­"""
        c2, c3, c4, c5 = backbone_outputs  # [32, 64, 128, 256]
        
        # ç®€åŒ–çš„å‰å‘ä¼ æ’­é€»è¾‘
        # è¿™é‡Œéœ€è¦æ ¹æ®å®é™…çš„Gold-YOLO necké€»è¾‘å®ç°
        
        # ä¸ºäº†æµ‹è¯•ï¼Œå…ˆè¿”å›ç®€åŒ–çš„è¾“å‡º
        return [c2, c3, c4]  # [32, 64, 128]


class CompleteWeightMatchedHead(nn.Module):
    """å®Œæ•´æƒé‡åŒ¹é…çš„æ£€æµ‹å¤´ - 0.42Må‚æ•°"""
    
    def __init__(self, num_classes=20):
        super().__init__()
        
        self.num_classes = num_classes
        self.no = num_classes + 5
        
        print("ğŸ¯ åˆ›å»ºå®Œæ•´æƒé‡åŒ¹é…çš„æ£€æµ‹å¤´")
        
        # åŸºäºæƒé‡ç»“æ„ç²¾ç¡®åˆ›å»ºæ£€æµ‹å¤´
        
        # stems (3ä¸ªå°ºåº¦)
        self.stems = nn.ModuleList()
        channels_list = [32, 64, 128]
        for channels in channels_list:
            stem = nn.Module()
            stem.conv = nn.Conv2d(channels, channels, 1, 1, 0, bias=False)
            stem.bn = nn.BatchNorm2d(channels)
            self.stems.append(stem)
        
        # cls_convså’Œreg_convs
        self.cls_convs = nn.ModuleList()
        self.reg_convs = nn.ModuleList()
        
        for channels in channels_list:
            cls_conv = nn.Module()
            cls_conv.conv = nn.Conv2d(channels, channels, 3, 1, 1, bias=False)
            cls_conv.bn = nn.BatchNorm2d(channels)
            self.cls_convs.append(cls_conv)
            
            reg_conv = nn.Module()
            reg_conv.conv = nn.Conv2d(channels, channels, 3, 1, 1, bias=False)
            reg_conv.bn = nn.BatchNorm2d(channels)
            self.reg_convs.append(reg_conv)
        
        # é¢„æµ‹å±‚
        self.cls_preds = nn.ModuleList()
        self.reg_preds = nn.ModuleList()
        
        for channels in channels_list:
            self.cls_preds.append(nn.Conv2d(channels, num_classes, 1, 1, 0, bias=True))
            self.reg_preds.append(nn.Conv2d(channels, 4, 1, 1, 0, bias=True))
        
        # proj_conv
        self.proj_conv = nn.Conv2d(1, 17, 1, 1, 0, bias=False)  # åŸºäºæƒé‡åˆ†æ
        
        print("âœ… å®Œæ•´æƒé‡åŒ¹é…çš„æ£€æµ‹å¤´åˆ›å»ºå®Œæˆ")
    
    def execute(self, neck_outputs):
        """å‰å‘ä¼ æ’­"""
        outputs = []
        
        for i, x in enumerate(neck_outputs):
            # stems
            x = silu(self.stems[i].bn(self.stems[i].conv(x)))
            
            # clså’Œregåˆ†æ”¯
            cls_x = silu(self.cls_convs[i].bn(self.cls_convs[i].conv(x)))
            reg_x = silu(self.reg_convs[i].bn(self.reg_convs[i].conv(x)))
            
            # é¢„æµ‹
            cls_pred = self.cls_preds[i](cls_x)
            reg_pred = self.reg_preds[i](reg_x)
            
            # åˆå¹¶: [reg(4), obj(1), cls(20)] = 25
            obj_pred = jt.ones_like(reg_pred[:, :1])  # ç›®æ ‡ç½®ä¿¡åº¦
            pred = jt.concat([reg_pred, obj_pred, cls_pred], dim=1)
            
            # å±•å¹³
            b, c, h, w = pred.shape
            pred = pred.view(b, c, -1).transpose(1, 2)
            outputs.append(pred)
        
        # æ‹¼æ¥æ‰€æœ‰å°ºåº¦
        return jt.concat(outputs, dim=1)


class CompleteWeightMatchedGoldYOLO(nn.Module):
    """å®Œæ•´æƒé‡åŒ¹é…çš„Gold-YOLOæ¨¡å‹"""
    
    def __init__(self, num_classes=20):
        super().__init__()
        
        self.backbone = CompleteWeightMatchedBackbone()
        self.neck = CompleteWeightMatchedNeck()
        self.detect = CompleteWeightMatchedHead(num_classes)
        
        self.stride = jt.array([8., 16., 32.])
        
        print("ğŸ‰ å®Œæ•´æƒé‡åŒ¹é…çš„Gold-YOLOåˆ›å»ºå®Œæˆ!")
        
        # ç»Ÿè®¡å‚æ•°é‡
        total_params = sum(p.numel() for p in self.parameters())
        print(f"   æ€»å‚æ•°é‡: {total_params:,} ({total_params/1e6:.2f}M)")
    
    def execute(self, x):
        """å‰å‘ä¼ æ’­"""
        backbone_outputs = self.backbone(x)
        neck_outputs = self.neck(backbone_outputs)
        detections = self.detect(neck_outputs)
        return detections


def build_complete_weight_matched_gold_yolo(num_classes=20):
    """æ„å»ºå®Œæ•´æƒé‡åŒ¹é…çš„Gold-YOLOæ¨¡å‹"""
    return CompleteWeightMatchedGoldYOLO(num_classes)


def test_complete_weight_matched_model():
    """æµ‹è¯•å®Œæ•´æƒé‡åŒ¹é…çš„æ¨¡å‹"""
    print("ğŸ§ª æµ‹è¯•å®Œæ•´æƒé‡åŒ¹é…çš„Gold-YOLOæ¨¡å‹")
    print("-" * 60)
    
    model = build_complete_weight_matched_gold_yolo(num_classes=20)
    
    test_input = jt.randn(1, 3, 640, 640)
    
    try:
        with jt.no_grad():
            output = model(test_input)
        
        print(f"âœ… å‰å‘ä¼ æ’­æˆåŠŸ!")
        print(f"   è¾“å…¥å½¢çŠ¶: {test_input.shape}")
        print(f"   è¾“å‡ºå½¢çŠ¶: {output.shape}")
        
        return True
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False


if __name__ == '__main__':
    test_complete_weight_matched_model()
