#!/usr/bin/env python3
# -*- coding:utf-8 -*-
"""
PyTorchç²¾ç¡®å¤åˆ¶çš„Gold-YOLO Jittoræ¨¡å‹
åŸºäºPyTorchæºç è¿›è¡Œ100%ç²¾ç¡®å¤åˆ¶ï¼Œè§£å†³æ£€æµ‹å‡†ç¡®ç‡é—®é¢˜
"""

import jittor as jt
import jittor.nn as nn
import math


def silu(x):
    """SiLUæ¿€æ´»å‡½æ•° (Swish)"""
    return x * jt.sigmoid(x)


class PyTorchExactConvBNSiLU(nn.Module):
    """ç²¾ç¡®å¤åˆ¶PyTorchçš„Conv+BN+SiLUå—"""
    
    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False):
        super().__init__()
        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding, bias=bias)
        self.bn = nn.BatchNorm2d(out_channels)
    
    def execute(self, x):
        x = self.conv(x)
        x = self.bn(x)
        return silu(x)  # ä½¿ç”¨SiLUè€Œä¸æ˜¯ReLU


class PyTorchExactRepVGGBlock(nn.Module):
    """ç²¾ç¡®å¤åˆ¶PyTorchçš„RepVGGBlock"""
    
    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, padding=1):
        super().__init__()
        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding, bias=False)
        self.bn = nn.BatchNorm2d(out_channels)
    
    def execute(self, x):
        x = self.conv(x)
        x = self.bn(x)
        return silu(x)  # ä½¿ç”¨SiLU


class PyTorchExactBackbone(nn.Module):
    """ç²¾ç¡®å¤åˆ¶PyTorchçš„Backbone"""
    
    def __init__(self):
        super().__init__()
        
        print(f"ğŸ”„ åˆ›å»ºPyTorchç²¾ç¡®å¤åˆ¶çš„Backbone")
        print(f"   ä½¿ç”¨SiLUæ¿€æ´»å‡½æ•°ï¼Œç²¾ç¡®åŒ¹é…PyTorchæ¶æ„")
        
        # åŸºäºPyTorché…ç½®çš„ç²¾ç¡®é€šé“
        # width_multiple = 0.25, depth_multiple = 0.33
        channels = [16, 32, 64, 128, 256]  # å·²ç»åº”ç”¨äº†width_multiple
        
        # Stem
        self.stem = PyTorchExactRepVGGBlock(3, channels[0], 3, 2, 1)
        
        # ERBlock_2
        self.ERBlock_2 = nn.Sequential()
        self.ERBlock_2.add_module("0", PyTorchExactRepVGGBlock(channels[0], channels[1], 3, 2, 1))
        
        # ERBlock_2.1 - RepBlock
        erblock_2_1 = nn.Module()
        erblock_2_1.conv1 = PyTorchExactRepVGGBlock(channels[1], channels[1], 3, 1, 1)
        erblock_2_1.block = nn.ModuleList()
        for i in range(1):  # depth_multipleåº”ç”¨åçš„é‡å¤æ¬¡æ•°
            erblock_2_1.block.append(PyTorchExactRepVGGBlock(channels[1], channels[1], 3, 1, 1))
        self.ERBlock_2.add_module("1", erblock_2_1)
        
        # ERBlock_3
        self.ERBlock_3 = nn.Sequential()
        self.ERBlock_3.add_module("0", PyTorchExactRepVGGBlock(channels[1], channels[2], 3, 2, 1))
        
        # ERBlock_3.1 - RepBlock
        erblock_3_1 = nn.Module()
        erblock_3_1.conv1 = PyTorchExactRepVGGBlock(channels[2], channels[2], 3, 1, 1)
        erblock_3_1.block = nn.ModuleList()
        for i in range(3):  # depth_multipleåº”ç”¨åçš„é‡å¤æ¬¡æ•°
            erblock_3_1.block.append(PyTorchExactRepVGGBlock(channels[2], channels[2], 3, 1, 1))
        self.ERBlock_3.add_module("1", erblock_3_1)
        
        # ERBlock_4
        self.ERBlock_4 = nn.Sequential()
        self.ERBlock_4.add_module("0", PyTorchExactRepVGGBlock(channels[2], channels[3], 3, 2, 1))
        
        # ERBlock_4.1 - RepBlock
        erblock_4_1 = nn.Module()
        erblock_4_1.conv1 = PyTorchExactRepVGGBlock(channels[3], channels[3], 3, 1, 1)
        erblock_4_1.block = nn.ModuleList()
        for i in range(5):  # depth_multipleåº”ç”¨åçš„é‡å¤æ¬¡æ•°
            erblock_4_1.block.append(PyTorchExactRepVGGBlock(channels[3], channels[3], 3, 1, 1))
        self.ERBlock_4.add_module("1", erblock_4_1)
        
        # ERBlock_5
        self.ERBlock_5 = nn.Sequential()
        self.ERBlock_5.add_module("0", PyTorchExactRepVGGBlock(channels[3], channels[4], 3, 2, 1))
        
        # ERBlock_5.1 - RepBlock
        erblock_5_1 = nn.Module()
        erblock_5_1.conv1 = PyTorchExactRepVGGBlock(channels[4], channels[4], 3, 1, 1)
        erblock_5_1.block = nn.ModuleList()
        for i in range(1):  # depth_multipleåº”ç”¨åçš„é‡å¤æ¬¡æ•°
            erblock_5_1.block.append(PyTorchExactRepVGGBlock(channels[4], channels[4], 3, 1, 1))
        self.ERBlock_5.add_module("1", erblock_5_1)
        
        # ERBlock_5.2 - SPPF
        erblock_5_2 = nn.Module()
        c_ = channels[4] // 2  # 128
        erblock_5_2.cv1 = PyTorchExactConvBNSiLU(channels[4], c_, 1, 1, 0)
        erblock_5_2.cv2 = PyTorchExactConvBNSiLU(c_ * 4, channels[4], 1, 1, 0)
        erblock_5_2.m = nn.MaxPool2d(kernel_size=5, stride=1, padding=2)
        self.ERBlock_5.add_module("2", erblock_5_2)
        
        print("âœ… PyTorchç²¾ç¡®å¤åˆ¶çš„Backboneåˆ›å»ºå®Œæˆ")
    
    def execute(self, x):
        """å‰å‘ä¼ æ’­ - ç²¾ç¡®åŒ¹é…PyTorché€»è¾‘"""
        # Stem
        x = self.stem(x)
        
        # ERBlock_2
        x = self.ERBlock_2[0](x)
        x = self.ERBlock_2[1].conv1(x)
        for block in self.ERBlock_2[1].block:
            x = block(x)
        c2 = x
        
        # ERBlock_3
        x = self.ERBlock_3[0](c2)
        x = self.ERBlock_3[1].conv1(x)
        for block in self.ERBlock_3[1].block:
            x = block(x)
        c3 = x
        
        # ERBlock_4
        x = self.ERBlock_4[0](c3)
        x = self.ERBlock_4[1].conv1(x)
        for block in self.ERBlock_4[1].block:
            x = block(x)
        c4 = x
        
        # ERBlock_5
        x = self.ERBlock_5[0](c4)
        x = self.ERBlock_5[1].conv1(x)
        for block in self.ERBlock_5[1].block:
            x = block(x)
        
        # SPPF
        sppf = self.ERBlock_5[2]
        x = sppf.cv1(x)
        y1 = sppf.m(x)
        y2 = sppf.m(y1)
        y3 = sppf.m(y2)
        x = sppf.cv2(jt.concat([x, y1, y2, y3], 1))
        c5 = x
        
        return [c2, c3, c4, c5]  # [32, 64, 128, 256]


class PyTorchExactNeck(nn.Module):
    """ç²¾ç¡®å¤åˆ¶PyTorchçš„Neck - ç®€åŒ–ç‰ˆæœ¬"""
    
    def __init__(self):
        super().__init__()
        
        print(f"ğŸ”„ åˆ›å»ºPyTorchç²¾ç¡®å¤åˆ¶çš„Neck (ç®€åŒ–ç‰ˆ)")
        
        # ç®€åŒ–çš„neckï¼Œåªä¿ç•™æ ¸å¿ƒåŠŸèƒ½
        self.reduce_layer = PyTorchExactConvBNSiLU(256, 128, 1, 1, 0)
        
        print("âœ… PyTorchç²¾ç¡®å¤åˆ¶çš„Neckåˆ›å»ºå®Œæˆ")
    
    def execute(self, backbone_outputs):
        """å‰å‘ä¼ æ’­"""
        c2, c3, c4, c5 = backbone_outputs
        
        # ç®€åŒ–å¤„ç†ï¼Œç›´æ¥è¿”å›å¤šå°ºåº¦ç‰¹å¾
        p5 = self.reduce_layer(c5)  # 256->128
        p4 = c4  # 128
        p3 = c3  # 64
        
        return [p3, p4, p5]  # [64, 128, 128]


class PyTorchExactHead(nn.Module):
    """ç²¾ç¡®å¤åˆ¶PyTorchçš„æ£€æµ‹å¤´"""
    
    def __init__(self, num_classes=20):
        super().__init__()
        
        self.num_classes = num_classes
        self.no = num_classes + 5  # è¾“å‡ºé€šé“æ•°
        
        print(f"ğŸ”„ åˆ›å»ºPyTorchç²¾ç¡®å¤åˆ¶çš„æ£€æµ‹å¤´")
        print(f"   ç±»åˆ«æ•°: {num_classes}, è¾“å‡ºé€šé“: {self.no}")
        
        # å¤šå°ºåº¦è¾“å…¥é€šé“
        ch = [64, 128, 128]  # P3, P4, P5
        
        # æ£€æµ‹å¤´
        self.m = nn.ModuleList()
        for i in range(3):  # 3ä¸ªæ£€æµ‹å°ºåº¦
            self.m.append(nn.Conv2d(ch[i], self.no, 1))  # ç›´æ¥è¾“å‡º
        
        # åˆå§‹åŒ–
        self._initialize_biases()
        
        print("âœ… PyTorchç²¾ç¡®å¤åˆ¶çš„æ£€æµ‹å¤´åˆ›å»ºå®Œæˆ")
    
    def _initialize_biases(self):
        """åˆå§‹åŒ–åç½® - åŒ¹é…PyTorch"""
        for mi in self.m:
            b = mi.bias.view(-1)
            # ç›®æ ‡ç½®ä¿¡åº¦åç½®åˆå§‹åŒ–
            b[4] = math.log(8 / (640 / 32) ** 2)  # obj
            # ç±»åˆ«åç½®åˆå§‹åŒ–
            b[5:] = math.log(0.6 / (self.num_classes - 0.99))
            mi.bias = b.view(-1)
    
    def execute(self, x):
        """å‰å‘ä¼ æ’­"""
        z = []  # æ¨ç†è¾“å‡º
        
        for i in range(3):
            x[i] = self.m[i](x[i])  # conv
            bs, _, ny, nx = x[i].shape
            x[i] = x[i].view(bs, self.no, ny * nx).transpose(1, 2)  # [bs, no, ny*nx] -> [bs, ny*nx, no]
            z.append(x[i])
        
        return jt.concat(z, 1)  # æ‹¼æ¥æ‰€æœ‰å°ºåº¦


class PyTorchExactGoldYOLO(nn.Module):
    """ç²¾ç¡®å¤åˆ¶PyTorchçš„Gold-YOLOæ¨¡å‹"""
    
    def __init__(self, num_classes=20):
        super().__init__()
        
        self.backbone = PyTorchExactBackbone()
        self.neck = PyTorchExactNeck()
        self.detect = PyTorchExactHead(num_classes)
        
        # æ£€æµ‹å°ºåº¦
        self.stride = jt.array([8., 16., 32.])
        
        print("ğŸ‰ PyTorchç²¾ç¡®å¤åˆ¶çš„Gold-YOLOåˆ›å»ºå®Œæˆ!")
        
        # ç»Ÿè®¡å‚æ•°é‡
        total_params = sum(p.numel() for p in self.parameters())
        print(f"   æ€»å‚æ•°é‡: {total_params:,} ({total_params/1e6:.2f}M)")
    
    def execute(self, x):
        """å‰å‘ä¼ æ’­"""
        # Backbone
        backbone_outputs = self.backbone(x)
        
        # Neck
        neck_outputs = self.neck(backbone_outputs)
        
        # Head
        detections = self.detect(neck_outputs)
        
        return detections


def build_pytorch_exact_gold_yolo(num_classes=20):
    """æ„å»ºPyTorchç²¾ç¡®å¤åˆ¶çš„Gold-YOLOæ¨¡å‹"""
    return PyTorchExactGoldYOLO(num_classes)


def test_pytorch_exact_model():
    """æµ‹è¯•PyTorchç²¾ç¡®å¤åˆ¶çš„æ¨¡å‹"""
    print("ğŸ§ª æµ‹è¯•PyTorchç²¾ç¡®å¤åˆ¶çš„Gold-YOLOæ¨¡å‹")
    print("-" * 60)
    
    # åˆ›å»ºæ¨¡å‹
    model = build_pytorch_exact_gold_yolo(num_classes=20)
    
    # æµ‹è¯•å‰å‘ä¼ æ’­
    test_input = jt.randn(1, 3, 640, 640)
    
    try:
        with jt.no_grad():
            output = model(test_input)
        
        print(f"âœ… å‰å‘ä¼ æ’­æˆåŠŸ!")
        print(f"   è¾“å…¥å½¢çŠ¶: {test_input.shape}")
        print(f"   è¾“å‡ºå½¢çŠ¶: {output.shape}")
        
        # åˆ†æè¾“å‡º
        output_sigmoid = jt.sigmoid(output)
        output_np = output_sigmoid.numpy()[0]
        
        obj_conf = output_np[:, 4]
        cls_probs = output_np[:, 5:]
        max_cls_probs = np.max(cls_probs, axis=1)
        total_conf = obj_conf * max_cls_probs
        
        print(f"   æœ€é«˜ç½®ä¿¡åº¦: {total_conf.max():.6f}")
        print(f"   >0.1æ£€æµ‹æ•°: {(total_conf > 0.1).sum()}")
        print(f"   >0.05æ£€æµ‹æ•°: {(total_conf > 0.05).sum()}")
        
        return True
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False


if __name__ == '__main__':
    import numpy as np
    test_pytorch_exact_model()
