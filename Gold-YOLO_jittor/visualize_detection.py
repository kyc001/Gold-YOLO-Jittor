#!/usr/bin/env python3
"""
å¯è§†åŒ–æ£€æµ‹ç»“æœè„šæœ¬ - å¯¹é½PyTorchç‰ˆæœ¬
æ”¯æŒè®­ç»ƒè¿‡ç¨‹å’Œæ¨ç†è¿‡ç¨‹çš„å¯è§†åŒ–
"""

import os
import sys
import cv2
import numpy as np
import jittor as jt
import math
from pathlib import Path

# æ·»åŠ è·¯å¾„
sys.path.append('.')
sys.path.append('./yolov6')

from models.perfect_gold_yolo import create_perfect_gold_yolo_model
from yolov6.data.data_augment import letterbox
from yolov6.models.losses import ComputeLoss
from yolov6.utils.nms import non_max_suppression

# VOCæ•°æ®é›†ç±»åˆ«åç§°
VOC_CLASSES = [
    'aeroplane', 'bicycle', 'bird', 'boat', 'bottle', 'bus', 'car', 'cat', 'chair', 'cow',
    'diningtable', 'dog', 'horse', 'motorbike', 'person', 'pottedplant', 'sheep', 'sofa', 'train', 'tvmonitor'
]

# ç±»åˆ«é¢œè‰² - å¯¹é½PyTorchç‰ˆæœ¬
COLORS = np.array([
    [255, 178, 50], [255, 0, 0], [0, 255, 0], [0, 0, 255], [255, 255, 0],
    [255, 0, 255], [0, 255, 255], [128, 0, 0], [0, 128, 0], [0, 0, 128],
    [128, 128, 0], [128, 0, 128], [0, 128, 128], [192, 192, 192], [128, 128, 128],
    [255, 165, 0], [255, 20, 147], [0, 191, 255], [255, 105, 180], [34, 139, 34]
], dtype=np.uint8)

def pytorch_exact_initialization(model):
    """å®Œå…¨ç…§æŠ„PyTorchç‰ˆæœ¬çš„åˆå§‹åŒ–"""
    for name, module in model.named_modules():
        if hasattr(module, 'initialize_biases'):
            module.initialize_biases()
            break
    return model

def plot_box_and_label(image, lw, box, label='', color=(128, 128, 128), txt_color=(255, 255, 255)):
    """ç»˜åˆ¶æ£€æµ‹æ¡†å’Œæ ‡ç­¾ - å®Œå…¨å¯¹é½PyTorchç‰ˆæœ¬"""
    p1, p2 = (int(box[0]), int(box[1])), (int(box[2]), int(box[3]))
    cv2.rectangle(image, p1, p2, color, thickness=lw, lineType=cv2.LINE_AA)
    
    if label:
        tf = max(lw - 1, 1)  # font thickness
        w, h = cv2.getTextSize(label, 0, fontScale=lw / 3, thickness=tf)[0]  # text width, height
        outside = p1[1] - h - 3 >= 0  # label fits outside box
        p2 = p1[0] + w, p1[1] - h - 3 if outside else p1[1] + h + 3
        cv2.rectangle(image, p1, p2, color, -1, cv2.LINE_AA)  # filled
        cv2.putText(image, label, (p1[0], p1[1] - 2 if outside else p1[1] + h + 2), 
                   cv2.FONT_HERSHEY_COMPLEX, lw / 3, txt_color, thickness=tf, lineType=cv2.LINE_AA)

def visualize_detections(image, detections, conf_thres=0.25, hide_labels=False, hide_conf=False):
    """å¯è§†åŒ–æ£€æµ‹ç»“æœ - å¯¹é½PyTorchç‰ˆæœ¬"""
    img_vis = image.copy()
    lw = max(round(sum(img_vis.shape) / 2 * 0.003), 2)  # line width
    
    detection_count = 0
    class_counts = {}
    
    if len(detections) > 0:
        # è½¬æ¢ä¸ºnumpy
        if hasattr(detections, 'numpy'):
            detections = detections.numpy()

        # ç¡®ä¿æ£€æµ‹ç»“æœæ˜¯2ç»´çš„
        if detections.ndim == 3:
            detections = detections.reshape(-1, detections.shape[-1])

        # å¤„ç†æ£€æµ‹ç»“æœ
        for detection in detections:
            if len(detection) >= 6:
                xyxy = detection[:4]
                conf = detection[4]
                cls = detection[5]
            if conf >= conf_thres:
                detection_count += 1
                class_num = int(cls)
                class_name = VOC_CLASSES[class_num] if class_num < len(VOC_CLASSES) else f'Class{class_num}'
                
                # ç»Ÿè®¡ç±»åˆ«æ•°é‡
                class_counts[class_name] = class_counts.get(class_name, 0) + 1
                
                # ç”Ÿæˆæ ‡ç­¾
                if hide_labels:
                    label = None
                elif hide_conf:
                    label = class_name
                else:
                    label = f'{class_name} {conf:.2f}'
                
                # è·å–é¢œè‰²
                color = COLORS[class_num % len(COLORS)].tolist()
                
                # ç»˜åˆ¶æ£€æµ‹æ¡†å’Œæ ‡ç­¾
                plot_box_and_label(img_vis, lw, xyxy, label, color=color)
            else:
                print(f"   âš ï¸ æ£€æµ‹ç»“æœæ ¼å¼é”™è¯¯: {detection}")
    
    return img_vis, detection_count, class_counts

def analyze_detection_results(detections, conf_thres=0.25):
    """åˆ†ææ£€æµ‹ç»“æœ - è¯¦ç»†ç»Ÿè®¡"""
    if len(detections) == 0:
        return {
            'total_detections': 0,
            'class_counts': {},
            'confidence_stats': {},
            'confidence_distribution': []
        }
    
    # è½¬æ¢ä¸ºnumpy
    if hasattr(detections, 'numpy'):
        detections = detections.numpy()
    
    total_detections = 0
    class_counts = {}
    confidence_stats = {}
    confidence_distribution = []
    
    for detection in detections:
        if len(detection) >= 6:
            xyxy = detection[:4]
            conf = detection[4]
            cls = detection[5]
        else:
            continue

        if conf >= conf_thres:
            total_detections += 1
            class_num = int(cls)
            class_name = VOC_CLASSES[class_num] if class_num < len(VOC_CLASSES) else f'Class{class_num}'
            
            # ç»Ÿè®¡ç±»åˆ«æ•°é‡
            class_counts[class_name] = class_counts.get(class_name, 0) + 1
            
            # ç»Ÿè®¡ç½®ä¿¡åº¦
            if class_name not in confidence_stats:
                confidence_stats[class_name] = []
            confidence_stats[class_name].append(float(conf))
            confidence_distribution.append((class_name, float(conf)))
    
    # è®¡ç®—ç½®ä¿¡åº¦ç»Ÿè®¡
    for class_name in confidence_stats:
        confs = confidence_stats[class_name]
        confidence_stats[class_name] = {
            'count': len(confs),
            'max': max(confs),
            'min': min(confs),
            'mean': sum(confs) / len(confs),
            'std': np.std(confs)
        }
    
    return {
        'total_detections': total_detections,
        'class_counts': class_counts,
        'confidence_stats': confidence_stats,
        'confidence_distribution': sorted(confidence_distribution, key=lambda x: x[1], reverse=True)
    }

def visualize_training_progress(model, img_tensor, targets_tensor, annotations, epoch, save_dir):
    """å¯è§†åŒ–è®­ç»ƒè¿›åº¦"""
    model.eval()
    
    with jt.no_grad():
        # å‰å‘ä¼ æ’­
        outputs = model(img_tensor)
        
        # NMSå¤„ç†
        pred = non_max_suppression(outputs, conf_thres=0.01, iou_thres=0.45, max_det=100)
        
        if len(pred) > 0 and len(pred[0]) > 0:
            detections = pred[0]
        else:
            detections = jt.array([])
        
        # å‡†å¤‡åŸå§‹å›¾åƒ
        img_path = "/home/kyc/project/GOLD-YOLO/2008_001420.jpg"
        original_img = cv2.imread(img_path)
        
        # å¯è§†åŒ–æ£€æµ‹ç»“æœ
        img_vis, detection_count, class_counts = visualize_detections(
            original_img, detections, conf_thres=0.01, hide_conf=False
        )
        
        # åˆ†ææ£€æµ‹ç»“æœ
        analysis = analyze_detection_results(detections, conf_thres=0.01)
        
        # æ·»åŠ è®­ç»ƒä¿¡æ¯
        info_text = [
            f"Epoch: {epoch}",
            f"Detections: {detection_count}",
            f"Expected: {len(annotations)}",
            f"Classes: {list(class_counts.keys())}"
        ]
        
        # ç»˜åˆ¶ä¿¡æ¯æ–‡æœ¬
        y_offset = 30
        for text in info_text:
            cv2.putText(img_vis, text, (10, y_offset), cv2.FONT_HERSHEY_SIMPLEX, 
                       0.7, (0, 255, 0), 2, cv2.LINE_AA)
            y_offset += 25
        
        # ä¿å­˜å¯è§†åŒ–ç»“æœ
        save_path = save_dir / f'epoch_{epoch:03d}_detection.jpg'
        cv2.imwrite(str(save_path), img_vis)
        
        return analysis, str(save_path)

def test_current_model_visualization():
    """æµ‹è¯•å½“å‰æ¨¡å‹çš„å¯è§†åŒ–æ•ˆæœ"""
    print(f"ğŸ”§ æµ‹è¯•å½“å‰æ¨¡å‹çš„å¯è§†åŒ–æ•ˆæœ")
    print("=" * 50)
    
    # å‡†å¤‡æ•°æ®
    label_file = "/home/kyc/project/GOLD-YOLO/2008_001420.txt"
    img_path = "/home/kyc/project/GOLD-YOLO/2008_001420.jpg"
    
    # è¯»å–çœŸå®æ ‡æ³¨
    annotations = []
    with open(label_file, 'r') as f:
        for line in f:
            line = line.strip()
            if line:
                parts = line.split()
                if len(parts) >= 5:
                    cls_id = int(parts[0])
                    x_center = float(parts[1])
                    y_center = float(parts[2])
                    width = float(parts[3])
                    height = float(parts[4])
                    annotations.append([cls_id, x_center, y_center, width, height])
    
    target_counts = {}
    for ann in annotations:
        cls_name = VOC_CLASSES[ann[0]]
        target_counts[cls_name] = target_counts.get(cls_name, 0) + 1
    
    print(f"ğŸ“‹ æœŸæœ›æ£€æµ‹ç»“æœ: {target_counts}")
    print(f"   æ€»ç›®æ ‡æ•°: {len(annotations)}")
    
    # å‡†å¤‡è¾“å…¥
    original_img = cv2.imread(img_path)
    img = letterbox(original_img, new_shape=640, stride=32, auto=False)[0]
    img = img.transpose((2, 0, 1))[::-1]
    img = np.ascontiguousarray(img)
    img = img.astype(np.float32) / 255.0
    img_tensor = jt.array(img).unsqueeze(0)
    
    # å‡†å¤‡æ ‡ç­¾
    targets = []
    for ann in annotations:
        cls_id, x_center, y_center, width, height = ann
        targets.append([0, cls_id, x_center, y_center, width, height])
    targets_tensor = jt.array(targets, dtype=jt.float32).unsqueeze(0)
    
    # åˆ›å»ºæ¨¡å‹
    model = create_perfect_gold_yolo_model()
    model = pytorch_exact_initialization(model)
    model.train()
    
    # åˆ›å»ºæŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨
    loss_fn = ComputeLoss(
        num_classes=20,
        ori_img_size=640,
        warmup_epoch=4,
        use_dfl=False,
        reg_max=0,
        iou_type='giou',
        loss_weight={'class': 1.0, 'iou': 2.5, 'dfl': 0.5}
    )
    optimizer = jt.optim.AdamW(model.parameters(), lr=0.05)
    
    # åˆ›å»ºä¿å­˜ç›®å½•
    save_dir = Path("runs/visualization_test")
    save_dir.mkdir(parents=True, exist_ok=True)
    
    print(f"\nğŸš€ å¿«é€Ÿè®­ç»ƒå¹¶å¯è§†åŒ–:")
    
    # è®­ç»ƒå¹¶å¯è§†åŒ–
    for epoch in range(0, 201, 50):  # 0, 50, 100, 150, 200
        if epoch > 0:
            # è®­ç»ƒ50è½®
            model.train()  # ç¡®ä¿è®­ç»ƒæ¨¡å¼
            for _ in range(50):
                outputs = model(img_tensor)
                loss, loss_items = loss_fn(outputs, targets_tensor, epoch_num=epoch, step_num=1)
                optimizer.step(loss)
        
        # å¯è§†åŒ–å½“å‰çŠ¶æ€
        analysis, save_path = visualize_training_progress(
            model, img_tensor, targets_tensor, annotations, epoch, save_dir
        )
        
        print(f"\n   Epoch {epoch}:")
        print(f"     æ£€æµ‹æ•°é‡: {analysis['total_detections']}")
        print(f"     æ£€æµ‹ç±»åˆ«: {list(analysis['class_counts'].keys())}")
        print(f"     ç½®ä¿¡åº¦ç»Ÿè®¡:")
        
        for class_name, stats in analysis['confidence_stats'].items():
            print(f"       {class_name}: æœ€å¤§{stats['max']:.3f}, å¹³å‡{stats['mean']:.3f}, æ•°é‡{stats['count']}")
        
        print(f"     å¯è§†åŒ–ä¿å­˜: {save_path}")
        
        # æ£€æŸ¥æ˜¯å¦æ£€æµ‹åˆ°æœŸæœ›ç±»åˆ«
        expected_classes = set(target_counts.keys())
        detected_classes = set(analysis['class_counts'].keys())
        correct_classes = expected_classes.intersection(detected_classes)
        
        if len(correct_classes) > 0:
            print(f"     âœ… æ£€æµ‹åˆ°æ­£ç¡®ç±»åˆ«: {correct_classes}")
            species_accuracy = len(correct_classes) / len(expected_classes)
            print(f"     ç§ç±»å‡†ç¡®ç‡: {species_accuracy*100:.1f}%")
            
            if species_accuracy >= 0.8:
                print(f"\nğŸ‰ ç§ç±»è¯†åˆ«æˆåŠŸï¼å¯ä»¥å¼€å§‹200è½®å®Œæ•´è®­ç»ƒï¼")
                return True
        else:
            print(f"     âŒ æœªæ£€æµ‹åˆ°æ­£ç¡®ç±»åˆ«ï¼ŒæœŸæœ›: {expected_classes}")
    
    return False

def main():
    print("ğŸ”¥ å¯è§†åŒ–æ£€æµ‹ç»“æœè„šæœ¬")
    print("=" * 70)
    print("åŠŸèƒ½ï¼šå¯è§†åŒ–æ£€æµ‹ç»“æœï¼Œåˆ†æç½®ä¿¡åº¦å’Œæ•°é‡é—®é¢˜")
    print("å¯¹é½ï¼šå®Œå…¨å¯¹é½PyTorchç‰ˆæœ¬çš„å¯è§†åŒ–å®ç°")
    print("=" * 70)
    
    success = test_current_model_visualization()
    
    if success:
        print(f"\nğŸ‰ğŸ‰ğŸ‰ å¯è§†åŒ–æµ‹è¯•æˆåŠŸï¼ğŸ‰ğŸ‰ğŸ‰")
        print(f"âœ… æ£€æµ‹ç»“æœå¯è§†åŒ–æ­£å¸¸")
        print(f"âœ… ç½®ä¿¡åº¦åˆ†æå®Œæ•´")
        print(f"âœ… å¯ä»¥å¼€å§‹200è½®å®Œæ•´è®­ç»ƒ")
    else:
        print(f"\nâš ï¸ éœ€è¦è¿›ä¸€æ­¥åˆ†æç½®ä¿¡åº¦é—®é¢˜")

if __name__ == "__main__":
    main()
