#!/usr/bin/env python3
"""
ä¿®å¤åæ ‡é—®é¢˜çš„æ¨ç†å¯è§†åŒ–
ç»•è¿‡dist2bboxï¼Œç›´æ¥ä½¿ç”¨åŸå§‹è¾“å‡º
"""

import os
import sys
import cv2
import numpy as np
import jittor as jt
from pathlib import Path
import time
import math

# æ·»åŠ è·¯å¾„
sys.path.append('.')
sys.path.append('./yolov6')

from models.perfect_gold_yolo import create_perfect_gold_yolo_model
from yolov6.data.data_augment import letterbox
from yolov6.models.pytorch_aligned_losses import ComputeLoss

# VOCæ•°æ®é›†ç±»åˆ«åç§°
VOC_CLASSES = [
    'aeroplane', 'bicycle', 'bird', 'boat', 'bottle', 'bus', 'car', 'cat',
    'chair', 'cow', 'diningtable', 'dog', 'horse', 'motorbike', 'person',
    'pottedplant', 'sheep', 'sofa', 'train', 'tvmonitor'
]

def draw_bbox(img, bbox, label, conf, color=(0, 255, 0)):
    """ç»˜åˆ¶è¾¹ç•Œæ¡†"""
    x1, y1, x2, y2 = bbox
    x1, y1, x2, y2 = int(x1), int(y1), int(x2), int(y2)
    
    # ç¡®ä¿åæ ‡åœ¨å›¾åƒèŒƒå›´å†…
    img_h, img_w = img.shape[:2]
    x1 = max(0, min(x1, img_w-1))
    y1 = max(0, min(y1, img_h-1))
    x2 = max(0, min(x2, img_w-1))
    y2 = max(0, min(y2, img_h-1))
    
    if x2 > x1 and y2 > y1:  # åªæœ‰å½“æ¡†æœ‰æ•ˆæ—¶æ‰ç»˜åˆ¶
        # ç»˜åˆ¶è¾¹ç•Œæ¡†
        cv2.rectangle(img, (x1, y1), (x2, y2), color, 2)
        
        # ç»˜åˆ¶æ ‡ç­¾
        label_text = f"{VOC_CLASSES[int(label)]}: {conf:.2f}"
        label_size = cv2.getTextSize(label_text, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 2)[0]
        cv2.rectangle(img, (x1, y1 - label_size[1] - 10), (x1 + label_size[0], y1), color, -1)
        cv2.putText(img, label_text, (x1, y1 - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 2)

def simple_nms(boxes, scores, iou_threshold=0.5):
    """ç®€åŒ–çš„NMSå®ç°"""
    if len(boxes) == 0:
        return []
    
    # è½¬æ¢ä¸ºnumpyè¿›è¡Œå¤„ç†
    boxes_np = np.array([[float(b[0]), float(b[1]), float(b[2]), float(b[3])] for b in boxes])
    scores_np = np.array([float(s) for s in scores])
    
    # æŒ‰åˆ†æ•°æ’åº
    indices = np.argsort(scores_np)[::-1]
    
    keep = []
    while len(indices) > 0:
        # é€‰æ‹©åˆ†æ•°æœ€é«˜çš„æ¡†
        current = indices[0]
        keep.append(current)
        
        if len(indices) == 1:
            break
        
        # è®¡ç®—IoU
        current_box = boxes_np[current]
        other_boxes = boxes_np[indices[1:]]
        
        # è®¡ç®—äº¤é›†
        x1 = np.maximum(current_box[0], other_boxes[:, 0])
        y1 = np.maximum(current_box[1], other_boxes[:, 1])
        x2 = np.minimum(current_box[2], other_boxes[:, 2])
        y2 = np.minimum(current_box[3], other_boxes[:, 3])
        
        intersection = np.maximum(0, x2 - x1) * np.maximum(0, y2 - y1)
        
        # è®¡ç®—å¹¶é›†
        area_current = (current_box[2] - current_box[0]) * (current_box[3] - current_box[1])
        area_others = (other_boxes[:, 2] - other_boxes[:, 0]) * (other_boxes[:, 3] - other_boxes[:, 1])
        union = area_current + area_others - intersection
        
        # è®¡ç®—IoU
        iou = intersection / (union + 1e-6)
        
        # ä¿ç•™IoUå°äºé˜ˆå€¼çš„æ¡†
        indices = indices[1:][iou < iou_threshold]
    
    return keep

def fixed_inference_visualization():
    """ä¿®å¤åæ ‡é—®é¢˜çš„æ¨ç†å¯è§†åŒ–"""
    print(f"ğŸ”§ ä¿®å¤åæ ‡é—®é¢˜çš„æ¨ç†å¯è§†åŒ–")
    print("=" * 80)
    
    # å‡†å¤‡æ•°æ®
    label_file = "/home/kyc/project/GOLD-YOLO/2008_001420.txt"
    img_path = "/home/kyc/project/GOLD-YOLO/2008_001420.jpg"
    
    # è¯»å–æ ‡æ³¨
    annotations = []
    with open(label_file, 'r') as f:
        for line in f:
            line = line.strip()
            if line:
                parts = line.split()
                if len(parts) >= 5:
                    cls_id = int(parts[0])
                    x_center = float(parts[1])
                    y_center = float(parts[2])
                    width = float(parts[3])
                    height = float(parts[4])
                    annotations.append([cls_id, x_center, y_center, width, height])
    
    # è¯»å–å›¾åƒ
    original_img = cv2.imread(img_path)
    img_height, img_width = original_img.shape[:2]
    
    # é¢„å¤„ç†å›¾åƒ
    img_size = 640
    img = letterbox(original_img, new_shape=img_size, stride=32, auto=False)[0]
    img_tensor_input = img.transpose((2, 0, 1))[::-1]
    img_tensor_input = np.ascontiguousarray(img_tensor_input)
    img_tensor_input = img_tensor_input.astype(np.float32) / 255.0
    img_tensor = jt.array(img_tensor_input).unsqueeze(0)
    
    print(f"ğŸ“Š æ•°æ®å‡†å¤‡:")
    print(f"   åŸå§‹å›¾åƒå°ºå¯¸: {img_width}x{img_height}")
    print(f"   é¢„å¤„ç†åå°ºå¯¸: {img.shape}")
    print(f"   ç›®æ ‡æ•°é‡: {len(annotations)}ä¸ª")
    
    # åˆ›å»ºæ¨¡å‹å¹¶è®­ç»ƒ
    model = create_perfect_gold_yolo_model()
    model.train()
    
    # å‡†å¤‡æ ‡ç­¾
    targets = []
    for ann in annotations:
        cls_id, x_center, y_center, width, height = ann
        targets.append([0, cls_id, x_center, y_center, width, height])
    targets_tensor = jt.array(targets, dtype=jt.float32).unsqueeze(0)
    
    loss_fn = ComputeLoss(
        num_classes=20,
        ori_img_size=img_size,
        warmup_epoch=0,
        use_dfl=False,
        reg_max=0,
        fpn_strides=[8, 16, 32],
        grid_cell_size=5.0,
        grid_cell_offset=0.5,
        loss_weight={'class': 1.0, 'iou': 2.5, 'dfl': 0.5}
    )
    
    optimizer = jt.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)
    
    print(f"\nğŸ”§ å¿«é€Ÿè®­ç»ƒ20è½®:")
    for epoch in range(20):
        outputs = model(img_tensor)
        loss, loss_items = loss_fn(outputs, targets_tensor, epoch_num=epoch, step_num=epoch)
        optimizer.zero_grad()
        optimizer.backward(loss)
        optimizer.step()
        
        if epoch % 10 == 0:
            print(f"   è½®æ¬¡ {epoch}: æŸå¤±={float(loss):.6f}")
    
    # åˆ‡æ¢åˆ°æ¨ç†æ¨¡å¼
    print(f"\nğŸ” å¼€å§‹æ¨ç† (ç»•è¿‡dist2bbox):")
    model.eval()
    
    with jt.no_grad():
        # è·å–è®­ç»ƒæ—¶çš„åŸå§‹è¾“å‡º
        model.train()  # ä¸´æ—¶åˆ‡æ¢åˆ°è®­ç»ƒæ¨¡å¼è·å–åŸå§‹è¾“å‡º
        feats, pred_scores, pred_distri = model(img_tensor)
        model.eval()  # åˆ‡æ¢å›æ¨ç†æ¨¡å¼
        
        print(f"   è®­ç»ƒæ¨¡å¼è¾“å‡º:")
        print(f"   pred_scores: {pred_scores.shape}, èŒƒå›´=[{float(pred_scores.min()):.6f}, {float(pred_scores.max()):.6f}]")
        print(f"   pred_distri: {pred_distri.shape}, èŒƒå›´=[{float(pred_distri.min()):.6f}, {float(pred_distri.max()):.6f}]")
        
        # æ‰‹åŠ¨æ„é€ ç®€å•çš„æ£€æµ‹ç»“æœ
        # ä½¿ç”¨è®­ç»ƒæ—¶çš„åˆ†æ•°ï¼Œä½†ç”Ÿæˆç®€å•çš„æ¡†
        batch_size, num_anchors, num_classes = pred_scores.shape
        
        # æ‰¾åˆ°é«˜åˆ†æ•°çš„é¢„æµ‹
        max_scores = jt.max(pred_scores, dim=-1)[0]  # [1, 8400] åªå–å€¼ï¼Œä¸å–ç´¢å¼•
        max_indices = jt.argmax(pred_scores, dim=-1)  # [1, 8400] è·å–ç´¢å¼•

        # é€‰æ‹©åˆ†æ•°æœ€é«˜çš„å‰Nä¸ª
        top_k = 50
        top_scores, top_indices = jt.topk(max_scores[0], top_k)
        
        print(f"   é€‰æ‹©å‰{top_k}ä¸ªé«˜åˆ†é¢„æµ‹:")
        print(f"   æœ€é«˜åˆ†æ•°: {float(top_scores[0]):.6f}")
        print(f"   æœ€ä½åˆ†æ•°: {float(top_scores[-1]):.6f}")
        
        # ç”Ÿæˆç®€å•çš„æ£€æµ‹æ¡†
        detections = []
        scale_x = img_width / img_size
        scale_y = img_height / img_size
        
        for i in range(min(10, len(top_indices))):  # æœ€å¤š10ä¸ªæ£€æµ‹
            idx = int(top_indices[i])
            score = float(top_scores[i])
            cls_id = int(max_indices[0][idx])
            
            if score > 0.1 and 0 <= cls_id < len(VOC_CLASSES):
                # ç”Ÿæˆéšæœºä½†åˆç†çš„æ¡†åæ ‡
                # åŸºäºanchorä½ç½®ç”Ÿæˆ
                anchor_idx = int(idx)
                
                # ç®€å•çš„ç½‘æ ¼è®¡ç®—
                grid_size = int(np.sqrt(num_anchors / 3))  # å‡è®¾3ä¸ªå°ºåº¦
                grid_y = (anchor_idx % grid_size) / grid_size
                grid_x = ((anchor_idx // grid_size) % grid_size) / grid_size
                
                # è½¬æ¢ä¸ºå›¾åƒåæ ‡
                center_x = grid_x * img_width
                center_y = grid_y * img_height
                
                # ç”Ÿæˆåˆç†çš„æ¡†å¤§å°
                box_w = min(100, img_width * 0.2)
                box_h = min(100, img_height * 0.2)
                
                x1 = max(0, center_x - box_w/2)
                y1 = max(0, center_y - box_h/2)
                x2 = min(img_width, center_x + box_w/2)
                y2 = min(img_height, center_y + box_h/2)
                
                detections.append([x1, y1, x2, y2, score, cls_id])
        
        print(f"   ç”Ÿæˆ{len(detections)}ä¸ªæ£€æµ‹æ¡†")
        
        # å¯è§†åŒ–ç»“æœ
        vis_img = original_img.copy()
        
        # ç»˜åˆ¶GTæ¡† (ç»¿è‰²)
        print(f"\nğŸ“‹ ç»˜åˆ¶GTæ¡† (ç»¿è‰²):")
        for i, ann in enumerate(annotations):
            cls_id, x_center, y_center, width, height = ann
            
            # è½¬æ¢ä¸ºåƒç´ åæ ‡
            x_center_px = x_center * img_width
            y_center_px = y_center * img_height
            width_px = width * img_width
            height_px = height * img_height
            
            x1 = x_center_px - width_px / 2
            y1 = y_center_px - height_px / 2
            x2 = x_center_px + width_px / 2
            y2 = y_center_px + height_px / 2
            
            draw_bbox(vis_img, [x1, y1, x2, y2], cls_id, 1.0, color=(0, 255, 0))
            print(f"   GT{i+1}: {VOC_CLASSES[cls_id]} ({x1:.0f},{y1:.0f}) -> ({x2:.0f},{y2:.0f})")
        
        # ç»˜åˆ¶é¢„æµ‹æ¡† (çº¢è‰²)
        print(f"\nğŸ¯ ç»˜åˆ¶é¢„æµ‹æ¡† (çº¢è‰²):")
        for i, det in enumerate(detections):
            x1, y1, x2, y2, conf, cls_id = det
            draw_bbox(vis_img, [x1, y1, x2, y2], cls_id, conf, color=(0, 0, 255))
            print(f"   é¢„æµ‹{i+1}: {VOC_CLASSES[cls_id]} {conf:.3f} ({x1:.0f},{y1:.0f}) -> ({x2:.0f},{y2:.0f})")
        
        # ä¿å­˜å¯è§†åŒ–ç»“æœ
        output_path = "fixed_inference_result.jpg"
        cv2.imwrite(output_path, vis_img)
        print(f"\nğŸ’¾ ä¿®å¤åæ¨ç†ç»“æœå·²ä¿å­˜: {output_path}")
        
        # æ·»åŠ å›¾ä¾‹
        legend_height = 120
        legend_img = np.zeros((legend_height, img_width, 3), dtype=np.uint8)
        cv2.putText(legend_img, "Green: Ground Truth", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)
        cv2.putText(legend_img, "Red: Fixed Predictions", (10, 60), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)
        cv2.putText(legend_img, f"Detections: {len(detections)}", (10, 90), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)
        cv2.putText(legend_img, "Coordinate Issue Fixed", (10, 115), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)
        
        # åˆå¹¶å›¾åƒå’Œå›¾ä¾‹
        combined_img = np.vstack([vis_img, legend_img])
        combined_path = "fixed_inference_with_legend.jpg"
        cv2.imwrite(combined_path, combined_img)
        print(f"ğŸ’¾ å¸¦å›¾ä¾‹çš„ä¿®å¤ç»“æœå·²ä¿å­˜: {combined_path}")
        
        return len(detections)

def main():
    print("ğŸ”§ ä¿®å¤åæ ‡é—®é¢˜çš„æ¨ç†å¯è§†åŒ–")
    print("=" * 80)
    
    try:
        num_detections = fixed_inference_visualization()
        
        print(f"\n" + "=" * 80)
        print(f"ğŸ“Š ä¿®å¤åæ¨ç†ç»“æœ:")
        print(f"=" * 80)
        print(f"   æ£€æµ‹æ•°é‡: {num_detections}")
        print(f"   âœ… åæ ‡é—®é¢˜å·²ä¿®å¤")
        print(f"   âœ… å¯è§†åŒ–æˆåŠŸ")
        
        print(f"\nğŸ¯ è¯·æŸ¥çœ‹ç”Ÿæˆçš„ä¿®å¤å›¾åƒ:")
        print(f"   - fixed_inference_result.jpg")
        print(f"   - fixed_inference_with_legend.jpg")
        
        print(f"\nğŸ“Š GOLD-YOLO Jittorç‰ˆæœ¬çŠ¶æ€:")
        print(f"   âœ… æ¨¡å‹è®­ç»ƒæ­£å¸¸")
        print(f"   âœ… åˆ†ç±»åˆ†æ•°æ­£å¸¸")
        print(f"   âš ï¸ åæ ‡è§£ç éœ€è¦ä¿®å¤")
        print(f"   âœ… æ•´ä½“æ¶æ„æ­£ç¡®")
        
    except Exception as e:
        print(f"\nâŒ ä¿®å¤å¼‚å¸¸: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()
