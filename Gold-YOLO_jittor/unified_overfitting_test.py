#!/usr/bin/env python3
"""
ç»Ÿä¸€è¿‡æ‹Ÿåˆæµ‹è¯•è„šæœ¬ - å¯å¤ç”¨ã€å¯ç»´æŠ¤
è§£å†³æ‚¨æå‡ºçš„4ä¸ªæ ¸å¿ƒé—®é¢˜ï¼š
1. è®­ç»ƒé€Ÿåº¦æ…¢çš„åŸå› åˆ†æå’Œä¿®å¤
2. ç‰©ä½“æ£€æµ‹ä½ç½®é”™è¯¯çš„åŸå› åˆ†æ
3. ä¸¤å¼ å›¾ç‰‡è¿‡æ‹Ÿåˆè®­ç»ƒæµ‹è¯•ï¼ˆç§ç±»ã€æ•°é‡ã€ä½ç½®å…¨éƒ¨æ­£ç¡®ï¼‰
4. æ¸…ç†è„šæœ¬ï¼Œå¢å¼ºå¯å¤ç”¨æ€§å’Œå¯ç»´æŠ¤æ€§

ä½¿ç”¨æ–¹æ³•ï¼š
python unified_overfitting_test.py --image 1  # æµ‹è¯•ç¬¬ä¸€å¼ å›¾ç‰‡
python unified_overfitting_test.py --image 2  # æµ‹è¯•ç¬¬äºŒå¼ å›¾ç‰‡
python unified_overfitting_test.py --image both  # æµ‹è¯•ä¸¤å¼ å›¾ç‰‡
"""

import os
import sys
import cv2
import numpy as np
import jittor as jt
from pathlib import Path
import time
import matplotlib.pyplot as plt
import argparse

# æ·»åŠ è·¯å¾„
sys.path.append('.')
sys.path.append('./yolov6')

from models.perfect_gold_yolo import create_perfect_gold_yolo_model
from yolov6.data.data_augment import letterbox
from yolov6.models.pytorch_aligned_losses import ComputeLoss
from yolov6.utils.nms import non_max_suppression

# VOCæ•°æ®é›†ç±»åˆ«åç§°
VOC_CLASSES = [
    'aeroplane', 'bicycle', 'bird', 'boat', 'bottle', 'bus', 'car', 'cat', 'chair', 'cow',
    'diningtable', 'dog', 'horse', 'motorbike', 'person', 'pottedplant', 'sheep', 'sofa', 'train', 'tvmonitor'
]

# å›¾ç‰‡é…ç½®
IMAGE_CONFIGS = {
    1: {
        'img_path': '/home/kyc/project/GOLD-YOLO/2008_001420.jpg',
        'label_path': '/home/kyc/project/GOLD-YOLO/2008_001420.txt',
        'expected_classes': {'dog', 'person', 'boat'},
        'expected_count': 6,
        'description': 'ç¬¬ä¸€å¼ å›¾ç‰‡ï¼šdog(4), person(1), boat(1)'
    },
    2: {
        'img_path': '/home/kyc/project/GOLD-YOLO/2011_002881.jpg', 
        'label_path': '/home/kyc/project/GOLD-YOLO/2011_002881.txt',
        'expected_classes': {'diningtable', 'person', 'sofa'},
        'expected_count': 7,
        'description': 'ç¬¬äºŒå¼ å›¾ç‰‡ï¼šdiningtable(3), person(3), sofa(1)'
    }
}

def pytorch_exact_initialization(model):
    """å®Œå…¨ç…§æŠ„PyTorchç‰ˆæœ¬çš„åˆå§‹åŒ–"""
    for name, module in model.named_modules():
        if hasattr(module, 'initialize_biases'):
            module.initialize_biases()
            break
    return model

def calculate_iou(box1, box2):
    """è®¡ç®—ä¸¤ä¸ªæ¡†çš„IoU"""
    x1 = max(box1[0], box2[0])
    y1 = max(box1[1], box2[1])
    x2 = min(box1[2], box2[2])
    y2 = min(box1[3], box2[3])
    
    if x2 <= x1 or y2 <= y1:
        return 0.0
    
    intersection = (x2 - x1) * (y2 - y1)
    area1 = (box1[2] - box1[0]) * (box1[3] - box1[1])
    area2 = (box2[2] - box2[0]) * (box2[3] - box2[1])
    union = area1 + area2 - intersection
    
    return intersection / union if union > 0 else 0.0

def match_detections_to_gt(detections, gt_boxes, gt_classes, iou_threshold=0.3):
    """å°†æ£€æµ‹ç»“æœä¸çœŸå®æ¡†åŒ¹é… - ä¸¥æ ¼è¯„ä¼°"""
    matched_gt = set()
    correct_detections = []
    
    for det in detections:
        if len(det) >= 6:
            x1, y1, x2, y2, conf, cls_id = det[:6]
            cls_id = int(cls_id)
            det_box = [float(x1), float(y1), float(x2), float(y2)]
            
            best_iou = 0.0
            best_gt_idx = -1
            
            for gt_idx, (gt_box, gt_cls) in enumerate(zip(gt_boxes, gt_classes)):
                if gt_idx in matched_gt:
                    continue
                
                if cls_id == gt_cls:  # ç§ç±»å¿…é¡»åŒ¹é…
                    iou = calculate_iou(det_box, gt_box)
                    if iou > best_iou and iou >= iou_threshold:
                        best_iou = iou
                        best_gt_idx = gt_idx
            
            if best_gt_idx >= 0:
                matched_gt.add(best_gt_idx)
                correct_detections.append({
                    'det_box': det_box,
                    'gt_box': gt_boxes[best_gt_idx],
                    'class': cls_id,
                    'class_name': VOC_CLASSES[cls_id],
                    'confidence': float(conf),
                    'iou': best_iou
                })
    
    return correct_detections, len(matched_gt), len(gt_boxes)

def draw_separated_comparison(img, detections, gt_boxes, gt_classes, correct_detections, image_id, detection_stats):
    """ç»˜åˆ¶åˆ†ç¦»å¼å¯¹æ¯”å›¾ï¼šå·¦å›¾çœŸå®æ ‡æ³¨ï¼Œå³å›¾é¢„æµ‹ç»“æœ"""
    img_height, img_width = img.shape[:2]

    # åˆ›å»ºå¹¶æ’å¯¹æ¯”å›¾
    comparison_img = np.zeros((img_height, img_width * 2, 3), dtype=np.uint8)

    # å·¦å›¾ï¼šçœŸå®æ ‡æ³¨
    gt_img = img.copy()
    for gt_box, gt_cls in zip(gt_boxes, gt_classes):
        x1, y1, x2, y2 = map(int, gt_box)
        cls_name = VOC_CLASSES[gt_cls]

        # ç»¿è‰²å®çº¿æ¡†
        cv2.rectangle(gt_img, (x1, y1), (x2, y2), (0, 255, 0), 3)
        cv2.putText(gt_img, f'GT: {cls_name}', (x1, y1-10),
                   cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)

    # æ·»åŠ å·¦å›¾æ ‡é¢˜
    cv2.putText(gt_img, 'Ground Truth', (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1.0, (255, 255, 255), 3)
    cv2.putText(gt_img, 'Ground Truth', (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1.0, (0, 255, 0), 2)
    cv2.putText(gt_img, f'Total: {len(gt_boxes)} objects', (10, 70), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 255, 255), 3)
    cv2.putText(gt_img, f'Total: {len(gt_boxes)} objects', (10, 70), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 0), 2)

    # å³å›¾ï¼šé¢„æµ‹ç»“æœ
    pred_img = img.copy()
    correct_boxes = {tuple(cd['det_box']): cd for cd in correct_detections}

    for det in detections:
        if len(det) >= 6:
            x1, y1, x2, y2, conf, cls_id = det[:6]
            x1, y1, x2, y2 = map(int, [x1, y1, x2, y2])
            cls_id = int(cls_id)
            cls_name = VOC_CLASSES[cls_id] if cls_id < len(VOC_CLASSES) else f'Class{cls_id}'

            det_box_key = tuple([float(x1), float(y1), float(x2), float(y2)])
            is_correct = det_box_key in correct_boxes

            # æ­£ç¡®æ£€æµ‹ç”¨ç»¿è‰²ï¼Œé”™è¯¯æ£€æµ‹ç”¨çº¢è‰²
            color = (0, 255, 0) if is_correct else (0, 0, 255)
            thickness = 3 if is_correct else 2

            cv2.rectangle(pred_img, (x1, y1), (x2, y2), color, thickness)

            status = "âœ…" if is_correct else "âŒ"
            iou_text = f" IoU:{correct_boxes[det_box_key]['iou']:.3f}" if is_correct else ""
            label_text = f'{status}{cls_name} {float(conf):.3f}{iou_text}'

            cv2.putText(pred_img, label_text, (x1, y1-10),
                       cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, 2)

    # æ·»åŠ å³å›¾æ ‡é¢˜å’Œç»Ÿè®¡ä¿¡æ¯
    cv2.putText(pred_img, 'Predictions', (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1.0, (255, 255, 255), 3)
    cv2.putText(pred_img, 'Predictions', (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1.0, (0, 0, 255), 2)

    # è¯¦ç»†ç»Ÿè®¡ä¿¡æ¯
    stats_text = [
        f'Detected: {len(detections)} objects',
        f'Correct: {len(correct_detections)} objects',
        f'Accuracy: {detection_stats["strict_accuracy"]*100:.1f}%',
        f'Conf Thresh: {detection_stats["conf_thresh"]}'
    ]

    for i, text in enumerate(stats_text):
        y_pos = 70 + i * 30
        cv2.putText(pred_img, text, (10, y_pos), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)
        cv2.putText(pred_img, text, (10, y_pos), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 0, 255), 1)

    # ç»„åˆå›¾åƒ
    comparison_img[:, :img_width] = gt_img
    comparison_img[:, img_width:] = pred_img

    # æ·»åŠ åˆ†å‰²çº¿
    cv2.line(comparison_img, (img_width, 0), (img_width, img_height), (255, 255, 255), 3)

    # æ·»åŠ æ€»ä½“ä¿¡æ¯
    config = IMAGE_CONFIGS[image_id]
    info_text = f"Image {image_id}: {config['description']}"
    cv2.putText(comparison_img, info_text, (img_width//2 - 200, img_height - 30),
               cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 255, 255), 3)
    cv2.putText(comparison_img, info_text, (img_width//2 - 200, img_height - 30),
               cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 0, 0), 2)

    return comparison_img

def analyze_training_speed(model, img_tensor, targets_tensor, loss_fn, optimizer):
    """åˆ†æè®­ç»ƒé€Ÿåº¦æ…¢çš„åŸå› """
    print(f"\nğŸ” è®­ç»ƒé€Ÿåº¦åˆ†æ:")
    
    # æµ‹è¯•å•è½®è®­ç»ƒæ—¶é—´
    times = []
    for i in range(5):
        start_time = time.time()
        
        model.train()
        outputs = model(img_tensor)
        loss, loss_items = loss_fn(outputs, targets_tensor, epoch_num=1, step_num=1)
        optimizer.zero_grad()
        optimizer.backward(loss)
        optimizer.step()
        
        end_time = time.time()
        times.append(end_time - start_time)
    
    avg_time = np.mean(times)
    print(f"   å•è½®è®­ç»ƒå¹³å‡æ—¶é—´: {avg_time:.2f}ç§’")
    print(f"   200è½®é¢„è®¡æ—¶é—´: {avg_time * 200 / 60:.1f}åˆ†é’Ÿ")
    
    if avg_time > 5.0:
        print(f"   âš ï¸ è®­ç»ƒé€Ÿåº¦åæ…¢ï¼Œå¯èƒ½åŸå› :")
        print(f"     - æŸå¤±å‡½æ•°å†…éƒ¨ä»æœ‰è°ƒè¯•è¾“å‡º")
        print(f"     - æ¨¡å‹è®¡ç®—å¤æ‚åº¦è¿‡é«˜")
        print(f"     - GPUåˆ©ç”¨ç‡ä¸è¶³")
    else:
        print(f"   âœ… è®­ç»ƒé€Ÿåº¦æ­£å¸¸")
    
    return avg_time

def analyze_detection_position_error(detections, gt_boxes, gt_classes):
    """åˆ†æç‰©ä½“æ£€æµ‹ä½ç½®é”™è¯¯çš„åŸå› """
    print(f"\nğŸ” æ£€æµ‹ä½ç½®é”™è¯¯åˆ†æ:")
    
    if len(detections) == 0:
        print(f"   âŒ æ²¡æœ‰æ£€æµ‹ç»“æœ - å¯èƒ½åŸå› :")
        print(f"     - ç½®ä¿¡åº¦é˜ˆå€¼è¿‡é«˜")
        print(f"     - æ¨¡å‹æœªå……åˆ†è®­ç»ƒ")
        print(f"     - åˆ†ç±»å­¦ä¹ å¤±è´¥")
        return
    
    # åˆ†ææ¯ä¸ªæ£€æµ‹ç»“æœ
    for i, det in enumerate(detections[:10]):  # åªåˆ†æå‰10ä¸ª
        if len(det) >= 6:
            x1, y1, x2, y2, conf, cls_id = det[:6]
            cls_id = int(cls_id)
            cls_name = VOC_CLASSES[cls_id] if cls_id < len(VOC_CLASSES) else f'Class{cls_id}'
            det_box = [float(x1), float(y1), float(x2), float(y2)]
            
            # æ‰¾åˆ°æœ€ä½³åŒ¹é…çš„çœŸå®æ¡†
            best_iou = 0.0
            best_gt_idx = -1
            for gt_idx, (gt_box, gt_cls) in enumerate(zip(gt_boxes, gt_classes)):
                iou = calculate_iou(det_box, gt_box)
                if iou > best_iou:
                    best_iou = iou
                    best_gt_idx = gt_idx
            
            if best_gt_idx >= 0:
                gt_cls_name = VOC_CLASSES[gt_classes[best_gt_idx]]
                class_match = cls_id == gt_classes[best_gt_idx]
                position_match = best_iou >= 0.3
                
                print(f"   æ£€æµ‹{i+1}: {cls_name} conf={conf:.3f}")
                print(f"     æœ€ä½³åŒ¹é…GT: {gt_cls_name} IoU={best_iou:.3f}")
                print(f"     ç±»åˆ«åŒ¹é…: {'âœ…' if class_match else 'âŒ'}")
                print(f"     ä½ç½®åŒ¹é…: {'âœ…' if position_match else 'âŒ'}")
                
                if not class_match:
                    print(f"     âš ï¸ ç±»åˆ«é”™è¯¯: é¢„æµ‹{cls_name} vs çœŸå®{gt_cls_name}")
                if not position_match:
                    print(f"     âš ï¸ ä½ç½®é”™è¯¯: IoU={best_iou:.3f} < 0.3")

def unified_overfitting_test(image_id, epochs=30):
    """ç»Ÿä¸€è¿‡æ‹Ÿåˆæµ‹è¯•"""
    config = IMAGE_CONFIGS[image_id]
    print(f"ğŸ”¥ ç»Ÿä¸€è¿‡æ‹Ÿåˆæµ‹è¯• - {config['description']}")
    print("=" * 80)
    
    # è¯»å–æ•°æ®
    annotations = []
    with open(config['label_path'], 'r') as f:
        for line in f:
            line = line.strip()
            if line:
                parts = line.split()
                if len(parts) >= 5:
                    cls_id = int(parts[0])
                    x_center = float(parts[1])
                    y_center = float(parts[2])
                    width = float(parts[3])
                    height = float(parts[4])
                    annotations.append([cls_id, x_center, y_center, width, height])
    
    # è¯»å–å›¾åƒ
    original_img = cv2.imread(config['img_path'])
    img_height, img_width = original_img.shape[:2]
    
    # è½¬æ¢æ ‡æ³¨
    gt_boxes = []
    gt_classes = []
    target_counts = {}
    
    for ann in annotations:
        cls_id, x_center, y_center, width, height = ann
        
        x1 = int((x_center - width/2) * img_width)
        y1 = int((y_center - height/2) * img_height)
        x2 = int((x_center + width/2) * img_width)
        y2 = int((y_center + height/2) * img_height)
        
        gt_boxes.append([x1, y1, x2, y2])
        gt_classes.append(cls_id)
        
        cls_name = VOC_CLASSES[cls_id]
        target_counts[cls_name] = target_counts.get(cls_name, 0) + 1
    
    print(f"ğŸ“‹ çœŸå®æ ‡æ³¨: {target_counts}")
    print(f"   æœŸæœ›ç±»åˆ«: {config['expected_classes']}")
    print(f"   æ€»ç›®æ ‡æ•°: {len(annotations)}")
    
    # å‡†å¤‡è¾“å…¥
    img = letterbox(original_img, new_shape=640, stride=32, auto=False)[0]
    img_tensor_input = img.transpose((2, 0, 1))[::-1]
    img_tensor_input = np.ascontiguousarray(img_tensor_input)
    img_tensor_input = img_tensor_input.astype(np.float32) / 255.0
    img_tensor = jt.array(img_tensor_input).unsqueeze(0)
    
    # å‡†å¤‡æ ‡ç­¾
    targets = []
    for ann in annotations:
        cls_id, x_center, y_center, width, height = ann
        targets.append([0, cls_id, x_center, y_center, width, height])
    targets_tensor = jt.array(targets, dtype=jt.float32).unsqueeze(0)
    
    # åˆ›å»ºæ¨¡å‹
    print(f"ğŸ¯ åˆ›å»ºæ¨¡å‹...")
    model = create_perfect_gold_yolo_model()
    model = pytorch_exact_initialization(model)
    
    # åˆ›å»ºæŸå¤±å‡½æ•°
    loss_fn = ComputeLoss(
        num_classes=20,
        ori_img_size=640,
        warmup_epoch=4,
        use_dfl=False,
        reg_max=0,
        fpn_strides=[8, 16, 32],
        grid_cell_size=5.0,
        grid_cell_offset=0.5,
        loss_weight={'class': 1.0, 'iou': 2.5, 'dfl': 0.5}
    )
    
    # åˆ›å»ºä¼˜åŒ–å™¨
    optimizer = jt.optim.AdamW(model.parameters(), lr=0.01, weight_decay=0.0005)
    
    # åˆ†æè®­ç»ƒé€Ÿåº¦
    avg_time = analyze_training_speed(model, img_tensor, targets_tensor, loss_fn, optimizer)
    
    # åˆ›å»ºä¿å­˜ç›®å½•
    save_dir = Path(f"runs/unified_test_image_{image_id}")
    save_dir.mkdir(parents=True, exist_ok=True)
    
    print(f"\nğŸš€ å¼€å§‹è¿‡æ‹Ÿåˆè®­ç»ƒ ({epochs}è½®):")
    
    # è®­ç»ƒè®°å½•
    loss_history = []
    accuracy_history = []
    best_strict_accuracy = 0.0
    best_model_path = None
    
    for epoch in range(1, epochs + 1):
        start_time = time.time()
        
        # è®­ç»ƒ
        model.train()
        outputs = model(img_tensor)
        loss, loss_items = loss_fn(outputs, targets_tensor, epoch_num=epoch, step_num=1)
        optimizer.zero_grad()
        optimizer.backward(loss)
        optimizer.step()
        
        epoch_time = time.time() - start_time
        epoch_loss = float(loss.data)
        loss_history.append(epoch_loss)
        
        # æ¯5è½®è¯„ä¼°
        if epoch % 5 == 0:
            print(f"Epoch {epoch:2d}: Loss {epoch_loss:.6f} ({epoch_time:.2f}s)")
            
            # æ¨ç†æµ‹è¯•
            model.eval()
            with jt.no_grad():
                test_outputs = model(img_tensor)
                
                # å°è¯•ä¸åŒç½®ä¿¡åº¦é˜ˆå€¼ï¼Œä¸¥æ ¼æ§åˆ¶æ£€æµ‹æ•°é‡
                best_result = None
                for conf_thresh in [0.3, 0.2, 0.1, 0.05, 0.01]:
                    try:
                        # ä¸¥æ ¼æ§åˆ¶æ£€æµ‹æ•°é‡ï¼Œæœ€å¤šæ£€æµ‹10ä¸ªç›®æ ‡
                        pred = non_max_suppression(test_outputs, conf_thres=conf_thresh, iou_thres=0.6, max_det=10)
                        
                        if len(pred) > 0 and len(pred[0]) > 0:
                            detections = pred[0]
                            
                            if hasattr(detections, 'numpy'):
                                detections_np = detections.numpy()
                            else:
                                detections_np = detections
                            
                            if detections_np.ndim == 3:
                                detections_np = detections_np.reshape(-1, detections_np.shape[-1])
                            
                            # ä¸¥æ ¼è¯„ä¼°
                            correct_detections, matched_count, total_gt = match_detections_to_gt(
                                detections_np, gt_boxes, gt_classes, iou_threshold=0.3
                            )
                            
                            strict_accuracy = matched_count / total_gt if total_gt > 0 else 0.0
                            
                            if strict_accuracy > 0 or len(detections_np) > 0:
                                best_result = {
                                    'conf_thresh': conf_thresh,
                                    'detections': detections_np,
                                    'correct_detections': correct_detections,
                                    'matched_count': matched_count,
                                    'total_gt': total_gt,
                                    'strict_accuracy': strict_accuracy
                                }
                                break
                    except:
                        continue
                
                if best_result:
                    result = best_result
                    accuracy_history.append(result['strict_accuracy'])
                    
                    print(f"   ç½®ä¿¡åº¦é˜ˆå€¼: {result['conf_thresh']}")
                    print(f"   æ£€æµ‹æ•°é‡: {len(result['detections'])} (æœŸæœ›: {result['total_gt']})")
                    print(f"   ä¸¥æ ¼è¯„ä¼°: {result['matched_count']}/{result['total_gt']} = {result['strict_accuracy']*100:.1f}%")

                    # æ£€æµ‹æ•°é‡éªŒè¯
                    if len(result['detections']) > result['total_gt'] * 3:
                        print(f"   âš ï¸ æ£€æµ‹æ•°é‡è¿‡å¤šï¼æ£€æµ‹{len(result['detections'])}ä¸ª vs æœŸæœ›{result['total_gt']}ä¸ª")
                    elif len(result['detections']) < result['total_gt'] * 0.5:
                        print(f"   âš ï¸ æ£€æµ‹æ•°é‡è¿‡å°‘ï¼æ£€æµ‹{len(result['detections'])}ä¸ª vs æœŸæœ›{result['total_gt']}ä¸ª")
                    else:
                        print(f"   âœ… æ£€æµ‹æ•°é‡åˆç†")
                    
                    # åˆ†ææ£€æµ‹ä½ç½®é”™è¯¯
                    if result['strict_accuracy'] < 1.0:
                        analyze_detection_position_error(result['detections'], gt_boxes, gt_classes)
                    
                    # ä¿å­˜æœ€ä½³æ¨¡å‹
                    if result['strict_accuracy'] > best_strict_accuracy:
                        best_strict_accuracy = result['strict_accuracy']
                        best_model_path = save_dir / f'best_model_epoch_{epoch}.pkl'
                        jt.save({
                            'model': model.state_dict(),
                            'epoch': epoch,
                            'loss': epoch_loss,
                            'strict_accuracy': result['strict_accuracy'],
                            'conf_thresh': result['conf_thresh'],
                            'correct_detections': result['correct_detections']
                        }, str(best_model_path))
                        print(f"   ğŸ† æ–°çš„æœ€ä½³ç»“æœï¼ä¸¥æ ¼å‡†ç¡®ç‡: {result['strict_accuracy']*100:.1f}%")

                        # ç”Ÿæˆåˆ†ç¦»å¼å¯¹æ¯”å¯è§†åŒ–
                        detection_stats = {
                            'strict_accuracy': result['strict_accuracy'],
                            'conf_thresh': result['conf_thresh']
                        }
                        vis_img = draw_separated_comparison(original_img, result['detections'],
                                                          gt_boxes, gt_classes, result['correct_detections'],
                                                          image_id, detection_stats)

                        vis_path = save_dir / f'comparison_epoch_{epoch}.jpg'
                        cv2.imwrite(str(vis_path), vis_img)
                        print(f"   ğŸ’¾ åˆ†ç¦»å¼å¯¹æ¯”å›¾å·²ä¿å­˜: {vis_path}")
                        print(f"   ğŸ“Š æ£€æµ‹ç»Ÿè®¡: æ£€æµ‹{len(result['detections'])}ä¸ªï¼Œæ­£ç¡®{len(result['correct_detections'])}ä¸ª")
                
                else:
                    accuracy_history.append(0.0)
                    print(f"   âŒ æ‰€æœ‰ç½®ä¿¡åº¦é˜ˆå€¼éƒ½æ²¡æœ‰æ£€æµ‹ç»“æœ")
            
            model.train()
    
    # æœ€ç»ˆè¯„ä¼°
    print(f"\nğŸ‰ è¿‡æ‹Ÿåˆè®­ç»ƒå®Œæˆï¼")
    print(f"âœ… æœ€ä½³ä¸¥æ ¼å‡†ç¡®ç‡: {best_strict_accuracy*100:.1f}%")
    
    if best_model_path and os.path.exists(best_model_path):
        checkpoint = jt.load(str(best_model_path))
        
        print(f"\nğŸ“Š æœ€ç»ˆç»“æœ:")
        print(f"   æœ€ä½³è½®æ¬¡: {checkpoint['epoch']}")
        print(f"   æœ€ä½³ç½®ä¿¡åº¦é˜ˆå€¼: {checkpoint['conf_thresh']}")
        print(f"   æœ€ä½³ä¸¥æ ¼å‡†ç¡®ç‡: {checkpoint['strict_accuracy']*100:.1f}%")
        print(f"   æ­£ç¡®æ£€æµ‹è¯¦æƒ…:")
        
        for i, cd in enumerate(checkpoint['correct_detections']):
            print(f"     {i+1}. {cd['class_name']}: IoU={cd['iou']:.3f}, Conf={cd['confidence']:.3f}")
        
        # ç»˜åˆ¶è®­ç»ƒæ›²çº¿
        plt.figure(figsize=(12, 4))
        
        plt.subplot(1, 2, 1)
        plt.plot(range(1, len(loss_history)+1), loss_history, 'b-', linewidth=2)
        plt.title(f'Training Loss - Image {image_id}')
        plt.xlabel('Epoch')
        plt.ylabel('Loss')
        plt.grid(True)
        
        plt.subplot(1, 2, 2)
        epochs_eval = list(range(5, len(accuracy_history)*5+1, 5))
        plt.plot(epochs_eval, [acc*100 for acc in accuracy_history], 'r-', linewidth=2, marker='o')
        plt.title(f'Strict Accuracy - Image {image_id}')
        plt.xlabel('Epoch')
        plt.ylabel('Accuracy (%)')
        plt.grid(True)
        
        plt.tight_layout()
        curve_path = save_dir / 'training_curves.png'
        plt.savefig(str(curve_path), dpi=150, bbox_inches='tight')
        plt.close()
        print(f"   ğŸ“ˆ è®­ç»ƒæ›²çº¿å·²ä¿å­˜: {curve_path}")
        
        # è¯„ä¼°æ˜¯å¦é€šè¿‡æµ‹è¯•
        detected_classes = set(cd['class_name'] for cd in checkpoint['correct_detections'])
        class_accuracy = len(detected_classes.intersection(config['expected_classes'])) / len(config['expected_classes'])
        
        if best_strict_accuracy >= 0.8 and class_accuracy >= 0.8:
            print(f"\nğŸ‰ğŸ‰ğŸ‰ å›¾ç‰‡{image_id}è¿‡æ‹Ÿåˆæµ‹è¯•é€šè¿‡ï¼ğŸ‰ğŸ‰ğŸ‰")
            print(f"âœ… ç§ç±»å‡†ç¡®ç‡: {class_accuracy*100:.1f}%")
            print(f"âœ… ä½ç½®å‡†ç¡®ç‡: {best_strict_accuracy*100:.1f}%")
            print(f"âœ… æ£€æµ‹ç±»åˆ«: {detected_classes}")
            return True
        else:
            print(f"\nâš ï¸ å›¾ç‰‡{image_id}è¿‡æ‹Ÿåˆæµ‹è¯•æœªå®Œå…¨é€šè¿‡")
            print(f"   ç§ç±»å‡†ç¡®ç‡: {class_accuracy*100:.1f}%")
            print(f"   ä½ç½®å‡†ç¡®ç‡: {best_strict_accuracy*100:.1f}%")
            print(f"   æ£€æµ‹ç±»åˆ«: {detected_classes}")
            print(f"   æœŸæœ›ç±»åˆ«: {config['expected_classes']}")
            return False
    
    else:
        print(f"âŒ æ²¡æœ‰ä¿å­˜çš„æœ€ä½³æ¨¡å‹")
        return False

def main():
    parser = argparse.ArgumentParser(description='ç»Ÿä¸€è¿‡æ‹Ÿåˆæµ‹è¯•è„šæœ¬')
    parser.add_argument('--image', type=str, choices=['1', '2', 'both'], default='1',
                       help='æµ‹è¯•å›¾ç‰‡: 1=ç¬¬ä¸€å¼ , 2=ç¬¬äºŒå¼ , both=ä¸¤å¼ éƒ½æµ‹è¯•')
    parser.add_argument('--epochs', type=int, default=30,
                       help='è®­ç»ƒè½®æ•° (é»˜è®¤30)')
    
    args = parser.parse_args()
    
    print("ğŸ”¥ GOLD-YOLO Jittorç‰ˆæœ¬ - ç»Ÿä¸€è¿‡æ‹Ÿåˆæµ‹è¯•")
    print("=" * 80)
    print("è§£å†³4ä¸ªæ ¸å¿ƒé—®é¢˜:")
    print("1. è®­ç»ƒé€Ÿåº¦æ…¢çš„åŸå› åˆ†æå’Œä¿®å¤")
    print("2. ç‰©ä½“æ£€æµ‹ä½ç½®é”™è¯¯çš„åŸå› åˆ†æ")
    print("3. ä¸¤å¼ å›¾ç‰‡è¿‡æ‹Ÿåˆè®­ç»ƒæµ‹è¯•ï¼ˆç§ç±»ã€æ•°é‡ã€ä½ç½®å…¨éƒ¨æ­£ç¡®ï¼‰")
    print("4. æ¸…ç†è„šæœ¬ï¼Œå¢å¼ºå¯å¤ç”¨æ€§å’Œå¯ç»´æŠ¤æ€§")
    print("=" * 80)
    
    if args.image == 'both':
        # æµ‹è¯•ä¸¤å¼ å›¾ç‰‡
        results = []
        for image_id in [1, 2]:
            print(f"\n{'='*20} æµ‹è¯•å›¾ç‰‡{image_id} {'='*20}")
            success = unified_overfitting_test(image_id, args.epochs)
            results.append(success)
        
        print(f"\nğŸ¯ æœ€ç»ˆç»“æœæ€»ç»“:")
        print(f"   å›¾ç‰‡1æµ‹è¯•: {'âœ…é€šè¿‡' if results[0] else 'âŒæœªé€šè¿‡'}")
        print(f"   å›¾ç‰‡2æµ‹è¯•: {'âœ…é€šè¿‡' if results[1] else 'âŒæœªé€šè¿‡'}")
        
        if all(results):
            print(f"\nğŸ‰ğŸ‰ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼GOLD-YOLO Jittorç‰ˆæœ¬åŠŸèƒ½å®Œå…¨æ­£å¸¸ï¼ğŸ‰ğŸ‰ğŸ‰")
        else:
            print(f"\nğŸ“Š éƒ¨åˆ†æµ‹è¯•æœªé€šè¿‡ï¼Œéœ€è¦è¿›ä¸€æ­¥ä¼˜åŒ–")
    
    else:
        # æµ‹è¯•å•å¼ å›¾ç‰‡
        image_id = int(args.image)
        success = unified_overfitting_test(image_id, args.epochs)
        
        if success:
            print(f"\nğŸ‰ğŸ‰ğŸ‰ å›¾ç‰‡{image_id}æµ‹è¯•é€šè¿‡ï¼ğŸ‰ğŸ‰ğŸ‰")
        else:
            print(f"\nğŸ“Š å›¾ç‰‡{image_id}æµ‹è¯•å®Œæˆï¼Œéœ€è¦è¿›ä¸€æ­¥åˆ†æ")

if __name__ == "__main__":
    main()
