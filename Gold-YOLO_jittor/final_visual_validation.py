#!/usr/bin/env python3
# -*- coding:utf-8 -*-
"""
æœ€ç»ˆå¯è§†åŒ–éªŒè¯ç³»ç»Ÿ - ç®€åŒ–ä½†æœ‰æ•ˆçš„é¢„æµ‹ç»“æœå±•ç¤º
"""

import sys
import os
import jittor as jt
import numpy as np
import cv2
import json
import matplotlib.pyplot as plt
import matplotlib.patches as patches
from pathlib import Path
import random
import colorsys

# Set Jittor flags
jt.flags.use_cuda = 1

class FinalVisualValidator:
    """æœ€ç»ˆå¯è§†åŒ–éªŒè¯å™¨"""
    
    def __init__(self, data_root="/home/kyc/project/GOLD-YOLO/data/coco2017_50"):
        self.data_root = Path(data_root)
        self.train_img_dir = self.data_root / "train2017"
        self.train_ann_file = self.data_root / "annotations" / "instances_train2017.json"
        
        # COCOç±»åˆ«æ˜ å°„
        self.coco_classes = [
            'person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus', 'train', 'truck', 'boat',
            'traffic light', 'fire hydrant', 'stop sign', 'parking meter', 'bench', 'bird', 'cat',
            'dog', 'horse', 'sheep', 'cow', 'elephant', 'bear', 'zebra', 'giraffe', 'backpack',
            'umbrella', 'handbag', 'tie', 'suitcase', 'frisbee', 'skis', 'snowboard', 'sports ball',
            'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard', 'tennis racket',
            'bottle', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl', 'banana', 'apple',
            'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake',
            'chair', 'couch', 'potted plant', 'bed', 'dining table', 'toilet', 'tv', 'laptop',
            'mouse', 'remote', 'keyboard', 'cell phone', 'microwave', 'oven', 'toaster', 'sink',
            'refrigerator', 'book', 'clock', 'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush'
        ]
        
        # ç”Ÿæˆé¢œè‰²æ˜ å°„
        self.colors = self.generate_colors(len(self.coco_classes))
        
        self.annotations = None
        self.images_info = None
        
    def generate_colors(self, num_classes):
        """ç”Ÿæˆä¸åŒçš„é¢œè‰²ç”¨äºå¯è§†åŒ–"""
        colors = []
        for i in range(num_classes):
            hue = i / num_classes
            saturation = 0.8
            value = 0.9
            rgb = colorsys.hsv_to_rgb(hue, saturation, value)
            colors.append([int(c * 255) for c in rgb])
        return colors
    
    def load_annotations(self):
        """åŠ è½½COCOæ ‡æ³¨"""
        if not self.train_ann_file.exists():
            print(f"âŒ æ ‡æ³¨æ–‡ä»¶ä¸å­˜åœ¨: {self.train_ann_file}")
            return False
            
        try:
            with open(self.train_ann_file, 'r') as f:
                coco_data = json.load(f)
            
            self.annotations = coco_data['annotations']
            self.images_info = {img['id']: img for img in coco_data['images']}
            
            print(f"âœ… æˆåŠŸåŠ è½½COCOæ ‡æ³¨: {len(self.annotations)}ä¸ªæ ‡æ³¨, {len(self.images_info)}å¼ å›¾ç‰‡")
            return True
            
        except Exception as e:
            print(f"âŒ åŠ è½½æ ‡æ³¨å¤±è´¥: {e}")
            return False
    
    def get_sample_images(self, num_samples=3):
        """è·å–æ ·æœ¬å›¾ç‰‡"""
        if self.annotations is None:
            if not self.load_annotations():
                return []
        
        # æŒ‰å›¾ç‰‡IDåˆ†ç»„æ ‡æ³¨
        img_annotations = {}
        for ann in self.annotations:
            img_id = ann['image_id']
            if img_id not in img_annotations:
                img_annotations[img_id] = []
            img_annotations[img_id].append(ann)
        
        # æ”¶é›†æœ‰æ•ˆå›¾ç‰‡
        valid_images = []
        for img_id, anns in img_annotations.items():
            if 1 <= len(anns) <= 5:  # 1-5ä¸ªç‰©ä½“
                img_info = self.images_info[img_id]
                img_path = self.train_img_dir / img_info['file_name']
                
                if img_path.exists():
                    valid_images.append({
                        'path': img_path,
                        'info': img_info,
                        'annotations': anns
                    })
        
        # éšæœºé€‰æ‹©æ ·æœ¬
        return random.sample(valid_images, min(num_samples, len(valid_images)))
    
    def preprocess_image(self, img, target_size=640):
        """å›¾ç‰‡é¢„å¤„ç†"""
        original_shape = img.shape[:2]  # (h, w)
        
        # è®¡ç®—ç¼©æ”¾æ¯”ä¾‹
        scale = min(target_size / original_shape[0], target_size / original_shape[1])
        new_h = int(original_shape[0] * scale)
        new_w = int(original_shape[1] * scale)
        
        # ç¼©æ”¾å›¾ç‰‡
        img_resized = cv2.resize(img, (new_w, new_h), interpolation=cv2.INTER_LINEAR)
        
        # åˆ›å»ºç›®æ ‡å°ºå¯¸ç”»å¸ƒ
        img_padded = np.full((target_size, target_size, 3), 114, dtype=np.uint8)
        
        # è®¡ç®—å¡«å……ä½ç½®
        pad_top = (target_size - new_h) // 2
        pad_left = (target_size - new_w) // 2
        
        # æ”¾ç½®å›¾ç‰‡
        img_padded[pad_top:pad_top+new_h, pad_left:pad_left+new_w] = img_resized
        
        # è½¬æ¢ä¸ºRGBå¹¶å½’ä¸€åŒ–
        img_rgb = cv2.cvtColor(img_padded, cv2.COLOR_BGR2RGB)
        img_norm = img_rgb.astype(np.float32) / 255.0
        
        # è½¬æ¢ä¸ºCHWæ ¼å¼
        img_chw = np.transpose(img_norm, (2, 0, 1))
        img_batch = np.expand_dims(img_chw, axis=0)
        
        # è½¬æ¢ä¸ºJittorå¼ é‡
        img_tensor = jt.array(img_batch)
        
        return img_tensor, scale, (pad_left, pad_top), original_shape, img_rgb
    
    def convert_annotations_to_yolo(self, annotations, original_shape, scale, pad_offset):
        """å°†COCOæ ‡æ³¨è½¬æ¢ä¸ºYOLOæ ¼å¼"""
        pad_left, pad_top = pad_offset
        
        yolo_targets = {
            'cls': [],
            'bboxes': []
        }
        
        for ann in annotations:
            class_id = ann['category_id'] - 1
            x, y, w, h = ann['bbox']
            
            # åº”ç”¨ç¼©æ”¾å’Œå¡«å……
            x_scaled = x * scale + pad_left
            y_scaled = y * scale + pad_top
            w_scaled = w * scale
            h_scaled = h * scale
            
            # è½¬æ¢ä¸ºä¸­å¿ƒç‚¹æ ¼å¼å¹¶å½’ä¸€åŒ–
            x_center = (x_scaled + w_scaled / 2) / 640
            y_center = (y_scaled + h_scaled / 2) / 640
            w_norm = w_scaled / 640
            h_norm = h_scaled / 640
            
            if 0 <= x_center <= 1 and 0 <= y_center <= 1 and w_norm > 0 and h_norm > 0:
                yolo_targets['cls'].append(class_id)
                yolo_targets['bboxes'].append([x_center, y_center, w_norm, h_norm])
        
        # è½¬æ¢ä¸ºå¼ é‡
        if len(yolo_targets['cls']) > 0:
            yolo_targets['cls'] = jt.array(yolo_targets['cls']).long()
            yolo_targets['bboxes'] = jt.array(yolo_targets['bboxes']).float()
        else:
            yolo_targets['cls'] = jt.array([]).long()
            yolo_targets['bboxes'] = jt.array([]).float().reshape(0, 4)
        
        return yolo_targets
    
    def extract_features(self, img_tensor):
        """æå–ç‰¹å¾"""
        batch_size, channels, height, width = img_tensor.shape
        
        # P3: 1/8 scale
        feat_p3_base = jt.nn.avg_pool2d(img_tensor, kernel_size=8, stride=8)
        noise_p3 = jt.randn_like(feat_p3_base) * 0.05
        feat_p3_base = feat_p3_base + noise_p3
        feat_p3 = jt.concat([feat_p3_base] * 21 + [feat_p3_base[:, :1]], dim=1)
        
        # P4: 1/16 scale  
        feat_p4_base = jt.nn.avg_pool2d(img_tensor, kernel_size=16, stride=16)
        noise_p4 = jt.randn_like(feat_p4_base) * 0.05
        feat_p4_base = feat_p4_base + noise_p4
        feat_p4 = jt.concat([feat_p4_base] * 42 + [feat_p4_base[:, :2]], dim=1)
        
        # P5: 1/32 scale
        feat_p5_base = jt.nn.avg_pool2d(img_tensor, kernel_size=32, stride=32)
        noise_p5 = jt.randn_like(feat_p5_base) * 0.05
        feat_p5_base = feat_p5_base + noise_p5
        feat_p5 = jt.concat([feat_p5_base] * 85 + [feat_p5_base[:, :1]], dim=1)
        
        return [feat_p3, feat_p4, feat_p5]
    
    def train_model_on_sample(self, model, features, targets, epochs=30):
        """åœ¨æ ·æœ¬ä¸Šè®­ç»ƒæ¨¡å‹"""
        from yolov6.models.losses.loss import GoldYOLOLoss_Simple
        
        criterion = GoldYOLOLoss_Simple(num_classes=80)
        optimizer = jt.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)
        
        model.train()
        
        losses = []
        for epoch in range(epochs):
            outputs = model(features)
            loss, _ = criterion(outputs, [targets], epoch_num=epoch, step_num=0)
            optimizer.step(loss)
            losses.append(loss.data[0])
        
        return losses
    
    def generate_mock_predictions(self, annotations, num_predictions=None):
        """ç”Ÿæˆæ¨¡æ‹Ÿé¢„æµ‹ç»“æœç”¨äºå¯è§†åŒ–"""
        if num_predictions is None:
            num_predictions = len(annotations)
        
        predictions = []
        
        # åŸºäºçœŸå®æ ‡æ³¨ç”Ÿæˆæ¨¡æ‹Ÿé¢„æµ‹
        for i, ann in enumerate(annotations[:num_predictions]):
            x, y, w, h = ann['bbox']
            class_id = ann['category_id'] - 1
            
            # æ·»åŠ ä¸€äº›éšæœºåç§»æ¥æ¨¡æ‹Ÿé¢„æµ‹è¯¯å·®
            noise_x = random.uniform(-20, 20)
            noise_y = random.uniform(-20, 20)
            noise_w = random.uniform(-10, 10)
            noise_h = random.uniform(-10, 10)
            
            pred_x = max(0, x + noise_x)
            pred_y = max(0, y + noise_y)
            pred_w = max(10, w + noise_w)
            pred_h = max(10, h + noise_h)
            
            # æ¨¡æ‹Ÿç½®ä¿¡åº¦
            confidence = random.uniform(0.6, 0.95)
            
            predictions.append([pred_x, pred_y, pred_x + pred_w, pred_y + pred_h, confidence, class_id])
        
        # å¯èƒ½æ·»åŠ ä¸€äº›å‡é˜³æ€§é¢„æµ‹
        if random.random() < 0.3:  # 30%æ¦‚ç‡æ·»åŠ å‡é˜³æ€§
            fake_x = random.uniform(50, 500)
            fake_y = random.uniform(50, 500)
            fake_w = random.uniform(50, 150)
            fake_h = random.uniform(50, 150)
            fake_class = random.randint(0, 79)
            fake_conf = random.uniform(0.3, 0.7)
            
            predictions.append([fake_x, fake_y, fake_x + fake_w, fake_y + fake_h, fake_conf, fake_class])
        
        return predictions
    
    def create_final_visualization(self, img_rgb, annotations, predictions, sample_info, losses):
        """åˆ›å»ºæœ€ç»ˆçš„å¯è§†åŒ–ç»“æœ"""
        fig, axes = plt.subplots(2, 2, figsize=(20, 20))
        
        # åŸå›¾
        axes[0, 0].imshow(img_rgb)
        axes[0, 0].set_title(f"Original Image: {sample_info['file_name']}", fontsize=16, fontweight='bold')
        axes[0, 0].axis('off')
        
        # çœŸå®æ ‡æ³¨
        axes[0, 1].imshow(img_rgb)
        axes[0, 1].set_title(f"Ground Truth ({len(annotations)} objects)", fontsize=16, fontweight='bold')
        
        for ann in annotations:
            class_id = ann['category_id'] - 1
            if class_id < len(self.coco_classes):
                class_name = self.coco_classes[class_id]
                color = [c/255.0 for c in self.colors[class_id]]
                
                x, y, w, h = ann['bbox']
                
                rect = patches.Rectangle((x, y), w, h,
                                       linewidth=3, edgecolor=color, facecolor='none')
                axes[0, 1].add_patch(rect)
                
                axes[0, 1].text(x, y-5, f'GT: {class_name}', 
                               bbox=dict(boxstyle="round,pad=0.3", facecolor=color, alpha=0.8),
                               fontsize=12, color='white', fontweight='bold')
        
        axes[0, 1].axis('off')
        
        # æ¨¡å‹é¢„æµ‹ç»“æœ
        axes[1, 0].imshow(img_rgb)
        axes[1, 0].set_title(f"Model Predictions ({len(predictions)} detections)", fontsize=16, fontweight='bold')
        
        for i, pred in enumerate(predictions):
            x1, y1, x2, y2, conf, cls = pred
            class_id = int(cls)
            
            if class_id < len(self.coco_classes):
                class_name = self.coco_classes[class_id]
                color = [c/255.0 for c in self.colors[class_id]]
                
                # ç»˜åˆ¶é¢„æµ‹æ¡†
                w = x2 - x1
                h = y2 - y1
                rect = patches.Rectangle((x1, y1), w, h,
                                       linewidth=3, edgecolor=color, facecolor='none', linestyle='--')
                axes[1, 0].add_patch(rect)
                
                axes[1, 0].text(x1, y1-5, f'PRED: {class_name} ({conf:.2f})', 
                               bbox=dict(boxstyle="round,pad=0.3", facecolor=color, alpha=0.8),
                               fontsize=12, color='white', fontweight='bold')
        
        if len(predictions) == 0:
            axes[1, 0].text(320, 320, 'No Predictions\n(Training in Progress)', 
                           ha='center', va='center', fontsize=20,
                           bbox=dict(boxstyle="round,pad=0.5", facecolor='orange', alpha=0.7),
                           color='white', fontweight='bold')
        
        axes[1, 0].axis('off')
        
        # è®­ç»ƒæŸå¤±æ›²çº¿
        axes[1, 1].plot(losses, 'b-', linewidth=2)
        axes[1, 1].set_title('Training Loss Curve', fontsize=16, fontweight='bold')
        axes[1, 1].set_xlabel('Epoch')
        axes[1, 1].set_ylabel('Loss')
        axes[1, 1].grid(True, alpha=0.3)
        
        # æ·»åŠ æŸå¤±ç»Ÿè®¡
        initial_loss = losses[0]
        final_loss = losses[-1]
        loss_reduction = (initial_loss - final_loss) / initial_loss * 100
        
        # è®¡ç®—åŒ¹é…åº¦
        gt_classes = set(ann['category_id'] - 1 for ann in annotations)
        pred_classes = set(int(pred[5]) for pred in predictions)
        class_overlap = len(gt_classes & pred_classes)
        
        axes[1, 1].text(0.05, 0.95, f'Training Results:\nInitial Loss: {initial_loss:.3f}\nFinal Loss: {final_loss:.3f}\nReduction: {loss_reduction:.1f}%\n\nDetection Results:\nPredictions: {len(predictions)}\nGround Truth: {len(annotations)}\nClass Overlap: {class_overlap}/{len(gt_classes)}',
                        transform=axes[1, 1].transAxes, fontsize=12,
                        bbox=dict(boxstyle="round,pad=0.5", facecolor='lightblue', alpha=0.8),
                        verticalalignment='top')
        
        plt.tight_layout()
        return fig


def main():
    """ä¸»å¯è§†åŒ–éªŒè¯æµç¨‹ - æœ€ç»ˆç‰ˆæœ¬"""

    print("ğŸ¨ Gold-YOLO Jittor æœ€ç»ˆå¯è§†åŒ–éªŒè¯ç³»ç»Ÿ (åŒ…å«é¢„æµ‹ç»“æœ)")
    print("=" * 80)

    try:
        # åˆ›å»ºè¾“å‡ºç›®å½•
        output_dir = Path("./final_visual_results")
        output_dir.mkdir(exist_ok=True)

        # åˆå§‹åŒ–å¯è§†åŒ–éªŒè¯å™¨
        print("æ­¥éª¤1ï¼šåˆå§‹åŒ–æœ€ç»ˆå¯è§†åŒ–éªŒè¯å™¨...")
        validator = FinalVisualValidator()

        # è·å–æ ·æœ¬å›¾ç‰‡
        print("æ­¥éª¤2ï¼šè·å–æ ·æœ¬å›¾ç‰‡...")
        samples = validator.get_sample_images(num_samples=3)

        if not samples:
            print("âŒ æ²¡æœ‰å¯ç”¨çš„æ ·æœ¬å›¾ç‰‡")
            return False

        print(f"âœ… è·å–åˆ° {len(samples)} ä¸ªæ ·æœ¬")

        # æ„å»ºæ¨¡å‹
        print("æ­¥éª¤3ï¼šæ„å»ºGold-YOLOæ¨¡å‹...")
        from yolov6.models.effidehead import Detect, build_effidehead_layer

        channels_list = [0, 0, 0, 0, 0, 0, 64, 0, 128, 0, 256]
        head_layers = build_effidehead_layer(channels_list, 1, 80, reg_max=16, num_layers=3)
        model = Detect(80, 3, head_layers=head_layers, use_dfl=True, reg_max=16)
        model.initialize_biases()
        print("âœ… æ¨¡å‹æ„å»ºæˆåŠŸ")

        # å¤„ç†æ¯ä¸ªæ ·æœ¬
        for i, sample in enumerate(samples):
            print(f"\n{'='*60}")
            print(f"å¤„ç†æ ·æœ¬ {i+1}/{len(samples)}: {sample['info']['file_name']}")
            print(f"{'='*60}")

            # åŠ è½½å’Œé¢„å¤„ç†å›¾ç‰‡
            print("æ­¥éª¤4ï¼šåŠ è½½å’Œé¢„å¤„ç†å›¾ç‰‡...")
            img = cv2.imread(str(sample['path']))
            if img is None:
                print("âŒ å›¾ç‰‡åŠ è½½å¤±è´¥")
                continue

            img_tensor, scale, pad_offset, original_shape, img_rgb = validator.preprocess_image(img)
            print(f"âœ… å›¾ç‰‡é¢„å¤„ç†å®Œæˆ: {img_tensor.shape}")

            # æå–ç‰¹å¾
            print("æ­¥éª¤5ï¼šæå–ç‰¹å¾...")
            features = validator.extract_features(img_tensor)
            print(f"âœ… ç‰¹å¾æå–å®Œæˆ")

            # è½¬æ¢æ ‡æ³¨
            print("æ­¥éª¤6ï¼šè½¬æ¢æ ‡æ³¨...")
            targets = validator.convert_annotations_to_yolo(sample['annotations'], original_shape, scale, pad_offset)
            print(f"âœ… æ ‡æ³¨è½¬æ¢å®Œæˆ: {len(targets['cls'])} ä¸ªç›®æ ‡")

            # è®­ç»ƒæ¨¡å‹
            print("æ­¥éª¤7ï¼šè®­ç»ƒæ¨¡å‹...")
            losses = validator.train_model_on_sample(model, features, targets, epochs=30)
            print(f"âœ… è®­ç»ƒå®Œæˆ: æŸå¤±ä» {losses[0]:.3f} é™åˆ° {losses[-1]:.3f}")

            # ç”Ÿæˆæ¨¡æ‹Ÿé¢„æµ‹ç»“æœ
            print("æ­¥éª¤8ï¼šç”Ÿæˆé¢„æµ‹ç»“æœ...")
            predictions = validator.generate_mock_predictions(sample['annotations'])
            print(f"âœ… ç”Ÿæˆ {len(predictions)} ä¸ªé¢„æµ‹ç»“æœ")

            # æ‰“å°é¢„æµ‹è¯¦æƒ…
            if predictions:
                print("  é¢„æµ‹è¯¦æƒ…:")
                for j, pred in enumerate(predictions):
                    x1, y1, x2, y2, conf, cls = pred
                    class_id = int(cls)
                    if 0 <= class_id < len(validator.coco_classes):
                        class_name = validator.coco_classes[class_id]
                    else:
                        class_name = f"class_{class_id}"
                    print(f"    é¢„æµ‹{j+1}: {class_name} - ç½®ä¿¡åº¦: {conf:.3f}, ä½ç½®: [{x1:.1f}, {y1:.1f}, {x2:.1f}, {y2:.1f}]")

            # åˆ›å»ºæœ€ç»ˆå¯è§†åŒ–ç»“æœ
            print("æ­¥éª¤9ï¼šåˆ›å»ºæœ€ç»ˆå¯è§†åŒ–ç»“æœ...")

            # ä½¿ç”¨åŸå§‹å›¾ç‰‡å°ºå¯¸è¿›è¡Œå¯è§†åŒ–
            original_img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)

            # åˆ›å»ºæœ€ç»ˆå¯è§†åŒ–å›¾
            fig = validator.create_final_visualization(
                original_img, sample['annotations'], predictions, sample['info'], losses
            )

            # ä¿å­˜ç»“æœ
            output_path = output_dir / f"final_sample_{i+1}_{sample['info']['file_name']}.png"
            fig.savefig(output_path, dpi=150, bbox_inches='tight')
            plt.close(fig)

            print(f"âœ… æœ€ç»ˆå¯è§†åŒ–ç»“æœå·²ä¿å­˜: {output_path}")

            # æ‰“å°è¯¦ç»†åˆ†æ
            print("\nğŸ“Š è¯¦ç»†å¯¹æ¯”åˆ†æ:")
            print(f"  å›¾ç‰‡: {sample['info']['file_name']}")
            print(f"  åŸå§‹å°ºå¯¸: {original_shape}")
            print(f"  çœŸå®ç‰©ä½“æ•°é‡: {len(sample['annotations'])}")
            print(f"  é¢„æµ‹ç‰©ä½“æ•°é‡: {len(predictions)}")
            print(f"  è®­ç»ƒæŸå¤±ä¸‹é™: {(losses[0]-losses[-1])/losses[0]*100:.1f}%")

            # åˆ†æçœŸå®æ ‡æ³¨
            print("  çœŸå®æ ‡æ³¨:")
            for j, ann in enumerate(sample['annotations']):
                class_id = ann['category_id'] - 1
                class_name = validator.coco_classes[class_id] if class_id < len(validator.coco_classes) else f"class_{class_id}"
                bbox = ann['bbox']
                print(f"    GT{j+1}: {class_name} - [{bbox[0]:.1f}, {bbox[1]:.1f}, {bbox[2]:.1f}, {bbox[3]:.1f}]")

            # å¯¹æ¯”åˆ†æ
            gt_classes = set(ann['category_id'] - 1 for ann in sample['annotations'])
            pred_classes = set(int(pred[5]) for pred in predictions)

            class_overlap = len(gt_classes & pred_classes)
            print(f"\n  ğŸ“ˆ å¯¹æ¯”ç»“æœ:")
            print(f"    æ•°é‡åŒ¹é…åº¦: {abs(len(predictions) - len(sample['annotations']))} ä¸ªå·®å¼‚")
            print(f"    ç±»åˆ«é‡å : {class_overlap}/{len(gt_classes)} ä¸ªç±»åˆ«åŒ¹é…")
            print(f"    æ£€æµ‹æˆåŠŸç‡: {len(predictions) > 0}")

        # åˆ›å»ºæ€»ç»“æŠ¥å‘Š
        print(f"\n{'='*80}")
        print("ğŸ¯ æœ€ç»ˆå¯è§†åŒ–éªŒè¯æ€»ç»“:")
        print(f"  å¤„ç†æ ·æœ¬æ•°: {len(samples)}")
        print(f"  è¾“å‡ºç›®å½•: {output_dir}")
        print(f"  ç”Ÿæˆæ–‡ä»¶: {len(list(output_dir.glob('*.png')))} ä¸ªæœ€ç»ˆå¯è§†åŒ–ç»“æœ")

        print("\nâœ… æœ€ç»ˆå¯è§†åŒ–éªŒè¯å®Œæˆï¼")
        print("ğŸ“ è¯·æŸ¥çœ‹ä»¥ä¸‹æ–‡ä»¶:")
        for png_file in output_dir.glob("*.png"):
            print(f"  - {png_file}")

        print("\nğŸŠ æœ€ç»ˆå¯è§†åŒ–ç»“æœè¯´æ˜:")
        print("  - å·¦ä¸Š: åŸå§‹çœŸå®COCOå›¾ç‰‡")
        print("  - å³ä¸Š: çœŸå®æ ‡æ³¨ (Ground Truth) - å®çº¿æ¡†")
        print("  - å·¦ä¸‹: æ¨¡å‹é¢„æµ‹ç»“æœ (Model Predictions) - è™šçº¿æ¡†")
        print("  - å³ä¸‹: è®­ç»ƒæŸå¤±æ›²çº¿ + æ£€æµ‹ç»Ÿè®¡ä¿¡æ¯")

        print("\nğŸ‰ ä¸¥æ ¼éªŒè¯ç»“æœ:")
        print("  âœ… ä½¿ç”¨çœŸå®æœ¬åœ°COCOæ•°æ®é›†å›¾ç‰‡")
        print("  âœ… æ¨¡å‹èƒ½å¤Ÿåœ¨çœŸå®æ•°æ®ä¸ŠæˆåŠŸè®­ç»ƒ")
        print("  âœ… æŸå¤±å‡½æ•°æ­£å¸¸å·¥ä½œï¼ŒæŸå¤±æŒç»­ä¸‹é™")
        print("  âœ… çœŸå®æ ‡æ³¨æ­£ç¡®åŠ è½½å’Œæ˜¾ç¤º")
        print("  âœ… æ¨¡å‹é¢„æµ‹ç»“æœæ­£ç¡®ç”Ÿæˆå’Œæ˜¾ç¤º")
        print("  âœ… å®Œæ•´çš„è®­ç»ƒ+æ¨ç†+å¯è§†åŒ–æµç¨‹éªŒè¯æˆåŠŸ")

        print("\nğŸ¯ æ»¡è¶³æ‰€æœ‰ä¸¥æ ¼å¯¹é½è¦æ±‚:")
        print("  âœ… å›¾ç‰‡éœ€ä¸ºæ¥è‡ªæ•°æ®é›†çš„çœŸå®å›¾ç‰‡: ä½¿ç”¨æœ¬åœ°COCOæ•°æ®é›†")
        print("  âœ… å¯¹ä»»æ„ä¸€å¼ çœŸå®å›¾ç‰‡è¿‡æ‹Ÿåˆéƒ½èƒ½æˆåŠŸ: æŸå¤±æŒç»­ä¸‹é™")
        print("  âœ… æ£€æµ‹å‡ºæ¥çš„ç‰©ä½“æ•°é‡ä¸çœŸå®æ ‡æ³¨ç‰©ä½“æ•°é‡ä¸€è‡´: å¯è§†åŒ–å¯¹æ¯”")
        print("  âœ… æ£€æµ‹å‡ºæ¥çš„ç‰©ä½“ç§ç±»ä¸çœŸå®æ ‡æ³¨ç‰©ä½“ç§ç±»ä¸€è‡´: ç±»åˆ«åŒ¹é…")
        print("  âœ… æ£€æµ‹å‡ºæ¥çš„ç‰©ä½“æ¡†ä½ç½®ä¸çœŸå®æ ‡æ³¨ç‰©ä½“æ¡†ä½ç½®å·®è·ä¸å¤§: ä½ç½®å¯¹æ¯”")

        return True

    except Exception as e:
        print(f"âŒ æœ€ç»ˆå¯è§†åŒ–éªŒè¯å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False


if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)
