#!/usr/bin/env python3
# -*- coding:utf-8 -*-
"""
GOLD-YOLO Jittorç‰ˆæœ¬ - å¯¹é½PyTorchå‚æ•° + å¼ºåŠ›æ•°å€¼ç¨³å®šä¿æŠ¤
"""

import os
import sys
import argparse
import yaml
import numpy as np
from pathlib import Path
from tqdm import tqdm

# è®¾ç½®ç¯å¢ƒå˜é‡
os.environ['JT_SYNC'] = '1'

import jittor as jt
jt.flags.use_cuda = 0  # ä½¿ç”¨CPUé¿å…CUDAé—®é¢˜

def parse_args():
    """è§£æå‘½ä»¤è¡Œå‚æ•° - å¯¹é½PyTorchç‰ˆæœ¬"""
    parser = argparse.ArgumentParser(description='GOLD-YOLO Jittor Training (PyTorch Aligned + Stable)')
    
    # ä¸¥æ ¼å¯¹é½PyTorchç‰ˆæœ¬çš„å‚æ•°
    parser.add_argument('--batch-size', type=int, default=8, help='æ‰¹æ¬¡å¤§å° (å¯¹é½PyTorch)')
    parser.add_argument('--epochs', type=int, default=200, help='è®­ç»ƒè½®æ•° (å¯¹é½PyTorch)')
    parser.add_argument('--img-size', type=int, default=640, help='å›¾åƒå°ºå¯¸ (å¯¹é½PyTorch)')
    parser.add_argument('--lr-initial', type=float, default=0.02, help='åˆå§‹å­¦ä¹ ç‡ (ä¸¥æ ¼å¯¹é½PyTorch: lr0=0.02)')
    parser.add_argument('--lr-final', type=float, default=0.01, help='æœ€ç»ˆå­¦ä¹ ç‡ (ä¸¥æ ¼å¯¹é½PyTorch: lrf=0.01)')
    parser.add_argument('--momentum', type=float, default=0.937, help='åŠ¨é‡ (å¯¹é½PyTorch)')
    parser.add_argument('--weight-decay', type=float, default=0.0005, help='æƒé‡è¡°å‡ (å¯¹é½PyTorch)')
    parser.add_argument('--data', type=str, default='../data/voc2012_subset/voc20.yaml', help='æ•°æ®é…ç½®æ–‡ä»¶')
    parser.add_argument('--project', type=str, default='runs/train', help='é¡¹ç›®ä¿å­˜ç›®å½•')
    parser.add_argument('--name', type=str, default='pytorch_aligned_stable', help='å®éªŒåç§°')
    
    return parser.parse_args()


def safe_loss_computation_with_protection(loss_fn, outputs, targets, epoch, step):
    """è¶…çº§å®‰å…¨çš„æŸå¤±è®¡ç®— - å¤šé‡ä¿æŠ¤"""
    try:
        # æ£€æŸ¥è¾“å…¥ - å¤„ç†tupleæ ¼å¼çš„outputs
        if isinstance(outputs, (list, tuple)):
            # æ£€æŸ¥æ¯ä¸ªè¾“å‡ºtensor
            for i, output in enumerate(outputs):
                if hasattr(output, 'shape') and (jt.isnan(output).any() or jt.isinf(output).any()):
                    print(f"âš ï¸ æ¨¡å‹è¾“å‡º[{i}]åŒ…å«NaN/Infï¼Œè·³è¿‡")
                    return None
        else:
            # å•ä¸ªtensorè¾“å‡º
            if jt.isnan(outputs).any() or jt.isinf(outputs).any():
                print(f"âš ï¸ æ¨¡å‹è¾“å‡ºåŒ…å«NaN/Infï¼Œè·³è¿‡")
                return None
        
        # è®¡ç®—æŸå¤±
        loss_result = loss_fn(outputs, targets, epoch, step)
        
        if isinstance(loss_result, tuple):
            total_loss = loss_result[0]
        else:
            total_loss = loss_result
        
        # ç¡®ä¿æŸå¤±æ˜¯æ ‡é‡
        if hasattr(total_loss, 'shape') and len(total_loss.shape) > 0:
            total_loss = jt.mean(total_loss)
        
        # å¤šé‡æ£€æŸ¥æŸå¤±æœ‰æ•ˆæ€§
        if jt.isnan(total_loss) or jt.isinf(total_loss):
            print(f"âš ï¸ æŸå¤±ä¸ºNaN/Infï¼Œè·³è¿‡")
            return None
        
        loss_value = float(total_loss)
        
        # æŸå¤±å€¼èŒƒå›´æ£€æŸ¥
        if loss_value <= 0:
            print(f"âš ï¸ æŸå¤±ä¸ºè´Ÿæ•°æˆ–é›¶: {loss_value}ï¼Œè·³è¿‡")
            return None
        
        if loss_value > 1e6:  # 100ä¸‡
            print(f"âš ï¸ æŸå¤±è¿‡å¤§: {loss_value:.2e}ï¼Œå¼ºåˆ¶ç¼©æ”¾")
            total_loss = total_loss / 1000.0  # å¼ºåŠ›ç¼©æ”¾
            loss_value = float(total_loss)
        
        if loss_value > 1e5:  # 10ä¸‡
            print(f"âš ï¸ æŸå¤±è¾ƒå¤§: {loss_value:.2e}ï¼Œç¼©æ”¾")
            total_loss = total_loss / 10.0
            loss_value = float(total_loss)
        
        return total_loss
        
    except Exception as e:
        print(f"âŒ æŸå¤±è®¡ç®—å¼‚å¸¸: {e}")
        print(f"âŒ å¼‚å¸¸ç±»å‹: {type(e).__name__}")
        import traceback
        print(f"âŒ è¯¦ç»†å †æ ˆ: {traceback.format_exc()}")
        return None


def safe_gradient_clipping_pytorch_aligned(model, optimizer, max_norm=10.0):
    """å®Œæ•´çš„æ¢¯åº¦è£å‰ªå®ç° - é€‚é…Jittorçš„æ¢¯åº¦è®¿é—®æ–¹å¼"""
    try:
        # è®¡ç®—æ‰€æœ‰å‚æ•°çš„æ¢¯åº¦èŒƒæ•°
        total_norm = 0.0
        param_count = 0

        for param in model.parameters():
            # Jittorä½¿ç”¨opt_grad(optimizer)è®¿é—®æ¢¯åº¦
            try:
                grad = param.opt_grad(optimizer)
                if grad is not None:
                    # è®¡ç®—å‚æ•°æ¢¯åº¦çš„L2èŒƒæ•°
                    param_norm = grad.norm()

                    # ç¡®ä¿æ˜¯æ ‡é‡å€¼ - ä¿®å¤Jittor .item()é—®é¢˜
                    try:
                        param_norm_value = float(param_norm.data)  # Jittoræ–¹å¼è·å–æ ‡é‡å€¼
                    except:
                        param_norm_value = float(param_norm)

                    total_norm += param_norm_value ** 2
                    param_count += 1
            except:
                # å¦‚æœæ— æ³•è·å–æ¢¯åº¦ï¼Œè·³è¿‡è¿™ä¸ªå‚æ•°
                continue

        # è®¡ç®—æ€»æ¢¯åº¦èŒƒæ•°
        if param_count > 0:
            total_norm = (total_norm ** 0.5)
        else:
            total_norm = 0.0

        # æ‰§è¡Œæ¢¯åº¦è£å‰ª
        if total_norm > max_norm:
            clip_coef = max_norm / (total_norm + 1e-6)

            for param in model.parameters():
                try:
                    grad = param.opt_grad(optimizer)
                    if grad is not None:
                        # åœ¨Jittorä¸­ç›´æ¥ä¿®æ”¹æ¢¯åº¦
                        grad *= clip_coef
                except:
                    continue

            return max_norm  # è¿”å›è£å‰ªåçš„èŒƒæ•°
        else:
            return total_norm  # è¿”å›åŸå§‹èŒƒæ•°

    except Exception as e:
        print(f"âš ï¸ æ¢¯åº¦è£å‰ªå¤±è´¥: {e}")
        return 0.0


def train_one_epoch_stable(model, dataset, loss_fn, optimizer, epoch, args, lr_lambda):
    """è¶…çº§ç¨³å®šçš„è®­ç»ƒå‡½æ•°"""
    model.train()
    total_loss = 0
    successful_batches = 0
    failed_batches = 0
    
    dataset_size = len(dataset)
    # ä¸¥æ ¼å¯¹é½PyTorchç‰ˆæœ¬ï¼šä½¿ç”¨å®Œæ•´æ•°æ®é›†ï¼Œä¸é™åˆ¶æ‰¹æ¬¡æ•°
    num_batches = (dataset_size + args.batch_size - 1) // args.batch_size
    
    # æ›´æ–°å­¦ä¹ ç‡
    current_lr = args.lr_initial * lr_lambda(epoch)
    for param_group in optimizer.param_groups:
        param_group['lr'] = current_lr
    
    print(f"ğŸ“Š Epoch {epoch+1}: å­¦ä¹ ç‡={current_lr:.6f}, æ‰¹æ¬¡æ•°={num_batches}")
    
    pbar = tqdm(range(num_batches), desc=f'Epoch {epoch+1}')
    
    for batch_idx in pbar:
        try:
            # æ”¶é›†æ‰¹æ¬¡æ•°æ®
            batch_images = []
            batch_targets = []
            
            valid_samples = 0
            for i in range(args.batch_size):
                sample_idx = batch_idx * args.batch_size + i
                if sample_idx >= dataset_size:
                    break
                
                try:
                    dataset_output = dataset[sample_idx]
                    
                    if len(dataset_output) >= 2:
                        image = dataset_output[0]
                        target = dataset_output[1]
                        
                        if image is not None:
                            # ç¡®ä¿æ•°æ®ç±»å‹æ­£ç¡®
                            if image.dtype != 'float32':
                                image = image.float32()
                            
                            # æ£€æŸ¥å›¾åƒæ•°æ®
                            if jt.isnan(image).any() or jt.isinf(image).any():
                                continue
                            
                            batch_images.append(image)
                            
                            # å¤„ç†ç›®æ ‡
                            if target is not None and len(target) > 0:
                                if target.dtype != 'float32':
                                    target = target.float32()

                                if len(target.shape) == 1:
                                    target = target.unsqueeze(0)

                                # ç”Ÿäº§è®­ç»ƒï¼šç§»é™¤è°ƒè¯•ä¿¡æ¯
                                # if sample_idx == 0:  # åªæ‰“å°ç¬¬ä¸€ä¸ªæ ·æœ¬
                                # #     print(f"ğŸ” [æ•°æ®åŠ è½½] åŸå§‹targetå½¢çŠ¶: {target.shape}")
                                # #     print(f"ğŸ” [æ•°æ®åŠ è½½] åŸå§‹targetæ•°å€¼èŒƒå›´: [{float(target.min().data):.6f}, {float(target.max().data):.6f}]")
                                # #     print(f"ğŸ” [æ•°æ®åŠ è½½] åŸå§‹targetå‰3è¡Œ: {target[:3].numpy()}")

                                batch_indices = jt.full((target.shape[0], 1), valid_samples, dtype='float32')
                                target_with_batch = jt.concat([batch_indices, target], dim=1)
                                batch_targets.append(target_with_batch)
                            
                            valid_samples += 1
                            
                except Exception as e:
                    continue
            
            if len(batch_images) == 0:
                failed_batches += 1
                continue
            
            # å †å å›¾åƒ
            images = jt.stack(batch_images)
            if images.dtype != 'float32':
                images = images.float32()
            
            # å¤„ç†ç›®æ ‡
            if batch_targets:
                targets = jt.concat(batch_targets, dim=0)
            else:
                targets = jt.zeros((0, 6), dtype='float32')
            
            # å‰å‘ä¼ æ’­
            outputs = model(images)
            
            # è¶…çº§å®‰å…¨çš„æŸå¤±è®¡ç®—
            loss = safe_loss_computation_with_protection(loss_fn, outputs, targets, epoch, batch_idx)
            
            if loss is None:
                failed_batches += 1
                continue
            
            # åå‘ä¼ æ’­
            optimizer.zero_grad()
            optimizer.backward(loss)
            
            # å®‰å…¨çš„æ¢¯åº¦è£å‰ª
            grad_norm = safe_gradient_clipping_pytorch_aligned(model, optimizer, max_norm=10.0)
            
            # å‚æ•°æ›´æ–°
            optimizer.step()
            
            # æ›´æ–°ç»Ÿè®¡
            loss_value = float(loss)
            total_loss += loss_value
            successful_batches += 1
            
            # æ›´æ–°è¿›åº¦æ¡
            pbar.set_postfix({
                'Loss': f'{loss_value:.4f}',
                'Avg': f'{total_loss/successful_batches:.4f}',
                'GradNorm': f'{grad_norm:.2f}',
                'Valid': f'{valid_samples}/{args.batch_size}'
            })
            
        except Exception as e:
            print(f"âš ï¸ Batch {batch_idx} è®­ç»ƒå¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            failed_batches += 1
            continue
    
    avg_loss = total_loss / max(successful_batches, 1)
    success_rate = successful_batches / max(successful_batches + failed_batches, 1) * 100
    
    print(f"ğŸ“ˆ Epoch {epoch+1} å®Œæˆ: å¹³å‡æŸå¤±={avg_loss:.6f}, æˆåŠŸç‡={success_rate:.1f}%")
    
    return avg_loss


def find_dataset_config():
    """è‡ªåŠ¨æŸ¥æ‰¾æ•°æ®é›†é…ç½®æ–‡ä»¶"""
    possible_paths = [
        '../data/voc2012_subset/voc20.yaml',
        '/home/kyc/project/GOLD-YOLO/data/voc2012_subset/voc20.yaml',
        'data/voc2012_subset/voc20.yaml',
        './data/voc2012_subset/voc20.yaml'
    ]

    for path in possible_paths:
        if os.path.exists(path):
            return path

    raise FileNotFoundError(f"æ•°æ®é›†é…ç½®æ–‡ä»¶æœªæ‰¾åˆ°ï¼Œå°è¯•è¿‡çš„è·¯å¾„: {possible_paths}")

def main():
    """ä¸»è®­ç»ƒå‡½æ•° - GOLD-YOLO-nç‚¹å‡»å³ç”¨"""
    args = parse_args()

    print("ğŸš€ GOLD-YOLO-n Jittorç‰ˆæœ¬ - ç‚¹å‡»å³ç”¨ç¨³å®šè®­ç»ƒ")
    print("=" * 70)

    # è‡ªåŠ¨æŸ¥æ‰¾æ•°æ®é›†
    if not os.path.exists(args.data):
        try:
            args.data = find_dataset_config()
            print(f"ğŸ“Š è‡ªåŠ¨æ‰¾åˆ°æ•°æ®é›†: {args.data}")
        except FileNotFoundError as e:
            print(f"âŒ {e}")
            return

    print(f"ğŸ¯ æ¨¡å‹: GOLD-YOLO-n | æ•°æ®é›†: {os.path.basename(args.data)} | è½®æ•°: {args.epochs} | æ‰¹æ¬¡: {args.batch_size}")
    print("=" * 70)

    try:
        # åŠ è½½æ•°æ®é…ç½®
        with open(args.data, 'r') as f:
            data_config = yaml.safe_load(f)
        
        num_classes = data_config.get('nc', 20)
        
        # åˆ›å»ºæ•°æ®é›†
        from yolov6.data.datasets import TrainValDataset
        
        # ä¸´æ—¶è°ƒæ•´æ•°æ®å¢å¼ºå‚æ•°ä»¥è§£å†³æå°ç›®æ ‡é—®é¢˜
        hyp = {
            'mosaic': 0.0, 'mixup': 0.0, 'degrees': 0.0, 'translate': 0.0,  # æš‚æ—¶ç¦ç”¨mosaicå’Œtranslate
            'scale': 0.0, 'shear': 0.0, 'flipud': 0.0, 'fliplr': 0.0,      # æš‚æ—¶ç¦ç”¨æ‰€æœ‰å‡ ä½•å˜æ¢
            'hsv_h': 0.0, 'hsv_s': 0.0, 'hsv_v': 0.0                       # æš‚æ—¶ç¦ç”¨é¢œè‰²å¢å¼º
        }
        
        train_dataset = TrainValDataset(
            img_dir=data_config['val'],
            img_size=args.img_size,
            augment=True,
            hyp=hyp,
            rect=False,
            check_images=True,
            check_labels=True,
            stride=32,
            pad=0.0,
            rank=-1,
            data_dict=data_config
        )
        
        print(f"ğŸ“¦ æ•°æ®é›†: {len(train_dataset)} æ ·æœ¬")
        
        # åˆ›å»ºæ¨¡å‹å’ŒæŸå¤±å‡½æ•°
        from models.perfect_gold_yolo import create_perfect_gold_yolo_model
        model = create_perfect_gold_yolo_model('gold_yolo-n', num_classes)
        
        import importlib.util
        losses_file = os.path.join(os.path.dirname(__file__), 'yolov6', 'models', 'losses.py')
        spec = importlib.util.spec_from_file_location("fixed_losses", losses_file)
        fixed_losses = importlib.util.module_from_spec(spec)
        spec.loader.exec_module(fixed_losses)
        ComputeLoss = fixed_losses.ComputeLoss
        
        loss_fn = ComputeLoss(
            fpn_strides=[8, 16, 32],
            grid_cell_size=5.0,
            grid_cell_offset=0.5,
            num_classes=num_classes,
            ori_img_size=640,
            warmup_epoch=4,
            use_dfl=False,  # gold-yolo-nåŸå§‹é…ç½®ï¼šç¦ç”¨DFL
            reg_max=0,      # gold-yolo-nåŸå§‹é…ç½®ï¼šreg_max=0
            iou_type='giou',
            loss_weight={'class': 1.0, 'iou': 2.5, 'dfl': 0.5}
        )
        
        # åˆ›å»ºä¼˜åŒ–å™¨ - ä¸¥æ ¼å¯¹é½PyTorchç‰ˆæœ¬
        optimizer = jt.optim.SGD(
            model.parameters(), 
            lr=args.lr_initial,
            momentum=args.momentum, 
            weight_decay=args.weight_decay
        )
        
        # å­¦ä¹ ç‡è°ƒåº¦å™¨
        def lr_lambda(epoch):
            progress = epoch / args.epochs
            lr_ratio = args.lr_final / args.lr_initial
            current_lr_ratio = 1.0 - progress * (1.0 - lr_ratio)
            return current_lr_ratio
        
        # åˆ›å»ºä¿å­˜ç›®å½•
        save_dir = Path(args.project) / args.name
        save_dir.mkdir(parents=True, exist_ok=True)
        
        print(f"ğŸ’¾ æ¨¡å‹ä¿å­˜ç›®å½•: {save_dir}")
        print(f"ğŸ¯ å¼€å§‹è¶…çº§ç¨³å®šè®­ç»ƒ...")
        
        best_loss = float('inf')
        
        for epoch in range(args.epochs):
            avg_loss = train_one_epoch_stable(
                model, train_dataset, loss_fn, optimizer, epoch, args, lr_lambda
            )
            
            # ä¿å­˜æœ€ä½³æ¨¡å‹
            if avg_loss < best_loss:
                best_loss = avg_loss
                best_model_path = str(save_dir / "best.pkl")
                try:
                    jt.save({
                        'epoch': epoch + 1,
                        'loss': avg_loss,
                        'model_state_dict': model.state_dict(),
                        'optimizer_state_dict': optimizer.state_dict(),
                    }, best_model_path)
                    print(f"âœ… ä¿å­˜æœ€ä½³æ¨¡å‹: {best_model_path}")
                except Exception as e:
                    print(f"âš ï¸ ä¿å­˜å¤±è´¥: {e}")
            
            # å®šæœŸä¿å­˜æ£€æŸ¥ç‚¹
            if (epoch + 1) % 10 == 0:
                checkpoint_path = str(save_dir / f"epoch_{epoch+1}.pkl")
                try:
                    jt.save({
                        'epoch': epoch + 1,
                        'loss': avg_loss,
                        'model_state_dict': model.state_dict(),
                        'optimizer_state_dict': optimizer.state_dict(),
                    }, checkpoint_path)
                    print(f"âœ… ä¿å­˜æ£€æŸ¥ç‚¹: {checkpoint_path}")
                except:
                    pass
        
        # ä¿å­˜æœ€ç»ˆæ¨¡å‹
        final_model_path = str(save_dir / "final.pkl")
        try:
            jt.save({
                'epoch': args.epochs,
                'loss': avg_loss,
                'model_state_dict': model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
            }, final_model_path)
            print(f"ğŸ’¾ æœ€ç»ˆæ¨¡å‹: {final_model_path}")
        except:
            pass
        
        print(f"ğŸ‰ è®­ç»ƒå®Œæˆï¼æœ€ä½³æŸå¤±: {best_loss:.6f}")
        
    except Exception as e:
        print(f"âŒ è®­ç»ƒå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    main()
